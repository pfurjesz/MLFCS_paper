{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a3ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy import cumsum, log, polyfit, sqrt, std, subtract\n",
    "from numpy.random import randn\n",
    "import pandas as pd\n",
    "# from pandas_datareader import data as web\n",
    "import seaborn as sns\n",
    "from pylab import rcParams \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from arch import arch_model\n",
    "from numpy.linalg import LinAlgError\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as tsa\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, q_stat, adfuller\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import probplot, moment\n",
    "from arch import arch_model\n",
    "from arch.univariate import ConstantMean, GARCH, Normal\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from itertools import product\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5531105e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_txn_data, preprocess_txn_data, compute_lob_features, create_lob_dataset, merge_txn_and_lob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8bca4cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"darkgrid\", color_codes=True)\n",
    "rcParams['figure.figsize'] = 8,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd0fd22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "778ccb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trx Data loaded successfully.\n",
      "preprocessed lob Data loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>buy_volume</th>\n",
       "      <th>sell_volume</th>\n",
       "      <th>buy_txn</th>\n",
       "      <th>sell_txn</th>\n",
       "      <th>volume_imbalance</th>\n",
       "      <th>txn_imbalance</th>\n",
       "      <th>total_volume</th>\n",
       "      <th>mean_volume</th>\n",
       "      <th>deseasoned_total_volume</th>\n",
       "      <th>log_deseasoned_total_volume</th>\n",
       "      <th>ask_volume</th>\n",
       "      <th>bid_volume</th>\n",
       "      <th>ask_slope_1</th>\n",
       "      <th>ask_slope_5</th>\n",
       "      <th>ask_slope_10</th>\n",
       "      <th>bid_slope_1</th>\n",
       "      <th>bid_slope_5</th>\n",
       "      <th>bid_slope_10</th>\n",
       "      <th>spread</th>\n",
       "      <th>lob_volume_imbalance</th>\n",
       "      <th>slope_imbalance_1</th>\n",
       "      <th>slope_imbalance_5</th>\n",
       "      <th>slope_imbalance_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5819</th>\n",
       "      <td>2018-06-04 22:00:05+00:00</td>\n",
       "      <td>0.059804</td>\n",
       "      <td>0.730357</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.670553</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.790162</td>\n",
       "      <td>4.380444</td>\n",
       "      <td>0.180384</td>\n",
       "      <td>-1.712667</td>\n",
       "      <td>2695.804973</td>\n",
       "      <td>586356.113693</td>\n",
       "      <td>1761.630667</td>\n",
       "      <td>2695.804973</td>\n",
       "      <td>2695.804973</td>\n",
       "      <td>3.972121</td>\n",
       "      <td>53.502450</td>\n",
       "      <td>160.246934</td>\n",
       "      <td>6.19</td>\n",
       "      <td>583660.308720</td>\n",
       "      <td>1757.658546</td>\n",
       "      <td>2642.302523</td>\n",
       "      <td>2535.558040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5820</th>\n",
       "      <td>2018-06-04 22:01:05+00:00</td>\n",
       "      <td>0.089359</td>\n",
       "      <td>0.849477</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.760118</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.938836</td>\n",
       "      <td>3.692009</td>\n",
       "      <td>0.254289</td>\n",
       "      <td>-1.369285</td>\n",
       "      <td>2699.165417</td>\n",
       "      <td>586350.938081</td>\n",
       "      <td>1765.312385</td>\n",
       "      <td>2699.165417</td>\n",
       "      <td>2699.165417</td>\n",
       "      <td>4.017044</td>\n",
       "      <td>52.408273</td>\n",
       "      <td>155.071322</td>\n",
       "      <td>4.97</td>\n",
       "      <td>583651.772664</td>\n",
       "      <td>1761.295341</td>\n",
       "      <td>2646.757144</td>\n",
       "      <td>2544.094095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5821</th>\n",
       "      <td>2018-06-04 22:02:05+00:00</td>\n",
       "      <td>0.313458</td>\n",
       "      <td>0.508952</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.195494</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.822410</td>\n",
       "      <td>3.324900</td>\n",
       "      <td>0.247349</td>\n",
       "      <td>-1.396955</td>\n",
       "      <td>2657.946212</td>\n",
       "      <td>586317.596946</td>\n",
       "      <td>1723.843180</td>\n",
       "      <td>2657.946212</td>\n",
       "      <td>2657.946212</td>\n",
       "      <td>3.831055</td>\n",
       "      <td>46.578294</td>\n",
       "      <td>158.194750</td>\n",
       "      <td>4.90</td>\n",
       "      <td>583659.650734</td>\n",
       "      <td>1720.012125</td>\n",
       "      <td>2611.367918</td>\n",
       "      <td>2499.751462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5822</th>\n",
       "      <td>2018-06-04 22:03:05+00:00</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.199219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.198227</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.200211</td>\n",
       "      <td>4.128645</td>\n",
       "      <td>0.048493</td>\n",
       "      <td>-3.026331</td>\n",
       "      <td>2650.599402</td>\n",
       "      <td>586308.612876</td>\n",
       "      <td>1718.061157</td>\n",
       "      <td>2650.599402</td>\n",
       "      <td>2650.599402</td>\n",
       "      <td>3.631836</td>\n",
       "      <td>51.036074</td>\n",
       "      <td>160.641345</td>\n",
       "      <td>4.32</td>\n",
       "      <td>583658.013474</td>\n",
       "      <td>1714.429321</td>\n",
       "      <td>2599.563327</td>\n",
       "      <td>2489.958056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823</th>\n",
       "      <td>2018-06-04 22:04:05+00:00</td>\n",
       "      <td>0.172042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172042</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.172042</td>\n",
       "      <td>6.271124</td>\n",
       "      <td>0.027434</td>\n",
       "      <td>-3.595966</td>\n",
       "      <td>2650.082079</td>\n",
       "      <td>586314.173248</td>\n",
       "      <td>1715.979046</td>\n",
       "      <td>2650.082079</td>\n",
       "      <td>2650.082079</td>\n",
       "      <td>3.704804</td>\n",
       "      <td>51.092926</td>\n",
       "      <td>160.489197</td>\n",
       "      <td>4.32</td>\n",
       "      <td>583664.091169</td>\n",
       "      <td>1712.274243</td>\n",
       "      <td>2598.989153</td>\n",
       "      <td>2489.592882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      datetime  buy_volume  sell_volume  buy_txn  sell_txn  \\\n",
       "5819 2018-06-04 22:00:05+00:00    0.059804     0.730357      5.0      10.0   \n",
       "5820 2018-06-04 22:01:05+00:00    0.089359     0.849477      3.0       4.0   \n",
       "5821 2018-06-04 22:02:05+00:00    0.313458     0.508952      2.0       4.0   \n",
       "5822 2018-06-04 22:03:05+00:00    0.000992     0.199219      1.0       4.0   \n",
       "5823 2018-06-04 22:04:05+00:00    0.172042     0.000000      7.0       0.0   \n",
       "\n",
       "      volume_imbalance  txn_imbalance  total_volume  mean_volume  \\\n",
       "5819          0.670553            5.0      0.790162     4.380444   \n",
       "5820          0.760118            1.0      0.938836     3.692009   \n",
       "5821          0.195494            2.0      0.822410     3.324900   \n",
       "5822          0.198227            3.0      0.200211     4.128645   \n",
       "5823          0.172042            7.0      0.172042     6.271124   \n",
       "\n",
       "      deseasoned_total_volume  log_deseasoned_total_volume   ask_volume  \\\n",
       "5819                 0.180384                    -1.712667  2695.804973   \n",
       "5820                 0.254289                    -1.369285  2699.165417   \n",
       "5821                 0.247349                    -1.396955  2657.946212   \n",
       "5822                 0.048493                    -3.026331  2650.599402   \n",
       "5823                 0.027434                    -3.595966  2650.082079   \n",
       "\n",
       "         bid_volume  ask_slope_1  ask_slope_5  ask_slope_10  bid_slope_1  \\\n",
       "5819  586356.113693  1761.630667  2695.804973   2695.804973     3.972121   \n",
       "5820  586350.938081  1765.312385  2699.165417   2699.165417     4.017044   \n",
       "5821  586317.596946  1723.843180  2657.946212   2657.946212     3.831055   \n",
       "5822  586308.612876  1718.061157  2650.599402   2650.599402     3.631836   \n",
       "5823  586314.173248  1715.979046  2650.082079   2650.082079     3.704804   \n",
       "\n",
       "      bid_slope_5  bid_slope_10  spread  lob_volume_imbalance  \\\n",
       "5819    53.502450    160.246934    6.19         583660.308720   \n",
       "5820    52.408273    155.071322    4.97         583651.772664   \n",
       "5821    46.578294    158.194750    4.90         583659.650734   \n",
       "5822    51.036074    160.641345    4.32         583658.013474   \n",
       "5823    51.092926    160.489197    4.32         583664.091169   \n",
       "\n",
       "      slope_imbalance_1  slope_imbalance_5  slope_imbalance_10  \n",
       "5819        1757.658546        2642.302523         2535.558040  \n",
       "5820        1761.295341        2646.757144         2544.094095  \n",
       "5821        1720.012125        2611.367918         2499.751462  \n",
       "5822        1714.429321        2599.563327         2489.958056  \n",
       "5823        1712.274243        2598.989153         2489.592882  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trx_df = read_txn_data(use_load=False)\n",
    "trx_df = preprocess_txn_data(trx_df, freq='1min')\n",
    "trx_df['log_deseasoned_total_volume'] = np.log(trx_df['deseasoned_total_volume'] + 1e-07)\n",
    "\n",
    "lob_df = create_lob_dataset(use_load=False)\n",
    "\n",
    "df_merged = merge_txn_and_lob(trx_df, lob_df)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28836579",
   "metadata": {},
   "source": [
    "## TME implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042671dd",
   "metadata": {},
   "source": [
    "### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2feeb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 10  # window length\n",
    "batch_size = 128\n",
    "\n",
    "# -----------------------------\n",
    "df = df_merged.sort_values('datetime').reset_index(drop=True)\n",
    "# STEP 1: Create time-of-day feature\n",
    "df['time_of_day'] = df['datetime'].dt.strftime('%H:%M')\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 2: Split indices (AFTER creating lags!)\n",
    "n_total = len(df)\n",
    "n_train = int(0.7 * n_total)\n",
    "n_val = int(0.1 * n_total)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 3: Create deseasonalizing map using per-time volume means from train only\n",
    "train_deseason_df = df.iloc[:n_train]\n",
    "mean_volume_by_time = train_deseason_df.groupby('time_of_day')['total_volume'].mean()\n",
    "df['mean_volume'] = df['time_of_day'].map(mean_volume_by_time)\n",
    "\n",
    "df['deseasoned_total_volume'] = df['total_volume'] / df['mean_volume']\n",
    "df['log_deseasoned_total_volume'] = np.log(df['deseasoned_total_volume'] + 1e-7)\n",
    "df['target'] = df['deseasoned_total_volume'] + 1e-7\n",
    "\n",
    "del train_deseason_df\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 4: Define the source-specific features\n",
    "source1_cols = ['buy_volume', 'sell_volume', 'buy_txn', 'sell_txn', 'volume_imbalance', 'txn_imbalance']\n",
    "source2_cols = ['ask_volume', 'bid_volume', 'ask_slope_1', 'ask_slope_5', 'ask_slope_10', 'bid_slope_1', 'bid_slope_5', 'bid_slope_10', 'spread',\n",
    "       'lob_volume_imbalance', 'slope_imbalance_1', 'slope_imbalance_5', 'slope_imbalance_10']\n",
    "# target_col = 'log_deseasoned_total_volume'\n",
    "target_col = 'target'\n",
    "datetime_col = 'datetime'\n",
    "\n",
    "\n",
    "# --- Create rolling windows efficiently ---\n",
    "source1_array = df[source1_cols].values  # shape (N, F1)\n",
    "source2_array = df[source2_cols].values  # shape (N, F2)\n",
    "target_array = df[target_col].values  # shape (N,)\n",
    "timestamps_array = df[datetime_col].values\n",
    "\n",
    "\n",
    "# Create sliding windows ([:-1] in windows and [h:] in targets make sure the targets are matched with corresponding features)\n",
    "source1_windows = sliding_window_view(source1_array, window_shape=(h,), axis=0)[:-1]  # shape (N - h, F1, h)\n",
    "source2_windows = sliding_window_view(source2_array, window_shape=(h,), axis=0)[:-1]  # shape (N - h, F2, h)\n",
    "y = target_array[h:]\n",
    "timestamps = timestamps_array[h:]\n",
    "\n",
    "# Convert to tensors\n",
    "source1_tensor = torch.tensor(source1_windows, dtype=torch.float32)\n",
    "source2_tensor = torch.tensor(source2_windows, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# --- Time-based split (preserving time order) ---\n",
    "n_total = len(y_tensor)\n",
    "n_train = int(n_total * 0.7)\n",
    "n_val = int(n_total * 0.1)\n",
    "\n",
    "source1_train, source1_val, source1_test = source1_tensor[:n_train], source1_tensor[n_train:n_train + n_val], source1_tensor[n_train + n_val:]\n",
    "source2_train, source2_val, source2_test = source2_tensor[:n_train], source2_tensor[n_train:n_train + n_val], source2_tensor[n_train + n_val:]\n",
    "y_train, y_val, y_test = y_tensor[:n_train], y_tensor[n_train:n_train + n_val], y_tensor[n_train + n_val:]\n",
    "\n",
    "# (Optional) timestamps split for tracking\n",
    "timestamps_train = timestamps[:n_train]\n",
    "timestamps_val = timestamps[n_train:n_train + n_val]\n",
    "timestamps_test = timestamps[n_train + n_val:]\n",
    "\n",
    "# Dataset ready for PyTorch training\n",
    "train_dataset = torch.utils.data.TensorDataset(source1_train, source2_train, y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(source1_val, source2_val, y_val)\n",
    "test_dataset = torch.utils.data.TensorDataset(source1_test, source2_test, y_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "## function for dataset creation for hyperparams search\n",
    "def create_datasets(source1_array, source2_array, target_array, batch_size, h):\n",
    "       # Create sliding windows ([:-1] in windows and [h:] in targets make sure the targets are matched with corresponding features)\n",
    "       source1_windows = sliding_window_view(source1_array, window_shape=(h,), axis=0)[:-1]  # shape (N - h, F1, h)\n",
    "       source2_windows = sliding_window_view(source2_array, window_shape=(h,), axis=0)[:-1]  # shape (N - h, F2, h)\n",
    "       y = target_array[h:]\n",
    "\n",
    "       # Convert to tensors\n",
    "       source1_tensor = torch.tensor(source1_windows, dtype=torch.float32)\n",
    "       source2_tensor = torch.tensor(source2_windows, dtype=torch.float32)\n",
    "       y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "       # --- Time-based split (preserving time order) ---\n",
    "       n_total = len(y_tensor)\n",
    "       n_train = int(n_total * 0.7)\n",
    "       n_val = int(n_total * 0.1)\n",
    "\n",
    "       source1_train, source1_val, source1_test = source1_tensor[:n_train], source1_tensor[n_train:n_train + n_val], source1_tensor[n_train + n_val:]\n",
    "       source2_train, source2_val, source2_test = source2_tensor[:n_train], source2_tensor[n_train:n_train + n_val], source2_tensor[n_train + n_val:]\n",
    "       y_train, y_val, y_test = y_tensor[:n_train], y_tensor[n_train:n_train + n_val], y_tensor[n_train + n_val:]\n",
    "\n",
    "       # Dataset ready for PyTorch training\n",
    "       train_dataset = torch.utils.data.TensorDataset(source1_train, source2_train, y_train)\n",
    "       val_dataset = torch.utils.data.TensorDataset(source1_val, source2_val, y_val)\n",
    "       test_dataset = torch.utils.data.TensorDataset(source1_test, source2_test, y_test)\n",
    "\n",
    "       train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "       val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "       test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "       return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1162b",
   "metadata": {},
   "source": [
    "### TME components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "946d1835",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearRegressor(nn.Module):\n",
    "    def __init__(self, d, h, latent_variable):\n",
    "        \"\"\"\n",
    "        d: number of features in the source data\n",
    "        h: number of lags in the source data\n",
    "        latent_variable (bool): if True the class is devoted for modeling latent variable z, if False => y|s_i\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_variable = latent_variable\n",
    "\n",
    "        # Mean parameters\n",
    "        self.L_mu = nn.Parameter(torch.empty(d))\n",
    "        self.R_mu = nn.Parameter(torch.empty(h))\n",
    "        self.b_mu = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        # Xavier init for 1D weight tensors\n",
    "        # nn.init.xavier_uniform_(self.L_mu.unsqueeze(0))\n",
    "        # nn.init.xavier_uniform_(self.R_mu.unsqueeze(0))\n",
    "\n",
    "        nn.init.xavier_normal_(self.L_mu.unsqueeze(0))\n",
    "        nn.init.xavier_normal_(self.R_mu.unsqueeze(0))\n",
    "\n",
    "        if not self.latent_variable:\n",
    "            self.L_sigma = nn.Parameter(torch.empty(d))\n",
    "            self.R_sigma = nn.Parameter(torch.empty(h))\n",
    "            self.b_sigma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "            # nn.init.xavier_uniform_(self.L_sigma.unsqueeze(0))\n",
    "            # nn.init.xavier_uniform_(self.R_sigma.unsqueeze(0))\n",
    "\n",
    "            nn.init.xavier_normal_(self.L_sigma.unsqueeze(0))\n",
    "            nn.init.xavier_normal_(self.R_sigma.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):  # x: (B, d, h)\n",
    "        mu = torch.einsum('bdh,d,h->b', x, self.L_mu, self.R_mu) + self.b_mu  # [B]\n",
    "        if self.latent_variable:\n",
    "            return mu\n",
    "        log_var = torch.einsum('bdh,d,h->b', x, self.L_sigma, self.R_sigma) + self.b_sigma\n",
    "        log_var = torch.clamp(log_var, min=-10, max=10)\n",
    "        var = torch.exp(log_var)  # Ensure positivity\n",
    "        return mu, var\n",
    "    \n",
    "\n",
    "class TME(nn.Module):\n",
    "    def __init__(self, d1, d2, h):\n",
    "        super().__init__()\n",
    "        self.target1 = BilinearRegressor(d1, h, latent_variable=False)\n",
    "        self.target2 = BilinearRegressor(d2, h, latent_variable=False)\n",
    "        self.latent1 = BilinearRegressor(d1, h, latent_variable=True)\n",
    "        self.latent2 = BilinearRegressor(d2, h, latent_variable=True)\n",
    "\n",
    "    def forward(self, x1, x2, return_all=False):\n",
    "        # x1: (B, d1, h), x2: (B, d2, h)\n",
    "        mu1, var1 = self.target1(x1)  # [B], [B]\n",
    "        mu2, var2 = self.target2(x2)\n",
    "\n",
    "        logit1 = self.latent1(x1)\n",
    "        logit2 = self.latent2(x2)\n",
    "\n",
    "        logits = torch.stack([logit1, logit2], dim=1)  # [B, num_sources]\n",
    "        probs = F.softmax(logits, dim=1)     # [B, num_sources]\n",
    "\n",
    "        if True:#not return_all:\n",
    "            # Clamp to avoid numerical instability\n",
    "            mu1 = torch.clamp(mu1, -10, 10)\n",
    "            mu2 = torch.clamp(mu2, -10, 10)\n",
    "            var1 = torch.clamp(var1, min=1e-5, max=10)\n",
    "            var2 = torch.clamp(var2, min=1e-5, max=10)\n",
    "\n",
    "        # Mixture of expected values under log-normal\n",
    "        exp1 = torch.exp(mu1 + 0.5 * var1)\n",
    "        exp2 = torch.exp(mu2 + 0.5 * var2)\n",
    "        final_pred = probs[:, 0] * exp1 + probs[:, 1] * exp2  # [B]\n",
    "\n",
    "        if return_all:\n",
    "            return final_pred, mu1, var1, mu2, var2, probs\n",
    "        return final_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f5c35d",
   "metadata": {},
   "source": [
    "### Training routine"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "9d512db1",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ee6af69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0 % 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f6f9a05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tme_loss(y, mu1, var1, mu2, var2, probs, model, l2_lambda=0.1):\n",
    "    \"\"\"\n",
    "    Implements:\n",
    "        -ln ∑_s [ lognormal(y_t | μ_s, σ_s^2) * P(z_t = s | x) ] + λ * ||θ||^2\n",
    "    \"\"\"\n",
    "\n",
    "    eps = 1e-8  # for numerical stability\n",
    "    log_y = torch.log(y + eps)\n",
    "\n",
    "    # Log-normal density terms (not in log-space)\n",
    "    def lognormal_pdf(y, log_y, mu, var):\n",
    "        coef = 1.0 / (y * torch.sqrt(2 * torch.pi * var + eps))\n",
    "        exponent = torch.exp(- (log_y - mu) ** 2 / (2 * var + eps))\n",
    "        return coef * exponent\n",
    "\n",
    "    p1 = lognormal_pdf(y, log_y, mu1, var1)\n",
    "    p2 = lognormal_pdf(y, log_y, mu2, var2)\n",
    "\n",
    "    # Combine with selector probabilities\n",
    "    # print(probs[:,1])\n",
    "    weighted_sum = probs[:,0] * p1 + probs[:,1] * p2\n",
    "\n",
    "    # Negative log-likelihood (mean over batch)\n",
    "    nll = -torch.log(weighted_sum + eps).mean() #maybe mean or sum\n",
    "\n",
    "    # L2 Regularization (Gaussian prior on θ)\n",
    "    l2_penalty = sum((p**2).sum() for p in model.parameters())\n",
    "    reg = l2_lambda * l2_penalty\n",
    "\n",
    "    return nll + reg\n",
    "\n",
    "\n",
    "def train_tme_model(model, train_loader, val_loader, lr=5e-4, weight_decay=0.1, l2_lambda=0.1,\n",
    "                    max_epochs=100, patience=10, device='cpu', adam=False):\n",
    "    model.to(device)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=lr)#, weight_decay=0.1)#, momentum=0.9)\n",
    "    if adam:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_state_dict = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        y_preds_train = []\n",
    "        y_true_train = []\n",
    "\n",
    "\n",
    "        for x1, x2, y in train_loader:\n",
    "            x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            final_pred, mu1, var1, mu2, var2, probs = model(x1, x2, return_all=True)\n",
    "\n",
    "            loss = tme_loss(y, mu1, var1, mu2, var2, probs, model, l2_lambda=l2_lambda)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            y_preds_train.append(final_pred.detach().cpu())\n",
    "            y_true_train.append(y.detach().cpu())\n",
    "\n",
    "        y_preds_train = torch.cat(y_preds_train).numpy()\n",
    "        y_true_train = torch.cat(y_true_train).numpy()\n",
    "        rmse_train = np.sqrt(mean_squared_error(y_true_train, y_preds_train))\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        y_preds_val = []\n",
    "        y_true_val = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x1, x2, y in val_loader:\n",
    "                x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
    "\n",
    "                final_pred, mu1, var1, mu2, var2, probs = model(x1, x2, return_all=True)\n",
    "\n",
    "                val_loss = tme_loss(y, mu1, var1, mu2, var2, probs, model, l2_lambda=l2_lambda)\n",
    "                val_losses.append(val_loss.item())\n",
    "                y_preds_val.append(final_pred.detach().cpu())\n",
    "                y_true_val.append(y.detach().cpu())\n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        y_preds_val = torch.cat(y_preds_val).numpy()\n",
    "        y_true_val = torch.cat(y_true_val).numpy()\n",
    "        rmse_val = np.sqrt(mean_squared_error(y_true_val, y_preds_val))\n",
    "        if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}.      Train Loss: {sum(train_losses)/len(train_losses):.4f}, Val Loss: {avg_val_loss:.4f}.      Train RMSE: {rmse_train:.4f}, Val RMSE: {rmse_val:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if avg_val_loss < best_val_loss - 1e-4:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_state_dict = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Restore best model\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    return model, best_val_loss\n",
    "\n",
    "\n",
    "def train_tme_ensemble(train_loader, val_loader, d1, d2, h, num_models=20, device='cpu', adam=False, **train_kwargs):\n",
    "    ensemble = []\n",
    "    val_losses = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        print(f\"\\n🌱 Training ensemble model {i + 1}/{num_models}\")\n",
    "\n",
    "        # Set seed for reproducibility\n",
    "        torch.manual_seed(i)\n",
    "        model = TME(d1, d2, h)  # Initialize new model\n",
    "\n",
    "        # Train the model using your function\n",
    "        trained_model, best_val_loss = train_tme_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            device=device,\n",
    "            adam=adam,\n",
    "            **train_kwargs\n",
    "        )\n",
    "\n",
    "        # Save the model and its validation loss\n",
    "        ensemble.append(trained_model)\n",
    "        val_losses.append(best_val_loss)\n",
    "\n",
    "    return ensemble, val_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "13366c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 20.5962, Val Loss: 20.5597.      Train RMSE: 26094.3004, Val RMSE: 22026.0370\n",
      "Epoch 5.      Train Loss: 20.3187, Val Loss: 20.2868.      Train RMSE: 26094.2893, Val RMSE: 22026.0370\n",
      "Epoch 10.      Train Loss: 20.0209, Val Loss: 19.9941.      Train RMSE: 26094.2869, Val RMSE: 22026.0370\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m ensemble, losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_tme_ensemble\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43md1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource1_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# number of features of the 1st source\u001b[39;49;00m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43md2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource2_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# number of features of the 2nd source\u001b[39;49;00m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource1_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# lag length\u001b[39;49;00m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#20,\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml2_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43madam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[99], line 129\u001b[0m, in \u001b[0;36mtrain_tme_ensemble\u001b[1;34m(train_loader, val_loader, d1, d2, h, num_models, device, adam, **train_kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m model \u001b[38;5;241m=\u001b[39m TME(d1, d2, h)  \u001b[38;5;66;03m# Initialize new model\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;66;03m# Train the model using your function\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m trained_model, best_val_loss \u001b[38;5;241m=\u001b[39m train_tme_model(\n\u001b[0;32m    130\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    131\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m    132\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[0;32m    133\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m    134\u001b[0m     adam\u001b[38;5;241m=\u001b[39madam,\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_kwargs\n\u001b[0;32m    136\u001b[0m )\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Save the model and its validation loss\u001b[39;00m\n\u001b[0;32m    139\u001b[0m ensemble\u001b[38;5;241m.\u001b[39mappend(trained_model)\n",
      "Cell \u001b[1;32mIn[99], line 65\u001b[0m, in \u001b[0;36mtrain_tme_model\u001b[1;34m(model, train_loader, val_loader, lr, weight_decay, l2_lambda, max_epochs, patience, device, adam)\u001b[0m\n\u001b[0;32m     62\u001b[0m final_pred, mu1, var1, mu2, var2, probs \u001b[38;5;241m=\u001b[39m model(x1, x2, return_all\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     64\u001b[0m loss \u001b[38;5;241m=\u001b[39m tme_loss(y, mu1, var1, mu2, var2, probs, model, l2_lambda\u001b[38;5;241m=\u001b[39ml2_lambda)\n\u001b[1;32m---> 65\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), max_norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10.0\u001b[39m)\n\u001b[0;32m     67\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\aliak\\my_space\\ETH\\ML Complex Systems\\MLFCS\\ML_fin_project\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aliak\\my_space\\ETH\\ML Complex Systems\\MLFCS\\ML_fin_project\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aliak\\my_space\\ETH\\ML Complex Systems\\MLFCS\\ML_fin_project\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "ensemble, losses = train_tme_ensemble(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    d1=source1_tensor.shape[1],  # number of features of the 1st source\n",
    "    d2=source2_tensor.shape[1],  # number of features of the 2nd source\n",
    "    h=source1_tensor.shape[2],  # lag length\n",
    "    num_models=1,#20,\n",
    "    device=device,\n",
    "    lr=5e-5,\n",
    "    weight_decay=0.1,\n",
    "    l2_lambda=0.1,\n",
    "    max_epochs=60,\n",
    "    patience=10,\n",
    "    adam=False\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b9ad404d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(4.4581513)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(np.max(y_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "70ee65ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Density'>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "18afcb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.2496799700817123]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "4c959971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.3521,  1.3211,  1.3178,  1.3222,  1.3254,  1.3046,  1.3077,  1.3080,\n",
      "         1.2940,  1.2928,  1.2996,  1.2921,  1.2883,  1.2883,  1.2999,  1.3254,\n",
      "         1.3422,  1.3824,  1.3951,  1.4215,  1.3567,  1.3969,  1.3317,  1.2888,\n",
      "         1.2936,  1.2881,  1.2891,  1.2894,  1.2836,  1.2826,  1.2769,  1.2754,\n",
      "         1.2757,  1.2637,  1.2650,  1.2649,  1.2700,  1.2666,  1.2639,  1.2663,\n",
      "         1.2670,  1.2661,  1.2687,  1.2690,  1.2718,  1.2773,  1.2855,  1.2826,\n",
      "         1.2785,  1.2792,  1.2776,  1.2762,  1.2785,  1.2830,  1.2872,  1.2896,\n",
      "         1.2855,  1.2836,  1.2875,  1.2791,  1.2727,  1.2686,  1.2696,  1.2669,\n",
      "         1.2681,  1.2871,  1.2758,  1.2762,  1.2800,  1.2704,  1.2776,  1.3085,\n",
      "         1.3114,  1.3275,  1.3363,  1.2781,  1.2769,  1.2858,  1.2610,  1.2600,\n",
      "         1.2673,  1.2840,  1.3105,  1.3159,  1.3189,  1.3206,  1.3142,  1.3120,\n",
      "         1.3105,  1.3123,  1.3122,  1.3129,  1.3121,  1.3107,  1.3102,  1.3119,\n",
      "         1.3200,  1.3212,  1.3223,  1.3117,  1.3082,  1.3112,  1.3154,  1.3128,\n",
      "         1.3154,  1.3188,  1.3076,  1.3068,  1.3042,  1.2958,  1.2936,  1.2861,\n",
      "         1.2837,  1.2803,  1.2818,  1.2862,  1.2851,  1.2840,  1.2921,  1.3138,\n",
      "         1.3151,  1.3147,  1.3095,  1.3005,  1.2956,  1.2879,  1.2803,  1.2821,\n",
      "         1.2872,  1.2958,  1.3108,  1.3361,  1.3624,  1.3189,  1.3403,  1.3214,\n",
      "         1.3002,  1.3082,  1.3154,  1.3041,  1.3020,  1.2865,  1.2860,  1.2895,\n",
      "         1.2998,  1.2975,  1.3013,  1.3024,  1.3033,  1.2978,  1.2940,  1.2963,\n",
      "         1.2956,  1.2940,  1.2970,  1.2895,  1.2926,  1.2949,  1.2941,  1.2989,\n",
      "         1.2996,  1.2983,  1.2982,  1.3011,  1.3031,  1.3022,  1.2988,  1.2897,\n",
      "         1.2915,  1.2939,  1.2892,  1.2966,  1.2990,  1.2724,  1.2731,  1.2780,\n",
      "         1.2819,  1.2890,  1.2868,  1.2905,  1.2890,  1.2918,  1.2927,  1.3001,\n",
      "         1.2978,  1.2987,  1.3009,  1.3041,  1.3101,  1.3102,  1.3072,  1.3084,\n",
      "         1.2983,  1.2973,  1.2942,  1.2888,  1.2852,  1.2832,  1.2863,  1.2779,\n",
      "         1.2812,  1.2840,  1.3007,  1.2838,  1.2789,  1.2786,  1.4932, 14.8950,\n",
      "        14.0570,  7.2995,  4.5868,  3.4877,  2.8313,  2.7386,  2.4643,  2.4766,\n",
      "         4.0122, 12.4162, 22.2993, 21.4213, 14.7398, 11.8351, 10.5168, 10.0586,\n",
      "         9.9083,  9.9136,  9.3536,  8.4711, 11.8018,  4.1155,  1.7780,  1.9165,\n",
      "         1.9942,  2.0665,  2.1256,  2.1474,  2.2496,  1.8382,  1.5722,  1.5396,\n",
      "         1.4390,  1.3231,  1.3173,  1.3176,  1.3142,  1.3188,  1.3043,  1.3115,\n",
      "         1.3196,  1.3031,  1.3050,  1.3050,  1.3050,  1.3004,  1.2950,  1.3049],\n",
      "       dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 39\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# print(f\"📊 Ensemble Test RMSE: {rmse:.4f}\")\u001b[39;00m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# print(f\"📊 Ensemble Test MAE: {mae:.4f}\")\u001b[39;00m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_preds, y_preds_median, rmse, mae\n\u001b[1;32m---> 39\u001b[0m y_preds, y_preds_median, rmse, mae \u001b[38;5;241m=\u001b[39m evaluate_tme_ensemble(ensemble, test_loader, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "def evaluate_tme_ensemble(ensemble, test_loader, all_preds=False, device='cpu'):\n",
    "    all_preds = []\n",
    "    all_preds_median = []\n",
    "    y_trues = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, y in test_loader:\n",
    "            x1, x2, y = x1.to(device).to(torch.float64), x2.to(device).to(torch.float64), y.to(device).to(torch.float64)\n",
    "            batch_preds = []\n",
    "\n",
    "            for model in ensemble:\n",
    "                model.eval()\n",
    "                model.to(device).to(torch.float64)\n",
    "                pred = model(x1, x2)\n",
    "                print(pred)\n",
    "                return\n",
    "                batch_preds.append(pred.cpu())\n",
    "\n",
    "            # Average predictions from all models\n",
    "            avg_pred = torch.stack(batch_preds).mean(dim=0)\n",
    "            median_pred = torch.stack(batch_preds).median(dim=0).values\n",
    "            # avg_pred = torch.stack(batch_preds).median(dim=0)\n",
    "            all_preds.append(avg_pred)\n",
    "            all_preds_median.append(median_pred)\n",
    "            y_trues.append(y.cpu())\n",
    "\n",
    "    y_preds = torch.cat(all_preds).numpy()\n",
    "    y_preds_median = torch.cat(all_preds_median).numpy()\n",
    "    y_trues = torch.cat(y_trues).numpy()\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_trues, y_preds))\n",
    "    mae = mean_absolute_error(y_trues, y_preds)\n",
    "\n",
    "    # print(f\"📊 Ensemble Test RMSE: {rmse:.4f}\")\n",
    "    # print(f\"📊 Ensemble Test MAE: {mae:.4f}\")\n",
    "    return y_preds, y_preds_median, rmse, mae\n",
    "\n",
    "\n",
    "y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, test_loader, device='cpu')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ded3c3",
   "metadata": {},
   "source": [
    "Some things to consider:  \n",
    "    \n",
    "    - Model training takes a long time (full 20 model ensemble took me more than 3 hours)  \n",
    "    - For hyperparams tuning we may use the smaller ensemble  \n",
    "    - Adam vs SGD?  \n",
    "    - Now I am clamping the values of the predicted variances and means when training and predicting (to avoid numerical blow up)  \n",
    "    - The RMSE and MAE I got are reasonable (they are not extremely different). This is just an indication that probably the calculations are doing what they are supposed to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ff569efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.35205115, 1.32105566, 1.31779135, ..., 1.27115033, 1.28265187,\n",
       "       1.3009004 ])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e0a1da47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Density'>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d8c8b798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31191"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9885dac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(22.588381745332175), 10.671468513748367)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_final = np.array(y_preds*df.iloc[-len(y_preds):]['mean_volume'])\n",
    "y_true_final = df.iloc[-len(y_preds):]['total_volume'].values\n",
    "np.sqrt(mean_squared_error(y_true_final, y_preds_final)), mean_absolute_error(y_true_final, y_preds_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdb1e78",
   "metadata": {},
   "source": [
    "### Hyperparams Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32300bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 128, 0.001, 0.1], [10, 128, 0.001, 1.0], [10, 128, 0.001, 3.0], [10, 128, 0.001, 5.0], [10, 128, 0.0001, 0.1], [10, 128, 0.0001, 1.0], [10, 128, 0.0001, 3.0], [10, 128, 0.0001, 5.0], [10, 128, 0.0005, 0.1], [10, 128, 0.0005, 1.0], [10, 128, 0.0005, 3.0], [10, 128, 0.0005, 5.0], [10, 128, 5e-05, 0.1], [10, 128, 5e-05, 1.0], [10, 128, 5e-05, 3.0], [10, 128, 5e-05, 5.0], [10, 256, 5e-05, 0.1], [10, 64, 0.001, 0.1], [10, 64, 0.001, 1.0], [10, 64, 0.001, 3.0], [10, 64, 0.001, 5.0], [10, 64, 0.0001, 0.1], [10, 64, 0.0001, 1.0], [10, 64, 0.0001, 3.0], [10, 64, 0.0001, 5.0], [10, 64, 0.0005, 0.1], [10, 64, 0.0005, 1.0], [10, 64, 0.0005, 3.0], [10, 64, 0.0005, 5.0], [10, 64, 5e-05, 0.1], [10, 64, 5e-05, 1.0], [10, 64, 5e-05, 3.0], [10, 64, 5e-05, 5.0]]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing your saved results\n",
    "input_dir = 'validation_results'\n",
    "# Make sure the folder exists\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "\n",
    "# Prepare a list to collect existing files (i.e. corresponding parameters have been already validated)\n",
    "existing_results = []\n",
    "\n",
    "# Loop over all files in the directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        \n",
    "        # Open and load the JSON file\n",
    "        with open(filepath, 'r') as f:\n",
    "            result = json.load(f)\n",
    "        \n",
    "        # Parse hyperparameters from the filename\n",
    "        name_parts = filename.replace('.json', '').split('_')\n",
    "        h = int(name_parts[0][1:])  # strip the 'h'\n",
    "        batch_size = int(name_parts[1][5:])  # strip 'batch'\n",
    "        lr = float(name_parts[2][2:].replace('e', 'e'))  # scientific notation stays\n",
    "        l2_lambda = float(name_parts[3][6:])  # strip 'lambda'\n",
    "\n",
    "        existing_results.append([h,batch_size,lr,l2_lambda])\n",
    "\n",
    "print(existing_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "00a1bfc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 5e-05, l2_lambda:0.1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 20.4741, Val Loss: 20.3234.      Train RMSE: 26094.3102, Val RMSE: 22026.0370\n",
      "Epoch 5.      Train Loss: 19.5398, Val Loss: 19.4533.      Train RMSE: 27907.1202, Val RMSE: 43077.5182\n",
      "Epoch 10.      Train Loss: 8.0754, Val Loss: 8.6831.      Train RMSE: 3268761.9813, Val RMSE: 3268702.1540\n",
      "Epoch 15.      Train Loss: 3.8504, Val Loss: 0.4545.      Train RMSE: 2041449.2995, Val RMSE: 4.5623\n",
      "Epoch 20.      Train Loss: 0.7834, Val Loss: 0.0773.      Train RMSE: 8.0529, Val RMSE: 7.4328\n",
      "Epoch 25.      Train Loss: 0.6777, Val Loss: -0.0251.      Train RMSE: 16.3693, Val RMSE: 9.1773\n",
      "Epoch 30.      Train Loss: 0.9111, Val Loss: 0.8501.      Train RMSE: 435.0767, Val RMSE: 88.0917\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 20.6137, Val Loss: 20.4584.      Train RMSE: 26094.1177, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 19.6665, Val Loss: 19.5803.      Train RMSE: 27906.9700, Val RMSE: 50407.6667\n",
      "Epoch 10.      Train Loss: 8.2026, Val Loss: 8.8098.      Train RMSE: 3268746.9043, Val RMSE: 3268911.7854\n",
      "Epoch 15.      Train Loss: 7.8727, Val Loss: 6.2162.      Train RMSE: 3178423.4219, Val RMSE: 44.9418\n",
      "Epoch 20.      Train Loss: 1.7060, Val Loss: 2.4232.      Train RMSE: 30.9314, Val RMSE: 44.0077\n",
      "Epoch 25.      Train Loss: 0.8869, Val Loss: 0.1957.      Train RMSE: 28556.6060, Val RMSE: 263.1392\n",
      "Epoch 30.      Train Loss: 0.7520, Val Loss: 0.0596.      Train RMSE: 27827.4305, Val RMSE: 147.8826\n",
      "Epoch 35.      Train Loss: 0.5941, Val Loss: 0.1295.      Train RMSE: 29012.5611, Val RMSE: 341.0081\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 13.0835, Val Loss: 10.7380.      Train RMSE: 515776.8061, Val RMSE: 379342.8956\n",
      "Epoch 5.      Train Loss: 2.7186, Val Loss: 2.6027.      Train RMSE: 449717.3393, Val RMSE: 388340.5667\n",
      "Epoch 10.      Train Loss: 1.4413, Val Loss: 1.2567.      Train RMSE: 196111.4250, Val RMSE: 165304.4303\n",
      "Epoch 15.      Train Loss: 1.0408, Val Loss: 0.8687.      Train RMSE: 39386.0158, Val RMSE: 1477.7308\n",
      "Epoch 20.      Train Loss: 0.9233, Val Loss: 0.7465.      Train RMSE: 9895.5958, Val RMSE: 87.2915\n",
      "Epoch 25.      Train Loss: 0.8030, Val Loss: 0.3306.      Train RMSE: 29.1337, Val RMSE: 24.0928\n",
      "Epoch 30.      Train Loss: 0.7626, Val Loss: 0.2852.      Train RMSE: 29.9364, Val RMSE: 23.4194\n",
      "Epoch 35.      Train Loss: 0.5547, Val Loss: -0.0257.      Train RMSE: 46.6692, Val RMSE: 23.1828\n",
      "Epoch 40.      Train Loss: 0.5133, Val Loss: -0.0343.      Train RMSE: 669.5296, Val RMSE: 34.0496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [15:09, 909.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 5e-05, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 38.9575, Val Loss: 37.4467.      Train RMSE: 26094.3102, Val RMSE: 22026.0370\n",
      "Epoch 5.      Train Loss: 29.6138, Val Loss: 28.7583.      Train RMSE: 27907.1202, Val RMSE: 43077.5182\n",
      "Epoch 10.      Train Loss: 9.0426, Val Loss: 7.3361.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 15.      Train Loss: 4.5675, Val Loss: 2.2142.      Train RMSE: 2.4168, Val RMSE: 2.6712\n",
      "Epoch 20.      Train Loss: 2.1990, Val Loss: 1.4052.      Train RMSE: 6.3969, Val RMSE: 5.7388\n",
      "Epoch 25.      Train Loss: 1.4383, Val Loss: 0.7326.      Train RMSE: 4.1630, Val RMSE: 3.8960\n",
      "Epoch 30.      Train Loss: 1.0375, Val Loss: 0.3995.      Train RMSE: 4.0775, Val RMSE: 3.3441\n",
      "Epoch 35.      Train Loss: 1.3006, Val Loss: 1.7532.      Train RMSE: 82.3931, Val RMSE: 82.7278\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 40.3564, Val Loss: 38.8113.      Train RMSE: 26094.1177, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 30.8845, Val Loss: 30.0300.      Train RMSE: 27906.9700, Val RMSE: 50407.6667\n",
      "Epoch 10.      Train Loss: 13.8160, Val Loss: 14.0476.      Train RMSE: 3268432.5166, Val RMSE: 3268911.7854\n",
      "Epoch 15.      Train Loss: 10.8057, Val Loss: 11.2553.      Train RMSE: 3268791.9748, Val RMSE: 3268911.7854\n",
      "Epoch 20.      Train Loss: 8.0606, Val Loss: 5.2764.      Train RMSE: 2780989.2598, Val RMSE: 24.6205\n",
      "Epoch 25.      Train Loss: 1.9893, Val Loss: 1.2888.      Train RMSE: 49.3198, Val RMSE: 54.7111\n",
      "Epoch 30.      Train Loss: 1.4131, Val Loss: 0.8752.      Train RMSE: 14.3931, Val RMSE: 12.0163\n",
      "Epoch 35.      Train Loss: 1.1212, Val Loss: 0.6230.      Train RMSE: 7.0242, Val RMSE: 8.3539\n",
      "Epoch 40.      Train Loss: 0.9248, Val Loss: 0.5266.      Train RMSE: 6.8180, Val RMSE: 13.6129\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 27.6964, Val Loss: 24.3181.      Train RMSE: 513973.8114, Val RMSE: 374865.8053\n",
      "Epoch 5.      Train Loss: 9.6783, Val Loss: 8.9624.      Train RMSE: 371801.5099, Val RMSE: 292234.6665\n",
      "Epoch 10.      Train Loss: 3.8285, Val Loss: 3.8050.      Train RMSE: 91763.2934, Val RMSE: 70610.9494\n",
      "Epoch 15.      Train Loss: 1.9098, Val Loss: 2.1857.      Train RMSE: 344.7555, Val RMSE: 94.3917\n",
      "Epoch 20.      Train Loss: 1.1133, Val Loss: 0.6337.      Train RMSE: 36.6525, Val RMSE: 29.9164\n",
      "Epoch 25.      Train Loss: 0.8244, Val Loss: 0.3794.      Train RMSE: 27.5073, Val RMSE: 35.0335\n",
      "Epoch 30.      Train Loss: 0.6258, Val Loss: 0.1482.      Train RMSE: 5.4045, Val RMSE: 6.9363\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [29:47, 890.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 5e-05, l2_lambda:3\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 79.9817, Val Loss: 75.3538.      Train RMSE: 26094.3102, Val RMSE: 22026.0370\n",
      "Epoch 5.      Train Loss: 50.6597, Val Loss: 47.9558.      Train RMSE: 27907.1202, Val RMSE: 43077.5182\n",
      "Epoch 10.      Train Loss: 16.9729, Val Loss: 14.4516.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 15.      Train Loss: 6.8724, Val Loss: 5.9541.      Train RMSE: 2.4297, Val RMSE: 3.2461\n",
      "Epoch 20.      Train Loss: 4.6677, Val Loss: 3.7601.      Train RMSE: 3.5739, Val RMSE: 3.4504\n",
      "Epoch 25.      Train Loss: 2.6226, Val Loss: 1.8189.      Train RMSE: 2.6433, Val RMSE: 2.7878\n",
      "Epoch 30.      Train Loss: 1.5018, Val Loss: 0.7819.      Train RMSE: 2.5664, Val RMSE: 2.6658\n",
      "Epoch 35.      Train Loss: 1.7131, Val Loss: 2.5838.      Train RMSE: 78.8393, Val RMSE: 66.1038\n",
      "Epoch 40.      Train Loss: 0.9840, Val Loss: 0.2277.      Train RMSE: 5.0415, Val RMSE: 3.4207\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 84.1800, Val Loss: 79.4537.      Train RMSE: 26094.1177, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 54.5607, Val Loss: 51.8799.      Train RMSE: 27906.9700, Val RMSE: 50407.6667\n",
      "Epoch 10.      Train Loss: 24.6216, Val Loss: 24.0482.      Train RMSE: 3268567.2579, Val RMSE: 3268911.7854\n",
      "Epoch 15.      Train Loss: 15.8823, Val Loss: 13.2970.      Train RMSE: 3126144.8779, Val RMSE: 40.6939\n",
      "Epoch 20.      Train Loss: 6.5120, Val Loss: 7.1444.      Train RMSE: 29.3369, Val RMSE: 38.4522\n",
      "Epoch 25.      Train Loss: 4.0762, Val Loss: 3.3142.      Train RMSE: 44.9253, Val RMSE: 44.1598\n",
      "Epoch 30.      Train Loss: 2.8452, Val Loss: 2.2785.      Train RMSE: 9.8123, Val RMSE: 8.1317\n",
      "Epoch 35.      Train Loss: 2.0478, Val Loss: 1.5477.      Train RMSE: 6.3229, Val RMSE: 12.6390\n",
      "Epoch 40.      Train Loss: 1.4751, Val Loss: 1.0568.      Train RMSE: 6.1949, Val RMSE: 14.7152\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 59.9734, Val Loss: 54.0299.      Train RMSE: 521031.2914, Val RMSE: 380072.7604\n",
      "Epoch 5.      Train Loss: 24.7313, Val Loss: 22.4096.      Train RMSE: 357829.9352, Val RMSE: 247406.8489\n",
      "Epoch 10.      Train Loss: 7.6423, Val Loss: 7.4949.      Train RMSE: 48519.0470, Val RMSE: 5878.8754\n",
      "Epoch 15.      Train Loss: 3.1110, Val Loss: 3.6922.      Train RMSE: 103.7572, Val RMSE: 81.1712\n",
      "Epoch 20.      Train Loss: 2.3389, Val Loss: 1.6588.      Train RMSE: 89.2870, Val RMSE: 66.6786\n",
      "Epoch 25.      Train Loss: 1.5627, Val Loss: 0.8426.      Train RMSE: 19662.3881, Val RMSE: 7.6423\n",
      "Epoch 30.      Train Loss: 0.8218, Val Loss: 0.3206.      Train RMSE: 6.5492, Val RMSE: 6.8480\n",
      "Epoch 35.      Train Loss: 0.6706, Val Loss: 0.1638.      Train RMSE: 24279.3081, Val RMSE: 6.4410\n",
      "Epoch 40.      Train Loss: 0.6368, Val Loss: 0.1623.      Train RMSE: 34.4097, Val RMSE: 8.2388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [45:53, 925.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 5e-05, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 121.0226, Val Loss: 113.3092.      Train RMSE: 26094.3102, Val RMSE: 22026.0370\n",
      "Epoch 5.      Train Loss: 72.1526, Val Loss: 67.6469.      Train RMSE: 27907.1202, Val RMSE: 43077.5182\n",
      "Epoch 10.      Train Loss: 25.5449, Val Loss: 22.2008.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 15.      Train Loss: 10.5770, Val Loss: 9.4844.      Train RMSE: 2.3433, Val RMSE: 2.6631\n",
      "Epoch 20.      Train Loss: 6.4414, Val Loss: 5.3036.      Train RMSE: 2.5908, Val RMSE: 2.8522\n",
      "Epoch 25.      Train Loss: 3.1789, Val Loss: 2.3032.      Train RMSE: 2.2769, Val RMSE: 2.7162\n",
      "Epoch 30.      Train Loss: 1.6947, Val Loss: 0.9270.      Train RMSE: 2.2821, Val RMSE: 2.6663\n",
      "Epoch 35.      Train Loss: 1.3178, Val Loss: 0.4925.      Train RMSE: 139.4437, Val RMSE: 7.4997\n",
      "Epoch 40.      Train Loss: 0.6442, Val Loss: 0.1651.      Train RMSE: 23201.8675, Val RMSE: 6.5423\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 128.0199, Val Loss: 120.1433.      Train RMSE: 26094.1177, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 78.6545, Val Loss: 74.1870.      Train RMSE: 27906.9700, Val RMSE: 50407.6667\n",
      "Epoch 10.      Train Loss: 35.9824, Val Loss: 34.5945.      Train RMSE: 3268567.2579, Val RMSE: 3268911.7854\n",
      "Epoch 15.      Train Loss: 20.3179, Val Loss: 18.5069.      Train RMSE: 1849245.2687, Val RMSE: 30.7725\n",
      "Epoch 20.      Train Loss: 9.5468, Val Loss: 10.0404.      Train RMSE: 27.5331, Val RMSE: 35.5007\n",
      "Epoch 25.      Train Loss: 6.0050, Val Loss: 5.2509.      Train RMSE: 32.2256, Val RMSE: 24.5633\n",
      "Epoch 30.      Train Loss: 4.1018, Val Loss: 3.4652.      Train RMSE: 8.2383, Val RMSE: 6.5323\n",
      "Epoch 35.      Train Loss: 2.7813, Val Loss: 2.2391.      Train RMSE: 6.3394, Val RMSE: 12.3999\n",
      "Epoch 40.      Train Loss: 1.8899, Val Loss: 1.4281.      Train RMSE: 6.2850, Val RMSE: 13.8220\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 92.0685, Val Loss: 83.3879.      Train RMSE: 516496.6699, Val RMSE: 374317.2394\n",
      "Epoch 5.      Train Loss: 39.5052, Val Loss: 35.6933.      Train RMSE: 361858.5355, Val RMSE: 235233.6694\n",
      "Epoch 10.      Train Loss: 11.7870, Val Loss: 11.2100.      Train RMSE: 43054.0602, Val RMSE: 3507.3567\n",
      "Epoch 15.      Train Loss: 3.9957, Val Loss: 4.7391.      Train RMSE: 90.3651, Val RMSE: 64.0347\n",
      "Epoch 20.      Train Loss: 2.3292, Val Loss: 3.3650.      Train RMSE: 91.1067, Val RMSE: 63.5980\n",
      "Epoch 25.      Train Loss: 0.8250, Val Loss: 0.2996.      Train RMSE: 6.3254, Val RMSE: 6.1439\n",
      "Epoch 30.      Train Loss: 0.6498, Val Loss: 0.1774.      Train RMSE: 6.9882, Val RMSE: 7.6819\n",
      "Epoch 35.      Train Loss: 0.6290, Val Loss: 0.1347.      Train RMSE: 7.7734, Val RMSE: 6.9019\n",
      "Epoch 40.      Train Loss: 0.6275, Val Loss: 0.1526.      Train RMSE: 6.8838, Val RMSE: 7.8768\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [1:01:42, 934.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0001, l2_lambda:0.1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 20.3312, Val Loss: 20.0594.      Train RMSE: 26094.4046, Val RMSE: 34211.2459\n",
      "Epoch 5.      Train Loss: 5.8933, Val Loss: 3.3177.      Train RMSE: 1918304.8730, Val RMSE: 2.8935\n",
      "Epoch 10.      Train Loss: 0.8873, Val Loss: 0.1301.      Train RMSE: 9.4223, Val RMSE: 9.8211\n",
      "Epoch 15.      Train Loss: 1.1005, Val Loss: 1.1394.      Train RMSE: 9917.9349, Val RMSE: 86.1347\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 20.4679, Val Loss: 20.1902.      Train RMSE: 26094.1177, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 8.2775, Val Loss: 8.8400.      Train RMSE: 3268716.9105, Val RMSE: 3268911.7854\n",
      "Epoch 10.      Train Loss: 7.7826, Val Loss: 8.4097.      Train RMSE: 3268971.9296, Val RMSE: 3268911.7854\n",
      "Epoch 15.      Train Loss: 0.8384, Val Loss: 0.1171.      Train RMSE: 8977.8768, Val RMSE: 110.9080\n",
      "Epoch 20.      Train Loss: 0.7663, Val Loss: 0.0324.      Train RMSE: 7986.8537, Val RMSE: 138.0763\n",
      "Epoch 25.      Train Loss: 0.6229, Val Loss: -0.0535.      Train RMSE: 27688.4450, Val RMSE: 338.9997\n",
      "Epoch 30.      Train Loss: 0.6087, Val Loss: -0.0740.      Train RMSE: 11199.7493, Val RMSE: 41.6686\n",
      "Epoch 35.      Train Loss: 0.5309, Val Loss: 0.0369.      Train RMSE: 14935.2576, Val RMSE: 6.6232\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 11.0194, Val Loss: 7.3135.      Train RMSE: 528249.3782, Val RMSE: 431248.0519\n",
      "Epoch 5.      Train Loss: 1.5688, Val Loss: 1.3415.      Train RMSE: 244476.9340, Val RMSE: 182829.4677\n",
      "Epoch 10.      Train Loss: 0.9660, Val Loss: 0.7928.      Train RMSE: 10685.2770, Val RMSE: 95.4567\n",
      "Epoch 15.      Train Loss: 0.7598, Val Loss: 0.0534.      Train RMSE: 5221.6503, Val RMSE: 90.7781\n",
      "Epoch 20.      Train Loss: 0.6326, Val Loss: -0.0079.      Train RMSE: 9716.1622, Val RMSE: 208.5759\n",
      "Epoch 25.      Train Loss: 0.5132, Val Loss: -0.0175.      Train RMSE: 12344.3584, Val RMSE: 9.8956\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [1:12:28, 830.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0001, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 37.5285, Val Loss: 34.8130.      Train RMSE: 26094.4046, Val RMSE: 34211.2459\n",
      "Epoch 5.      Train Loss: 11.0073, Val Loss: 7.6520.      Train RMSE: 1920192.0928, Val RMSE: 2.8935\n",
      "Epoch 10.      Train Loss: 2.2190, Val Loss: 1.5559.      Train RMSE: 3.9367, Val RMSE: 5.8744\n",
      "Epoch 15.      Train Loss: 1.2269, Val Loss: 0.5601.      Train RMSE: 3.9868, Val RMSE: 3.5767\n",
      "Epoch 20.      Train Loss: 0.8773, Val Loss: 0.2032.      Train RMSE: 4.1385, Val RMSE: 3.2526\n",
      "Epoch 25.      Train Loss: 0.6217, Val Loss: 0.1719.      Train RMSE: 19087.7955, Val RMSE: 9.2141\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 38.8977, Val Loss: 36.1287.      Train RMSE: 26094.1177, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 14.5618, Val Loss: 14.3495.      Train RMSE: 3268627.2481, Val RMSE: 3268911.7854\n",
      "Epoch 10.      Train Loss: 9.6246, Val Loss: 10.0447.      Train RMSE: 3268957.4951, Val RMSE: 3268911.7854\n",
      "Epoch 15.      Train Loss: 1.7555, Val Loss: 1.0266.      Train RMSE: 72.9914, Val RMSE: 68.5115\n",
      "Epoch 20.      Train Loss: 1.0540, Val Loss: 0.5655.      Train RMSE: 8.2132, Val RMSE: 8.3182\n",
      "Epoch 25.      Train Loss: 0.7837, Val Loss: 0.3074.      Train RMSE: 6.7634, Val RMSE: 9.3579\n",
      "Epoch 30.      Train Loss: 0.6419, Val Loss: 0.0996.      Train RMSE: 10.5403, Val RMSE: 4.0027\n",
      "Epoch 35.      Train Loss: 0.6151, Val Loss: 0.1423.      Train RMSE: 5.7148, Val RMSE: 8.3215\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 24.6860, Val Loss: 19.0783.      Train RMSE: 525583.4308, Val RMSE: 411494.0409\n",
      "Epoch 5.      Train Loss: 4.1896, Val Loss: 3.8622.      Train RMSE: 117196.2462, Val RMSE: 80236.8302\n",
      "Epoch 10.      Train Loss: 1.4681, Val Loss: 1.8062.      Train RMSE: 95.4369, Val RMSE: 89.8910\n",
      "Epoch 15.      Train Loss: 0.8700, Val Loss: 0.4035.      Train RMSE: 37.6073, Val RMSE: 33.6530\n",
      "Epoch 20.      Train Loss: 0.7883, Val Loss: 3.2859.      Train RMSE: 6.8681, Val RMSE: 5.3926\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [1:24:13, 787.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0001, l2_lambda:3\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 75.5641, Val Loss: 67.0940.      Train RMSE: 26094.4058, Val RMSE: 34211.2459\n",
      "Epoch 5.      Train Loss: 18.3263, Val Loss: 14.3811.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 10.      Train Loss: 4.6709, Val Loss: 3.8414.      Train RMSE: 3.1255, Val RMSE: 3.3743\n",
      "Epoch 15.      Train Loss: 1.8985, Val Loss: 1.0491.      Train RMSE: 2.6452, Val RMSE: 2.7763\n",
      "Epoch 20.      Train Loss: 1.7207, Val Loss: 2.6163.      Train RMSE: 23797.1360, Val RMSE: 61.7839\n",
      "Epoch 25.      Train Loss: 0.6533, Val Loss: 0.1362.      Train RMSE: 6.4488, Val RMSE: 5.0249\n",
      "Epoch 30.      Train Loss: 0.6313, Val Loss: 0.2413.      Train RMSE: 10.6658, Val RMSE: 15.0411\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 79.6771, Val Loss: 71.0588.      Train RMSE: 26094.1177, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 26.0024, Val Loss: 24.0363.      Train RMSE: 3268462.6735, Val RMSE: 3268911.7854\n",
      "Epoch 10.      Train Loss: 6.8211, Val Loss: 7.0471.      Train RMSE: 28.7800, Val RMSE: 39.9621\n",
      "Epoch 15.      Train Loss: 3.2691, Val Loss: 2.4724.      Train RMSE: 38.4726, Val RMSE: 28.1163\n",
      "Epoch 20.      Train Loss: 1.7741, Val Loss: 1.1909.      Train RMSE: 6.7535, Val RMSE: 6.1160\n",
      "Epoch 25.      Train Loss: 1.0008, Val Loss: 0.5105.      Train RMSE: 6.5987, Val RMSE: 8.6093\n",
      "Epoch 30.      Train Loss: 0.6793, Val Loss: 0.1656.      Train RMSE: 7.2517, Val RMSE: 5.0365\n",
      "Epoch 35.      Train Loss: 0.6335, Val Loss: 0.1787.      Train RMSE: 7.6787, Val RMSE: 13.2700\n",
      "Epoch 40.      Train Loss: 0.6344, Val Loss: 0.1574.      Train RMSE: 7.9864, Val RMSE: 11.8410\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 54.5263, Val Loss: 44.1251.      Train RMSE: 533966.2335, Val RMSE: 402716.1745\n",
      "Epoch 5.      Train Loss: 8.2234, Val Loss: 7.1799.      Train RMSE: 68233.6091, Val RMSE: 6025.9758\n",
      "Epoch 10.      Train Loss: 1.5954, Val Loss: 1.0841.      Train RMSE: 19920.9462, Val RMSE: 53.5802\n",
      "Epoch 15.      Train Loss: 0.7850, Val Loss: 0.2978.      Train RMSE: 12.6224, Val RMSE: 9.1877\n",
      "Epoch 20.      Train Loss: 0.6341, Val Loss: 0.2451.      Train RMSE: 31.5828, Val RMSE: 13.8558\n",
      "Epoch 25.      Train Loss: 0.6322, Val Loss: 0.1349.      Train RMSE: 8.3529, Val RMSE: 6.4961\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [1:37:29, 790.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0001, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 113.6600, Val Loss: 99.5434.      Train RMSE: 26094.4058, Val RMSE: 34211.2459\n",
      "Epoch 5.      Train Loss: 27.8001, Val Loss: 22.0833.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 10.      Train Loss: 7.0899, Val Loss: 5.9070.      Train RMSE: 2.3286, Val RMSE: 3.1504\n",
      "Epoch 15.      Train Loss: 2.6607, Val Loss: 2.9654.      Train RMSE: 34880.5101, Val RMSE: 2.9881\n",
      "Epoch 20.      Train Loss: 1.1460, Val Loss: 0.3837.      Train RMSE: 16.3703, Val RMSE: 3.2084\n",
      "Epoch 25.      Train Loss: 0.6343, Val Loss: 0.1897.      Train RMSE: 20235.0223, Val RMSE: 12.1833\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 120.5151, Val Loss: 106.1518.      Train RMSE: 26094.1177, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 38.2889, Val Loss: 34.5739.      Train RMSE: 3268522.3447, Val RMSE: 3268806.8912\n",
      "Epoch 10.      Train Loss: 9.9084, Val Loss: 9.7487.      Train RMSE: 27.7327, Val RMSE: 37.5983\n",
      "Epoch 15.      Train Loss: 4.5192, Val Loss: 3.7120.      Train RMSE: 19.6978, Val RMSE: 14.8767\n",
      "Epoch 20.      Train Loss: 2.3061, Val Loss: 1.6659.      Train RMSE: 6.9427, Val RMSE: 5.5625\n",
      "Epoch 25.      Train Loss: 1.1236, Val Loss: 0.6429.      Train RMSE: 6.7724, Val RMSE: 10.7199\n",
      "Epoch 30.      Train Loss: 0.6801, Val Loss: 0.1659.      Train RMSE: 7.2792, Val RMSE: 4.9895\n",
      "Epoch 35.      Train Loss: 0.6334, Val Loss: 0.1519.      Train RMSE: 7.6880, Val RMSE: 8.0024\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 84.0706, Val Loss: 68.6268.      Train RMSE: 530798.6686, Val RMSE: 393330.1918\n",
      "Epoch 5.      Train Loss: 13.0051, Val Loss: 10.7680.      Train RMSE: 67247.1834, Val RMSE: 3354.4794\n",
      "Epoch 10.      Train Loss: 2.0993, Val Loss: 5.1287.      Train RMSE: 22124.4466, Val RMSE: 5.1717\n",
      "Epoch 15.      Train Loss: 0.8485, Val Loss: 0.3299.      Train RMSE: 25.0125, Val RMSE: 15.3471\n",
      "Epoch 20.      Train Loss: 0.6373, Val Loss: 0.1964.      Train RMSE: 7.6603, Val RMSE: 8.9268\n",
      "Epoch 25.      Train Loss: 0.6311, Val Loss: 0.1576.      Train RMSE: 7.3615, Val RMSE: 7.9588\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [1:50:04, 779.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0005, l2_lambda:0.1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 18.1451, Val Loss: 8.7471.      Train RMSE: 1189213.1649, Val RMSE: 3268702.1540\n",
      "Epoch 5.      Train Loss: 2.6711, Val Loss: 8.2727.      Train RMSE: 1012600.1284, Val RMSE: 3269016.5158\n",
      "Epoch 10.      Train Loss: 0.8734, Val Loss: 0.6911.      Train RMSE: 2109.0152, Val RMSE: 175.1224\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 19.7071, Val Loss: 19.0656.      Train RMSE: 43076.6833, Val RMSE: 50452.3385\n",
      "Epoch 5.      Train Loss: 0.8992, Val Loss: 0.7069.      Train RMSE: 1972.4018, Val RMSE: 183.2853\n",
      "Epoch 10.      Train Loss: 0.8735, Val Loss: 0.6890.      Train RMSE: 2137.7282, Val RMSE: 288.1847\n",
      "Epoch 15.      Train Loss: 15.3130, Val Loss: 18.4345.      Train RMSE: 95.2885, Val RMSE: 2.8957\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 4.3358, Val Loss: 1.4200.      Train RMSE: 446580.7737, Val RMSE: 217624.2528\n",
      "Epoch 5.      Train Loss: 4.1557, Val Loss: 2.8493.      Train RMSE: 2.6501, Val RMSE: 2.8935\n",
      "Epoch 10.      Train Loss: 0.8735, Val Loss: 0.7000.      Train RMSE: 2629.5585, Val RMSE: 229.6754\n",
      "Epoch 15.      Train Loss: 0.8746, Val Loss: 0.6956.      Train RMSE: 2362.8893, Val RMSE: 157.5067\n",
      "Epoch 20.      Train Loss: 0.8735, Val Loss: 0.6840.      Train RMSE: 3130.6899, Val RMSE: 188.7574\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [1:56:58, 664.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0005, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 28.5678, Val Loss: 13.4145.      Train RMSE: 1189213.1649, Val RMSE: 3268702.1540\n",
      "Epoch 5.      Train Loss: 5.1929, Val Loss: 8.2296.      Train RMSE: 1845582.8864, Val RMSE: 3269016.5158\n",
      "Epoch 10.      Train Loss: 4.4053, Val Loss: 8.2274.      Train RMSE: 1156988.8209, Val RMSE: 3269016.5158\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 31.3002, Val Loss: 24.8892.      Train RMSE: 43076.6833, Val RMSE: 50452.3538\n",
      "Epoch 5.      Train Loss: 2.9682, Val Loss: 1.8299.      Train RMSE: 1462.0028, Val RMSE: 95.7392\n",
      "Epoch 10.      Train Loss: 1.2949, Val Loss: 1.6623.      Train RMSE: 111997.1337, Val RMSE: 91.0282\n",
      "Epoch 15.      Train Loss: 1.2702, Val Loss: 1.6653.      Train RMSE: 94.1463, Val RMSE: 91.6820\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 11.5182, Val Loss: 3.8858.      Train RMSE: 370164.5697, Val RMSE: 84748.5066\n",
      "Epoch 5.      Train Loss: 2.0447, Val Loss: 3.0159.      Train RMSE: 80.2678, Val RMSE: 2.8935\n",
      "Epoch 10.      Train Loss: 0.8974, Val Loss: 0.1728.      Train RMSE: 3.9231, Val RMSE: 3.3096\n",
      "Epoch 15.      Train Loss: 1.2919, Val Loss: 1.6850.      Train RMSE: 94.0903, Val RMSE: 89.5018\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [2:02:44, 566.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0005, l2_lambda:3\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 48.2216, Val Loss: 19.6573.      Train RMSE: 1689584.2520, Val RMSE: 3268806.8912\n",
      "Epoch 5.      Train Loss: 1.4805, Val Loss: 0.3671.      Train RMSE: 61.9177, Val RMSE: 20.1750\n",
      "Epoch 10.      Train Loss: 1.2650, Val Loss: 0.5001.      Train RMSE: 61.4609, Val RMSE: 4.0533\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 54.0513, Val Loss: 23.9762.      Train RMSE: 1062214.5535, Val RMSE: 3268911.7854\n",
      "Epoch 5.      Train Loss: 3.6243, Val Loss: 2.5628.      Train RMSE: 1067230.3799, Val RMSE: 92.8062\n",
      "Epoch 10.      Train Loss: 1.2379, Val Loss: 2.5705.      Train RMSE: 10881.4014, Val RMSE: 76.8681\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 26.4982, Val Loss: 6.7473.      Train RMSE: 364394.8347, Val RMSE: 5016.3184\n",
      "Epoch 5.      Train Loss: 1.2403, Val Loss: 0.3517.      Train RMSE: 7483.4781, Val RMSE: 11.4763\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [2:07:20, 477.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0005, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 69.4056, Val Loss: 21.8947.      Train RMSE: 983910.9808, Val RMSE: 2.8935\n",
      "Epoch 5.      Train Loss: 2.1835, Val Loss: 0.7747.      Train RMSE: 85065.5435, Val RMSE: 8.1210\n",
      "Epoch 10.      Train Loss: 1.3779, Val Loss: 0.3333.      Train RMSE: 78625.4229, Val RMSE: 4.3932\n",
      "Epoch 15.      Train Loss: 1.1274, Val Loss: 0.3247.      Train RMSE: 2.6744, Val RMSE: 2.7252\n",
      "Epoch 20.      Train Loss: 1.3403, Val Loss: 0.7238.      Train RMSE: 14920.6461, Val RMSE: 53.1775\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 78.5315, Val Loss: 34.4751.      Train RMSE: 1079675.0150, Val RMSE: 3268911.7854\n",
      "Epoch 5.      Train Loss: 2.3748, Val Loss: 1.3696.      Train RMSE: 81041.5400, Val RMSE: 33.2013\n",
      "Epoch 10.      Train Loss: 1.5460, Val Loss: 0.4551.      Train RMSE: 62542.4180, Val RMSE: 19.5862\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 41.6673, Val Loss: 10.1533.      Train RMSE: 366251.2159, Val RMSE: 2960.2524\n",
      "Epoch 5.      Train Loss: 1.2128, Val Loss: 0.3158.      Train RMSE: 151.1216, Val RMSE: 2.6798\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [2:13:02, 436.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.001, l2_lambda:0.1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 11.2998, Val Loss: 2.9523.      Train RMSE: 982029.3471, Val RMSE: 2.8935\n",
      "Epoch 5.      Train Loss: 5.3852, Val Loss: 0.7172.      Train RMSE: 2670189.1230, Val RMSE: 140.1011\n",
      "Epoch 10.      Train Loss: 4.1015, Val Loss: 2.8272.      Train RMSE: 2.4800, Val RMSE: 2.8935\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 14.1802, Val Loss: 8.4634.      Train RMSE: 2238072.1194, Val RMSE: 3268911.7854\n",
      "Epoch 5.      Train Loss: 8.0757, Val Loss: 18.4469.      Train RMSE: 2047.5728, Val RMSE: 2.8957\n",
      "Epoch 10.      Train Loss: 4.8503, Val Loss: 18.4540.      Train RMSE: 2343.0314, Val RMSE: 2.8957\n",
      "Epoch 15.      Train Loss: 8.9969, Val Loss: 18.4360.      Train RMSE: 4651.8022, Val RMSE: 2.8957\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 3.6904, Val Loss: 8.4187.      Train RMSE: 1200741.4678, Val RMSE: 3268911.7854\n",
      "Epoch 5.      Train Loss: 4.5790, Val Loss: 8.2282.      Train RMSE: 1189036.0875, Val RMSE: 3269016.1951\n",
      "Epoch 10.      Train Loss: 0.8993, Val Loss: 0.7063.      Train RMSE: 3116.0924, Val RMSE: 334.8319\n",
      "Epoch 15.      Train Loss: 0.9836, Val Loss: 0.6916.      Train RMSE: 2852.8215, Val RMSE: 142.1959\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [2:19:20, 418.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.001, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 17.7636, Val Loss: 4.0910.      Train RMSE: 982029.3471, Val RMSE: 2.8935\n",
      "Epoch 5.      Train Loss: 4.8073, Val Loss: 8.4774.      Train RMSE: 280632.6074, Val RMSE: 3269016.5158\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 21.7657, Val Loss: 10.5601.      Train RMSE: 2238364.9237, Val RMSE: 3269016.5158\n",
      "Epoch 5.      Train Loss: 3.6940, Val Loss: 1.6534.      Train RMSE: 121.4604, Val RMSE: 92.0319\n",
      "Epoch 10.      Train Loss: 2.9036, Val Loss: 1.6761.      Train RMSE: 1266384.5716, Val RMSE: 113.0471\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 7.9788, Val Loss: 8.6373.      Train RMSE: 1294057.1345, Val RMSE: 3268911.7854\n",
      "Epoch 5.      Train Loss: 2.7500, Val Loss: 1.7085.      Train RMSE: 316532.5611, Val RMSE: 104.3951\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [2:23:04, 359.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.001, l2_lambda:3\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 27.8356, Val Loss: 4.2710.      Train RMSE: 671408.5016, Val RMSE: 9.9493\n",
      "Epoch 5.      Train Loss: 4.5689, Val Loss: 2.4474.      Train RMSE: 1694827.8105, Val RMSE: 92.0126\n",
      "Epoch 10.      Train Loss: 4.6485, Val Loss: 8.2627.      Train RMSE: 2078598.4280, Val RMSE: 3269016.5158\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 32.8839, Val Loss: 6.5606.      Train RMSE: 2295649.8500, Val RMSE: 43.3798\n",
      "Epoch 5.      Train Loss: 3.8095, Val Loss: 3.3359.      Train RMSE: 1450053.8288, Val RMSE: 26.6484\n",
      "Epoch 10.      Train Loss: 3.7503, Val Loss: 2.5630.      Train RMSE: 1576729.7494, Val RMSE: 84.1035\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 14.8765, Val Loss: 3.4595.      Train RMSE: 251289.7552, Val RMSE: 2.8935\n",
      "Epoch 5.      Train Loss: 2.5626, Val Loss: 2.4903.      Train RMSE: 236.6699, Val RMSE: 90.2086\n",
      "Epoch 10.      Train Loss: 2.8857, Val Loss: 2.5717.      Train RMSE: 981292.0794, Val RMSE: 82.6428\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [2:27:31, 331.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.001, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 39.9619, Val Loss: 4.4807.      Train RMSE: 671408.5016, Val RMSE: 7.2145\n",
      "Epoch 5.      Train Loss: 3.0489, Val Loss: 3.1129.      Train RMSE: 244273.4876, Val RMSE: 52.4092\n",
      "Epoch 10.      Train Loss: 2.4408, Val Loss: 0.3367.      Train RMSE: 868073.9555, Val RMSE: 2.7645\n",
      "Epoch 15.      Train Loss: 4.4052, Val Loss: 0.6314.      Train RMSE: 1648842.5615, Val RMSE: 2.8126\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 47.8367, Val Loss: 8.8067.      Train RMSE: 2295645.0540, Val RMSE: 38.4418\n",
      "Epoch 5.      Train Loss: 3.1012, Val Loss: 2.9433.      Train RMSE: 826516.7648, Val RMSE: 2.8935\n",
      "Epoch 10.      Train Loss: 3.9827, Val Loss: 3.0852.      Train RMSE: 1446720.6668, Val RMSE: 50.7981\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 23.6209, Val Loss: 3.4937.      Train RMSE: 1142643.3175, Val RMSE: 2.8935\n",
      "Epoch 5.      Train Loss: 5.0901, Val Loss: 2.8286.      Train RMSE: 2011997.2087, Val RMSE: 2.8935\n",
      "Epoch 10.      Train Loss: 4.9883, Val Loss: 8.2681.      Train RMSE: 2206599.4213, Val RMSE: 3269016.5158\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [2:32:39, 324.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 5e-05, l2_lambda:0.1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 20.5515, Val Loss: 20.4719.      Train RMSE: 26094.3053, Val RMSE: 22026.0370\n",
      "Epoch 5.      Train Loss: 19.9976, Val Loss: 19.9385.      Train RMSE: 26094.3960, Val RMSE: 34211.2459\n",
      "Epoch 10.      Train Loss: 19.4951, Val Loss: 19.4523.      Train RMSE: 27907.1191, Val RMSE: 43077.5182\n",
      "Epoch 15.      Train Loss: 11.5864, Val Loss: 3.5212.      Train RMSE: 1894503.1583, Val RMSE: 2.8935\n",
      "Epoch 20.      Train Loss: 4.5901, Val Loss: 3.2838.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 25.      Train Loss: 4.4173, Val Loss: 3.1153.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 30.      Train Loss: 1.2162, Val Loss: 0.3710.      Train RMSE: 2.9876, Val RMSE: 4.5261\n",
      "Epoch 35.      Train Loss: 0.8790, Val Loss: 0.1486.      Train RMSE: 11.3955, Val RMSE: 11.3971\n",
      "Epoch 40.      Train Loss: 0.7632, Val Loss: 0.0480.      Train RMSE: 8.0519, Val RMSE: 7.7842\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 20.6929, Val Loss: 20.6102.      Train RMSE: 26094.1177, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 20.1280, Val Loss: 20.0674.      Train RMSE: 24145.6487, Val RMSE: 50407.6667\n",
      "Epoch 10.      Train Loss: 19.6220, Val Loss: 19.5794.      Train RMSE: 27906.9700, Val RMSE: 50407.6667\n",
      "Epoch 15.      Train Loss: 8.4276, Val Loss: 9.0467.      Train RMSE: 3268731.9876, Val RMSE: 3268806.8912\n",
      "Epoch 20.      Train Loss: 8.1799, Val Loss: 8.8090.      Train RMSE: 3268911.6250, Val RMSE: 3268911.7854\n",
      "Epoch 25.      Train Loss: 8.0090, Val Loss: 8.6441.      Train RMSE: 3268881.6327, Val RMSE: 3268911.7854\n",
      "Epoch 30.      Train Loss: 7.8911, Val Loss: 8.5305.      Train RMSE: 3268986.3641, Val RMSE: 3268911.7854\n",
      "Epoch 35.      Train Loss: 7.8065, Val Loss: 8.4496.      Train RMSE: 3268986.3641, Val RMSE: 3268911.7854\n",
      "Epoch 40.      Train Loss: 3.7960, Val Loss: 3.8293.      Train RMSE: 1276674.4383, Val RMSE: 353361.8688\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 14.0456, Val Loss: 12.4075.      Train RMSE: 500901.6004, Val RMSE: 366207.7204\n",
      "Epoch 5.      Train Loss: 6.0007, Val Loss: 5.6820.      Train RMSE: 528025.3356, Val RMSE: 427496.6604\n",
      "Epoch 10.      Train Loss: 2.5033, Val Loss: 2.5021.      Train RMSE: 423783.1609, Val RMSE: 377650.1770\n",
      "Epoch 15.      Train Loss: 1.7651, Val Loss: 1.6491.      Train RMSE: 288488.5882, Val RMSE: 254440.2725\n",
      "Epoch 20.      Train Loss: 1.3592, Val Loss: 1.2019.      Train RMSE: 161266.5074, Val RMSE: 149740.3338\n",
      "Epoch 25.      Train Loss: 1.1298, Val Loss: 0.9652.      Train RMSE: 71656.1389, Val RMSE: 51121.0041\n",
      "Epoch 30.      Train Loss: 1.0082, Val Loss: 0.8437.      Train RMSE: 25622.1354, Val RMSE: 311.5095\n",
      "Epoch 35.      Train Loss: 0.9442, Val Loss: 0.7719.      Train RMSE: 9901.8261, Val RMSE: 95.8013\n",
      "Epoch 40.      Train Loss: 0.9052, Val Loss: 0.7280.      Train RMSE: 9918.3172, Val RMSE: 95.7594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [2:43:29, 422.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 5e-05, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 39.7323, Val Loss: 38.9324.      Train RMSE: 26094.3053, Val RMSE: 22026.0370\n",
      "Epoch 5.      Train Loss: 34.1924, Val Loss: 33.6041.      Train RMSE: 26094.3985, Val RMSE: 34211.2459\n",
      "Epoch 10.      Train Loss: 29.1679, Val Loss: 28.7487.      Train RMSE: 27907.1191, Val RMSE: 43077.5182\n",
      "Epoch 15.      Train Loss: 18.0672, Val Loss: 9.7276.      Train RMSE: 1905760.0148, Val RMSE: 2.8935\n",
      "Epoch 20.      Train Loss: 8.8120, Val Loss: 7.3211.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 25.      Train Loss: 7.1053, Val Loss: 5.6773.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 30.      Train Loss: 3.0054, Val Loss: 2.2618.      Train RMSE: 2.3733, Val RMSE: 3.2071\n",
      "Epoch 35.      Train Loss: 2.5817, Val Loss: 1.8373.      Train RMSE: 6.3240, Val RMSE: 6.6763\n",
      "Epoch 40.      Train Loss: 2.1229, Val Loss: 1.3945.      Train RMSE: 5.9946, Val RMSE: 5.7821\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 41.1486, Val Loss: 40.3292.      Train RMSE: 26094.1177, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 35.4984, Val Loss: 34.9009.      Train RMSE: 24145.6487, Val RMSE: 50407.6667\n",
      "Epoch 10.      Train Loss: 30.4387, Val Loss: 30.0205.      Train RMSE: 27906.9700, Val RMSE: 50407.6667\n",
      "Epoch 15.      Train Loss: 16.0611, Val Loss: 16.4159.      Train RMSE: 3268477.4310, Val RMSE: 3268806.8912\n",
      "Epoch 20.      Train Loss: 13.5929, Val Loss: 14.0393.      Train RMSE: 3268717.3917, Val RMSE: 3268911.7854\n",
      "Epoch 25.      Train Loss: 11.8820, Val Loss: 12.3896.      Train RMSE: 3268747.0647, Val RMSE: 3268911.7854\n",
      "Epoch 30.      Train Loss: 10.6979, Val Loss: 11.2492.      Train RMSE: 3268912.1062, Val RMSE: 3268911.7854\n",
      "Epoch 35.      Train Loss: 6.1116, Val Loss: 6.3391.      Train RMSE: 24.1574, Val RMSE: 25.1634\n",
      "Epoch 40.      Train Loss: 3.4237, Val Loss: 4.2293.      Train RMSE: 29.2133, Val RMSE: 38.5938\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 29.1739, Val Loss: 26.9849.      Train RMSE: 499065.1059, Val RMSE: 362283.3161\n",
      "Epoch 5.      Train Loss: 17.2131, Val Loss: 16.4008.      Train RMSE: 506344.8650, Val RMSE: 396610.7833\n",
      "Epoch 10.      Train Loss: 8.9664, Val Loss: 8.7701.      Train RMSE: 346830.5441, Val RMSE: 288022.0152\n",
      "Epoch 15.      Train Loss: 5.5651, Val Loss: 5.5438.      Train RMSE: 189758.2408, Val RMSE: 160610.8149\n",
      "Epoch 20.      Train Loss: 3.5846, Val Loss: 3.7272.      Train RMSE: 76190.6392, Val RMSE: 59469.8176\n",
      "Epoch 25.      Train Loss: 2.4480, Val Loss: 2.7065.      Train RMSE: 15564.0848, Val RMSE: 213.5551\n",
      "Epoch 30.      Train Loss: 1.8723, Val Loss: 2.1619.      Train RMSE: 99.2412, Val RMSE: 94.1247\n",
      "Epoch 35.      Train Loss: 1.5211, Val Loss: 1.8839.      Train RMSE: 94.5478, Val RMSE: 89.9906\n",
      "Epoch 40.      Train Loss: 1.2439, Val Loss: 0.6333.      Train RMSE: 69.3691, Val RMSE: 25.5216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [2:53:58, 484.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 5e-05, l2_lambda:3\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 82.3434, Val Loss: 79.9167.      Train RMSE: 26094.3053, Val RMSE: 22026.0370\n",
      "Epoch 5.      Train Loss: 65.1761, Val Loss: 63.3185.      Train RMSE: 26094.3985, Val RMSE: 34211.2459\n",
      "Epoch 10.      Train Loss: 49.2479, Val Loss: 47.9257.      Train RMSE: 27907.1191, Val RMSE: 43077.5182\n",
      "Epoch 15.      Train Loss: 23.8274, Val Loss: 21.6348.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 20.      Train Loss: 16.3059, Val Loss: 14.4191.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 25.      Train Loss: 11.4001, Val Loss: 9.7411.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 30.      Train Loss: 6.8132, Val Loss: 5.9548.      Train RMSE: 2.5790, Val RMSE: 3.4733\n",
      "Epoch 35.      Train Loss: 5.8093, Val Loss: 4.9704.      Train RMSE: 3.4970, Val RMSE: 3.4992\n",
      "Epoch 40.      Train Loss: 4.4768, Val Loss: 3.6903.      Train RMSE: 3.4915, Val RMSE: 3.4630\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 86.5930, Val Loss: 84.1106.      Train RMSE: 26094.1177, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 69.1136, Val Loss: 67.2344.      Train RMSE: 24145.6487, Val RMSE: 50407.6667\n",
      "Epoch 10.      Train Loss: 53.1593, Val Loss: 51.8501.      Train RMSE: 27906.9700, Val RMSE: 50407.6667\n",
      "Epoch 15.      Train Loss: 31.3641, Val Loss: 31.1238.      Train RMSE: 3268656.9219, Val RMSE: 3268911.7854\n",
      "Epoch 20.      Train Loss: 23.9688, Val Loss: 24.0211.      Train RMSE: 3268717.2313, Val RMSE: 3268911.7854\n",
      "Epoch 25.      Train Loss: 19.0277, Val Loss: 19.2773.      Train RMSE: 3268387.7620, Val RMSE: 3268911.7854\n",
      "Epoch 30.      Train Loss: 15.7151, Val Loss: 16.0979.      Train RMSE: 3268896.7091, Val RMSE: 3268911.7854\n",
      "Epoch 35.      Train Loss: 8.8961, Val Loss: 9.4064.      Train RMSE: 23.9669, Val RMSE: 28.2223\n",
      "Epoch 40.      Train Loss: 6.2547, Val Loss: 7.0573.      Train RMSE: 32.9067, Val RMSE: 40.8922\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 62.7180, Val Loss: 59.1841.      Train RMSE: 503071.0871, Val RMSE: 366171.5242\n",
      "Epoch 5.      Train Loss: 41.3120, Val Loss: 39.3482.      Train RMSE: 515032.4915, Val RMSE: 380201.3284\n",
      "Epoch 10.      Train Loss: 23.0074, Val Loss: 22.1221.      Train RMSE: 327633.5592, Val RMSE: 241072.5304\n",
      "Epoch 15.      Train Loss: 12.7410, Val Loss: 12.5209.      Train RMSE: 143599.6853, Val RMSE: 116578.3980\n",
      "Epoch 20.      Train Loss: 7.1364, Val Loss: 7.4114.      Train RMSE: 33679.0879, Val RMSE: 4243.8544\n",
      "Epoch 25.      Train Loss: 4.4030, Val Loss: 4.9280.      Train RMSE: 561.0996, Val RMSE: 99.9598\n",
      "Epoch 30.      Train Loss: 2.9834, Val Loss: 3.6781.      Train RMSE: 103.3080, Val RMSE: 81.5183\n",
      "Epoch 35.      Train Loss: 1.7802, Val Loss: 1.2869.      Train RMSE: 22287.8658, Val RMSE: 31.3582\n",
      "Epoch 40.      Train Loss: 1.3089, Val Loss: 0.8795.      Train RMSE: 10330.1469, Val RMSE: 42.7000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [3:04:23, 526.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 5e-05, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 124.9587, Val Loss: 120.9140.      Train RMSE: 26094.3053, Val RMSE: 22026.0370\n",
      "Epoch 5.      Train Loss: 96.3467, Val Loss: 93.2508.      Train RMSE: 26094.3985, Val RMSE: 34211.2459\n",
      "Epoch 10.      Train Loss: 69.7996, Val Loss: 67.5967.      Train RMSE: 27907.1191, Val RMSE: 43077.5182\n",
      "Epoch 15.      Train Loss: 36.9676, Val Loss: 34.1713.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 20.      Train Loss: 24.4333, Val Loss: 22.1469.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 25.      Train Loss: 16.2095, Val Loss: 14.2715.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 30.      Train Loss: 10.6029, Val Loss: 9.6955.      Train RMSE: 2.3520, Val RMSE: 2.6784\n",
      "Epoch 35.      Train Loss: 9.0273, Val Loss: 8.0676.      Train RMSE: 2.5149, Val RMSE: 3.1340\n",
      "Epoch 40.      Train Loss: 7.0207, Val Loss: 6.1168.      Train RMSE: 2.6930, Val RMSE: 3.0372\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 132.0416, Val Loss: 127.9048.      Train RMSE: 26094.1177, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 102.9092, Val Loss: 99.7779.      Train RMSE: 24145.6487, Val RMSE: 50407.6667\n",
      "Epoch 10.      Train Loss: 76.3188, Val Loss: 74.1373.      Train RMSE: 27906.9700, Val RMSE: 50407.6667\n",
      "Epoch 15.      Train Loss: 47.2141, Val Loss: 46.3826.      Train RMSE: 3268612.0100, Val RMSE: 3268911.7854\n",
      "Epoch 20.      Train Loss: 34.8913, Val Loss: 34.5481.      Train RMSE: 3268717.2313, Val RMSE: 3268911.7854\n",
      "Epoch 25.      Train Loss: 26.6558, Val Loss: 26.6438.      Train RMSE: 3268618.9072, Val RMSE: 3268911.7854\n",
      "Epoch 30.      Train Loss: 21.1346, Val Loss: 21.3347.      Train RMSE: 3268933.5978, Val RMSE: 3268911.7854\n",
      "Epoch 35.      Train Loss: 16.1653, Val Loss: 13.6457.      Train RMSE: 2741229.4478, Val RMSE: 18957.9839\n",
      "Epoch 40.      Train Loss: 10.8037, Val Loss: 11.5410.      Train RMSE: 113034.6586, Val RMSE: 67273.4342\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 96.2022, Val Loss: 91.2358.      Train RMSE: 501156.8620, Val RMSE: 365022.1808\n",
      "Epoch 5.      Train Loss: 64.7670, Val Loss: 61.6107.      Train RMSE: 529886.6386, Val RMSE: 378145.6018\n",
      "Epoch 10.      Train Loss: 36.9251, Val Loss: 35.3749.      Train RMSE: 335914.2291, Val RMSE: 238134.7206\n",
      "Epoch 15.      Train Loss: 20.3946, Val Loss: 19.8485.      Train RMSE: 148068.3120, Val RMSE: 121056.6217\n",
      "Epoch 20.      Train Loss: 10.9290, Val Loss: 11.1020.      Train RMSE: 32157.8139, Val RMSE: 3055.1244\n",
      "Epoch 25.      Train Loss: 5.9800, Val Loss: 6.6405.      Train RMSE: 115.4850, Val RMSE: 75.2235\n",
      "Epoch 30.      Train Loss: 3.5036, Val Loss: 2.6728.      Train RMSE: 9969.0192, Val RMSE: 29.5088\n",
      "Epoch 35.      Train Loss: 2.3568, Val Loss: 1.8413.      Train RMSE: 24235.9614, Val RMSE: 30.1108\n",
      "Epoch 40.      Train Loss: 1.7234, Val Loss: 1.1838.      Train RMSE: 22139.8797, Val RMSE: 30.5728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [3:14:47, 555.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0001, l2_lambda:0.1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 20.4740, Val Loss: 20.3230.      Train RMSE: 26094.3102, Val RMSE: 22026.0370\n",
      "Epoch 5.      Train Loss: 19.5510, Val Loss: 19.4671.      Train RMSE: 27907.1202, Val RMSE: 43077.5182\n",
      "Epoch 10.      Train Loss: 8.1059, Val Loss: 8.7145.      Train RMSE: 3268702.3144, Val RMSE: 3268597.2531\n",
      "Epoch 15.      Train Loss: 7.8039, Val Loss: 8.4325.      Train RMSE: 3268926.7013, Val RMSE: 3268911.7854\n",
      "Epoch 20.      Train Loss: 1.0881, Val Loss: 1.2638.      Train RMSE: 17137.3996, Val RMSE: 79.3855\n",
      "Epoch 25.      Train Loss: 0.7963, Val Loss: 0.0922.      Train RMSE: 78.9491, Val RMSE: 85.7032\n",
      "Epoch 30.      Train Loss: 0.7557, Val Loss: 0.0612.      Train RMSE: 142.6283, Val RMSE: 88.1427\n",
      "Epoch 35.      Train Loss: 0.7257, Val Loss: -0.0085.      Train RMSE: 969.3569, Val RMSE: 64.4690\n",
      "Epoch 40.      Train Loss: 0.6151, Val Loss: -0.0727.      Train RMSE: 22336.7371, Val RMSE: 177.4496\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 20.6137, Val Loss: 20.4581.      Train RMSE: 26094.1177, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 19.6782, Val Loss: 19.5943.      Train RMSE: 26094.2121, Val RMSE: 50407.6667\n",
      "Epoch 10.      Train Loss: 8.2304, Val Loss: 8.8387.      Train RMSE: 3268881.6327, Val RMSE: 3268911.7854\n",
      "Epoch 15.      Train Loss: 7.9259, Val Loss: 8.5550.      Train RMSE: 3268986.2037, Val RMSE: 3268911.7854\n",
      "Epoch 20.      Train Loss: 7.7697, Val Loss: 8.4086.      Train RMSE: 3268986.3641, Val RMSE: 3268911.7854\n",
      "Epoch 25.      Train Loss: 5.7208, Val Loss: 2.7477.      Train RMSE: 2634007.1765, Val RMSE: 24.7821\n",
      "Epoch 30.      Train Loss: 0.8134, Val Loss: 0.0953.      Train RMSE: 25489.3057, Val RMSE: 135.5529\n",
      "Epoch 35.      Train Loss: 0.7049, Val Loss: 0.0111.      Train RMSE: 29032.2778, Val RMSE: 179.4088\n",
      "Epoch 40.      Train Loss: 0.6523, Val Loss: -0.0443.      Train RMSE: 27904.6284, Val RMSE: 305.1962\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 13.0354, Val Loss: 10.6565.      Train RMSE: 512679.5490, Val RMSE: 375850.4390\n",
      "Epoch 5.      Train Loss: 2.6148, Val Loss: 2.5086.      Train RMSE: 440084.2346, Val RMSE: 380777.6262\n",
      "Epoch 10.      Train Loss: 1.4509, Val Loss: 1.2904.      Train RMSE: 201384.2554, Val RMSE: 170251.1936\n",
      "Epoch 15.      Train Loss: 1.0728, Val Loss: 0.9145.      Train RMSE: 49165.7194, Val RMSE: 3397.3710\n",
      "Epoch 20.      Train Loss: 0.9449, Val Loss: 0.7768.      Train RMSE: 9896.1310, Val RMSE: 85.8038\n",
      "Epoch 25.      Train Loss: 0.8180, Val Loss: 0.3602.      Train RMSE: 9894.2529, Val RMSE: 27.7223\n",
      "Epoch 30.      Train Loss: 0.7840, Val Loss: 0.3032.      Train RMSE: 31.0687, Val RMSE: 22.6155\n",
      "Epoch 35.      Train Loss: 0.7514, Val Loss: 0.3025.      Train RMSE: 29.9815, Val RMSE: 30.8672\n",
      "Epoch 40.      Train Loss: 0.7264, Val Loss: 0.3016.      Train RMSE: 30.2243, Val RMSE: 35.9278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [3:25:10, 576.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0001, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 38.9569, Val Loss: 37.4437.      Train RMSE: 26094.3102, Val RMSE: 22026.0370\n",
      "Epoch 5.      Train Loss: 29.7270, Val Loss: 28.8966.      Train RMSE: 27907.1202, Val RMSE: 43077.5182\n",
      "Epoch 10.      Train Loss: 12.8008, Val Loss: 13.0383.      Train RMSE: 3268866.8770, Val RMSE: 3268806.8912\n",
      "Epoch 15.      Train Loss: 6.3414, Val Loss: 4.8534.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 20.      Train Loss: 2.1606, Val Loss: 1.3952.      Train RMSE: 4.4286, Val RMSE: 5.5679\n",
      "Epoch 25.      Train Loss: 1.7247, Val Loss: 0.9727.      Train RMSE: 5.0741, Val RMSE: 4.6469\n",
      "Epoch 30.      Train Loss: 1.2720, Val Loss: 0.5490.      Train RMSE: 4.0380, Val RMSE: 3.4845\n",
      "Epoch 35.      Train Loss: 0.9748, Val Loss: 0.2911.      Train RMSE: 3.8131, Val RMSE: 3.2345\n",
      "Epoch 40.      Train Loss: 0.8675, Val Loss: 0.2202.      Train RMSE: 3.9682, Val RMSE: 3.3968\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 40.3558, Val Loss: 38.8082.      Train RMSE: 26094.1177, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 30.9998, Val Loss: 30.1697.      Train RMSE: 26094.2121, Val RMSE: 50407.6667\n",
      "Epoch 10.      Train Loss: 14.1036, Val Loss: 14.3414.      Train RMSE: 3268761.9813, Val RMSE: 3268911.7854\n",
      "Epoch 15.      Train Loss: 11.0652, Val Loss: 11.5047.      Train RMSE: 3268612.9724, Val RMSE: 3268911.7854\n",
      "Epoch 20.      Train Loss: 9.4997, Val Loss: 10.0390.      Train RMSE: 3268986.3641, Val RMSE: 3268911.4646\n",
      "Epoch 25.      Train Loss: 2.4231, Val Loss: 2.9075.      Train RMSE: 61.4492, Val RMSE: 72.3617\n",
      "Epoch 30.      Train Loss: 1.9068, Val Loss: 2.2457.      Train RMSE: 88.0863, Val RMSE: 89.7966\n",
      "Epoch 35.      Train Loss: 1.3556, Val Loss: 0.7421.      Train RMSE: 83.1163, Val RMSE: 74.8643\n",
      "Epoch 40.      Train Loss: 1.1539, Val Loss: 0.4890.      Train RMSE: 15.4771, Val RMSE: 10.9310\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 27.6461, Val Loss: 24.2263.      Train RMSE: 510579.3731, Val RMSE: 372722.0437\n",
      "Epoch 5.      Train Loss: 9.3257, Val Loss: 8.6084.      Train RMSE: 364099.4480, Val RMSE: 286464.7149\n",
      "Epoch 10.      Train Loss: 3.7792, Val Loss: 3.7864.      Train RMSE: 97051.1360, Val RMSE: 75288.6252\n",
      "Epoch 15.      Train Loss: 1.9802, Val Loss: 2.2553.      Train RMSE: 9866.7056, Val RMSE: 104.4559\n",
      "Epoch 20.      Train Loss: 1.4268, Val Loss: 1.7938.      Train RMSE: 94.9703, Val RMSE: 90.2899\n",
      "Epoch 25.      Train Loss: 1.2888, Val Loss: 0.6290.      Train RMSE: 3732.2312, Val RMSE: 44.4410\n",
      "Epoch 30.      Train Loss: 0.8102, Val Loss: 0.3954.      Train RMSE: 28.1611, Val RMSE: 29.4811\n",
      "Epoch 35.      Train Loss: 0.6221, Val Loss: 0.0789.      Train RMSE: 5.5929, Val RMSE: 3.7463\n",
      "Epoch 40.      Train Loss: 0.6111, Val Loss: 0.0916.      Train RMSE: 5.4717, Val RMSE: 4.7392\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [3:35:34, 590.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0001, l2_lambda:3\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 79.9811, Val Loss: 75.3436.      Train RMSE: 26094.3102, Val RMSE: 22026.0370\n",
      "Epoch 5.      Train Loss: 50.6816, Val Loss: 47.9766.      Train RMSE: 27907.1202, Val RMSE: 43077.5182\n",
      "Epoch 10.      Train Loss: 20.3473, Val Loss: 19.7419.      Train RMSE: 3268941.4567, Val RMSE: 3268911.7854\n",
      "Epoch 15.      Train Loss: 7.4278, Val Loss: 4.6448.      Train RMSE: 2.4131, Val RMSE: 2.7413\n",
      "Epoch 20.      Train Loss: 4.7448, Val Loss: 3.8874.      Train RMSE: 3.0462, Val RMSE: 3.1380\n",
      "Epoch 25.      Train Loss: 3.4573, Val Loss: 2.6856.      Train RMSE: 3.0507, Val RMSE: 3.0428\n",
      "Epoch 30.      Train Loss: 2.1387, Val Loss: 1.3640.      Train RMSE: 2.6587, Val RMSE: 2.7291\n",
      "Epoch 35.      Train Loss: 1.3728, Val Loss: 0.6312.      Train RMSE: 2.5554, Val RMSE: 2.6049\n",
      "Epoch 40.      Train Loss: 1.1031, Val Loss: 0.4269.      Train RMSE: 41.2418, Val RMSE: 14.5924\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 84.1793, Val Loss: 79.4434.      Train RMSE: 26094.1177, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 54.6095, Val Loss: 51.9323.      Train RMSE: 27906.9700, Val RMSE: 50407.6667\n",
      "Epoch 10.      Train Loss: 24.6015, Val Loss: 24.0129.      Train RMSE: 3268702.1540, Val RMSE: 3268911.7854\n",
      "Epoch 15.      Train Loss: 15.8502, Val Loss: 15.9453.      Train RMSE: 3268883.5573, Val RMSE: 3268911.7854\n",
      "Epoch 20.      Train Loss: 6.3391, Val Loss: 6.9747.      Train RMSE: 28.7188, Val RMSE: 39.3412\n",
      "Epoch 25.      Train Loss: 3.7981, Val Loss: 3.0791.      Train RMSE: 71.4546, Val RMSE: 65.8628\n",
      "Epoch 30.      Train Loss: 2.8535, Val Loss: 2.2847.      Train RMSE: 16.9857, Val RMSE: 13.1245\n",
      "Epoch 35.      Train Loss: 2.1879, Val Loss: 1.7074.      Train RMSE: 7.7117, Val RMSE: 12.2391\n",
      "Epoch 40.      Train Loss: 1.6111, Val Loss: 1.0672.      Train RMSE: 6.1611, Val RMSE: 5.4230\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 59.9334, Val Loss: 53.9320.      Train RMSE: 523759.4523, Val RMSE: 381508.1604\n",
      "Epoch 5.      Train Loss: 24.2924, Val Loss: 21.9029.      Train RMSE: 354785.5456, Val RMSE: 246118.7798\n",
      "Epoch 10.      Train Loss: 7.1809, Val Loss: 7.1005.      Train RMSE: 43722.8478, Val RMSE: 4912.7620\n",
      "Epoch 15.      Train Loss: 3.0617, Val Loss: 3.6550.      Train RMSE: 115.7035, Val RMSE: 90.2217\n",
      "Epoch 20.      Train Loss: 5.2433, Val Loss: 19.1374.      Train RMSE: 22124.4372, Val RMSE: 2.8957\n",
      "Epoch 25.      Train Loss: 1.0798, Val Loss: 0.5796.      Train RMSE: 17141.6611, Val RMSE: 28.1299\n",
      "Epoch 30.      Train Loss: 0.7194, Val Loss: 0.2129.      Train RMSE: 9894.2149, Val RMSE: 6.0637\n",
      "Epoch 35.      Train Loss: 0.6488, Val Loss: 0.1281.      Train RMSE: 9893.1593, Val RMSE: 5.5410\n",
      "Epoch 40.      Train Loss: 0.6294, Val Loss: 0.1315.      Train RMSE: 7.3087, Val RMSE: 6.3402\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [3:45:57, 600.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0001, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 121.0217, Val Loss: 113.2923.      Train RMSE: 26094.3102, Val RMSE: 22026.0370\n",
      "Epoch 5.      Train Loss: 72.1891, Val Loss: 67.6815.      Train RMSE: 27907.1202, Val RMSE: 43077.5182\n",
      "Epoch 10.      Train Loss: 28.8600, Val Loss: 27.4187.      Train RMSE: 3268941.4567, Val RMSE: 3268911.7854\n",
      "Epoch 15.      Train Loss: 9.1551, Val Loss: 7.6201.      Train RMSE: 2.3411, Val RMSE: 2.7262\n",
      "Epoch 20.      Train Loss: 6.8674, Val Loss: 5.8681.      Train RMSE: 2.4647, Val RMSE: 2.9873\n",
      "Epoch 25.      Train Loss: 4.4467, Val Loss: 3.4912.      Train RMSE: 2.4421, Val RMSE: 2.7303\n",
      "Epoch 30.      Train Loss: 2.4630, Val Loss: 1.6144.      Train RMSE: 2.2669, Val RMSE: 2.6816\n",
      "Epoch 35.      Train Loss: 1.4153, Val Loss: 0.6523.      Train RMSE: 2.2852, Val RMSE: 2.6395\n",
      "Epoch 40.      Train Loss: 1.0882, Val Loss: 0.4232.      Train RMSE: 3.9655, Val RMSE: 2.9365\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 128.0188, Val Loss: 120.1262.      Train RMSE: 26094.1177, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 78.7358, Val Loss: 74.2744.      Train RMSE: 27906.9700, Val RMSE: 50407.6667\n",
      "Epoch 10.      Train Loss: 35.9438, Val Loss: 34.5291.      Train RMSE: 3268373.1645, Val RMSE: 3268911.7854\n",
      "Epoch 15.      Train Loss: 21.3594, Val Loss: 21.0740.      Train RMSE: 3268823.8926, Val RMSE: 3268911.7854\n",
      "Epoch 20.      Train Loss: 9.1498, Val Loss: 9.6442.      Train RMSE: 28.0768, Val RMSE: 38.0850\n",
      "Epoch 25.      Train Loss: 5.5440, Val Loss: 4.7493.      Train RMSE: 48.6143, Val RMSE: 41.4490\n",
      "Epoch 30.      Train Loss: 4.1162, Val Loss: 3.4942.      Train RMSE: 10.5599, Val RMSE: 8.8485\n",
      "Epoch 35.      Train Loss: 2.9482, Val Loss: 2.4283.      Train RMSE: 6.3507, Val RMSE: 13.4025\n",
      "Epoch 40.      Train Loss: 2.0486, Val Loss: 1.5508.      Train RMSE: 6.5091, Val RMSE: 9.0243\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 92.0396, Val Loss: 83.2875.      Train RMSE: 520828.0835, Val RMSE: 377948.3681\n",
      "Epoch 5.      Train Loss: 39.0725, Val Loss: 35.2121.      Train RMSE: 359775.8733, Val RMSE: 237355.3962\n",
      "Epoch 10.      Train Loss: 11.2127, Val Loss: 10.6805.      Train RMSE: 40909.4563, Val RMSE: 3109.0687\n",
      "Epoch 15.      Train Loss: 3.7684, Val Loss: 4.5413.      Train RMSE: 95.8993, Val RMSE: 67.9787\n",
      "Epoch 20.      Train Loss: 1.9052, Val Loss: 1.1156.      Train RMSE: 63.1673, Val RMSE: 45.5038\n",
      "Epoch 25.      Train Loss: 1.1977, Val Loss: 0.5141.      Train RMSE: 24.5940, Val RMSE: 12.5171\n",
      "Epoch 30.      Train Loss: 0.6981, Val Loss: 0.1702.      Train RMSE: 6.5707, Val RMSE: 5.7428\n",
      "Epoch 35.      Train Loss: 0.6376, Val Loss: 0.1243.      Train RMSE: 7.1524, Val RMSE: 6.1743\n",
      "Epoch 40.      Train Loss: 0.6287, Val Loss: 0.1622.      Train RMSE: 7.0893, Val RMSE: 7.0752\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [3:56:17, 606.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0005, l2_lambda:0.1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 19.9798, Val Loss: 19.4759.      Train RMSE: 26094.4144, Val RMSE: 43077.5182\n",
      "Epoch 5.      Train Loss: 6.2590, Val Loss: 1.4795.      Train RMSE: 2868918.2509, Val RMSE: 79.6319\n",
      "Epoch 10.      Train Loss: 0.8742, Val Loss: 0.7018.      Train RMSE: 2624.1173, Val RMSE: 198.3102\n",
      "Epoch 15.      Train Loss: 0.8786, Val Loss: 0.7101.      Train RMSE: 2127.0651, Val RMSE: 154.6374\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 20.1113, Val Loss: 19.6031.      Train RMSE: 27906.9620, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 7.7564, Val Loss: 8.3661.      Train RMSE: 3269001.4400, Val RMSE: 3269016.5158\n",
      "Epoch 10.      Train Loss: 4.1732, Val Loss: 2.8661.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 15.      Train Loss: 0.8747, Val Loss: 0.6913.      Train RMSE: 2338.0400, Val RMSE: 174.3799\n",
      "Epoch 20.      Train Loss: 4.1188, Val Loss: 2.8276.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 6.6456, Val Loss: 2.4907.      Train RMSE: 508479.0293, Val RMSE: 393927.2526\n",
      "Epoch 5.      Train Loss: 0.9912, Val Loss: 0.8237.      Train RMSE: 18313.8094, Val RMSE: 103.9258\n",
      "Epoch 10.      Train Loss: 0.8886, Val Loss: 0.7221.      Train RMSE: 1341.8964, Val RMSE: 103.7860\n",
      "Epoch 15.      Train Loss: 0.8733, Val Loss: 0.6913.      Train RMSE: 1696.8637, Val RMSE: 163.4409\n",
      "Epoch 20.      Train Loss: 0.8735, Val Loss: 0.6907.      Train RMSE: 2734.6765, Val RMSE: 175.1275\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [4:01:07, 511.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0005, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 34.0142, Val Loss: 28.9842.      Train RMSE: 26094.4144, Val RMSE: 43077.5182\n",
      "Epoch 5.      Train Loss: 4.9012, Val Loss: 3.3670.      Train RMSE: 2.4787, Val RMSE: 2.8935\n",
      "Epoch 10.      Train Loss: 3.9843, Val Loss: 2.8267.      Train RMSE: 2.4740, Val RMSE: 2.8935\n",
      "Epoch 15.      Train Loss: 0.8733, Val Loss: 0.1944.      Train RMSE: 3.7309, Val RMSE: 3.1693\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 35.3320, Val Loss: 30.2576.      Train RMSE: 27906.9620, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 9.3668, Val Loss: 9.6190.      Train RMSE: 3268941.4567, Val RMSE: 3269016.5158\n",
      "Epoch 10.      Train Loss: 4.2988, Val Loss: 2.9573.      Train RMSE: 1364.3707, Val RMSE: 2.8935\n",
      "Epoch 15.      Train Loss: 1.3072, Val Loss: 1.6524.      Train RMSE: 93.7153, Val RMSE: 91.2174\n",
      "Epoch 20.      Train Loss: 1.2701, Val Loss: 1.6686.      Train RMSE: 94.3319, Val RMSE: 91.4302\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 17.3095, Val Loss: 8.4581.      Train RMSE: 479651.9357, Val RMSE: 285569.9363\n",
      "Epoch 5.      Train Loss: 1.4209, Val Loss: 1.7460.      Train RMSE: 100.7598, Val RMSE: 91.3193\n",
      "Epoch 10.      Train Loss: 0.9805, Val Loss: 0.2734.      Train RMSE: 73.1049, Val RMSE: 49.6239\n",
      "Epoch 15.      Train Loss: 0.9049, Val Loss: 0.3343.      Train RMSE: 4.1735, Val RMSE: 3.5289\n",
      "Epoch 20.      Train Loss: 0.8726, Val Loss: 0.2496.      Train RMSE: 3.9723, Val RMSE: 3.6625\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [4:06:48, 460.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0005, l2_lambda:3\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 64.3867, Val Loss: 47.9938.      Train RMSE: 26094.4144, Val RMSE: 43077.5182\n",
      "Epoch 5.      Train Loss: 5.2402, Val Loss: 8.6800.      Train RMSE: 968319.6870, Val RMSE: 3269016.5158\n",
      "Epoch 10.      Train Loss: 3.5130, Val Loss: 2.6459.      Train RMSE: 1114671.5066, Val RMSE: 61.5880\n",
      "Epoch 15.      Train Loss: 1.0391, Val Loss: 0.3751.      Train RMSE: 43.6382, Val RMSE: 3.2523\n",
      "Epoch 20.      Train Loss: 1.0169, Val Loss: 0.2789.      Train RMSE: 2.7924, Val RMSE: 2.5908\n",
      "Epoch 25.      Train Loss: 1.0898, Val Loss: 0.3007.      Train RMSE: 19704.9379, Val RMSE: 3.0653\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 68.3842, Val Loss: 51.9662.      Train RMSE: 27906.9620, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 4.5911, Val Loss: 4.5475.      Train RMSE: 73.9787, Val RMSE: 87.3597\n",
      "Epoch 10.      Train Loss: 4.4708, Val Loss: 2.8974.      Train RMSE: 1568399.2465, Val RMSE: 64.6769\n",
      "Epoch 15.      Train Loss: 1.5623, Val Loss: 0.4062.      Train RMSE: 2.3448, Val RMSE: 2.7408\n",
      "Epoch 20.      Train Loss: 1.0442, Val Loss: 0.3092.      Train RMSE: 391.8073, Val RMSE: 2.6137\n",
      "Epoch 25.      Train Loss: 0.9991, Val Loss: 0.3263.      Train RMSE: 2.9220, Val RMSE: 2.6270\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 40.5937, Val Loss: 21.6961.      Train RMSE: 469305.5371, Val RMSE: 234724.7967\n",
      "Epoch 5.      Train Loss: 3.7719, Val Loss: 2.6378.      Train RMSE: 1825528.6585, Val RMSE: 83.8479\n",
      "Epoch 10.      Train Loss: 1.4911, Val Loss: 0.4739.      Train RMSE: 98.0956, Val RMSE: 52.9996\n",
      "Epoch 15.      Train Loss: 1.0302, Val Loss: 0.3922.      Train RMSE: 2.8656, Val RMSE: 2.8147\n",
      "Epoch 20.      Train Loss: 1.0085, Val Loss: 0.5792.      Train RMSE: 2.8531, Val RMSE: 2.6993\n",
      "Epoch 25.      Train Loss: 1.6795, Val Loss: 2.5311.      Train RMSE: 101.5129, Val RMSE: 79.0765\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [4:13:23, 440.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0005, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 95.0309, Val Loss: 67.7102.      Train RMSE: 26094.4144, Val RMSE: 43077.5182\n",
      "Epoch 5.      Train Loss: 3.4121, Val Loss: 1.2549.      Train RMSE: 2.3839, Val RMSE: 2.7564\n",
      "Epoch 10.      Train Loss: 1.4752, Val Loss: 3.2518.      Train RMSE: 35480.3382, Val RMSE: 23.2947\n",
      "Epoch 15.      Train Loss: 1.4523, Val Loss: 0.6636.      Train RMSE: 50.2149, Val RMSE: 9.1291\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 101.6935, Val Loss: 74.3308.      Train RMSE: 27906.9620, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 5.9017, Val Loss: 5.4843.      Train RMSE: 74.8741, Val RMSE: 84.5264\n",
      "Epoch 10.      Train Loss: 1.3026, Val Loss: 0.7067.      Train RMSE: 9894.3220, Val RMSE: 11.8331\n",
      "Epoch 15.      Train Loss: 1.4667, Val Loss: 0.7517.      Train RMSE: 56.5226, Val RMSE: 97.9447\n",
      "Epoch 20.      Train Loss: 0.8497, Val Loss: 0.3717.      Train RMSE: 9894.4607, Val RMSE: 57.6283\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 63.4907, Val Loss: 35.0263.      Train RMSE: 474680.6094, Val RMSE: 236565.0207\n",
      "Epoch 5.      Train Loss: 1.7062, Val Loss: 0.8074.      Train RMSE: 94961.4328, Val RMSE: 10.4104\n",
      "Epoch 10.      Train Loss: 0.8432, Val Loss: 0.3768.      Train RMSE: 9894.2496, Val RMSE: 48.7225\n",
      "Epoch 15.      Train Loss: 0.7570, Val Loss: 0.2870.      Train RMSE: 56.3693, Val RMSE: 34.0598\n",
      "Epoch 20.      Train Loss: 0.8649, Val Loss: 0.2287.      Train RMSE: 3075.6856, Val RMSE: 19.0148\n",
      "Epoch 25.      Train Loss: 0.7996, Val Loss: 0.3501.      Train RMSE: 9894.2990, Val RMSE: 51.8466\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [4:19:18, 415.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:0.1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 18.0557, Val Loss: 8.7518.      Train RMSE: 1220574.0129, Val RMSE: 3268806.8912\n",
      "Epoch 5.      Train Loss: 0.8767, Val Loss: 0.6921.      Train RMSE: 1246.9129, Val RMSE: 207.6266\n",
      "Epoch 10.      Train Loss: 12.7424, Val Loss: 18.4388.      Train RMSE: 899.4631, Val RMSE: 2.8957\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 19.3504, Val Loss: 8.8763.      Train RMSE: 579723.1211, Val RMSE: 3268911.7854\n",
      "Epoch 5.      Train Loss: 0.9125, Val Loss: 0.7079.      Train RMSE: 1386.8261, Val RMSE: 161.1082\n",
      "Epoch 10.      Train Loss: 2.8032, Val Loss: 0.7096.      Train RMSE: 7506.6672, Val RMSE: 259.3601\n",
      "Epoch 15.      Train Loss: 0.8737, Val Loss: 0.6854.      Train RMSE: 3303.1532, Val RMSE: 213.0019\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 4.1978, Val Loss: 1.3361.      Train RMSE: 407787.9116, Val RMSE: 179085.3119\n",
      "Epoch 5.      Train Loss: 2.0716, Val Loss: 0.7264.      Train RMSE: 996.3843, Val RMSE: 87.1253\n",
      "Epoch 10.      Train Loss: 0.8740, Val Loss: 0.6954.      Train RMSE: 3292.7595, Val RMSE: 206.5757\n",
      "Epoch 15.      Train Loss: 0.8738, Val Loss: 0.6919.      Train RMSE: 2131.9617, Val RMSE: 154.9343\n",
      "Epoch 20.      Train Loss: 0.8737, Val Loss: 0.6849.      Train RMSE: 3747.8903, Val RMSE: 189.6131\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [4:23:48, 371.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 28.4568, Val Loss: 13.4238.      Train RMSE: 1220493.7933, Val RMSE: 3268806.8912\n",
      "Epoch 5.      Train Loss: 2.6527, Val Loss: 2.8591.      Train RMSE: 65.6594, Val RMSE: 2.8935\n",
      "Epoch 10.      Train Loss: 2.8801, Val Loss: 1.6491.      Train RMSE: 1598662.0966, Val RMSE: 91.2854\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 30.9180, Val Loss: 14.7048.      Train RMSE: 581390.1469, Val RMSE: 3268806.8912\n",
      "Epoch 5.      Train Loss: 1.4996, Val Loss: 1.7522.      Train RMSE: 93.9995, Val RMSE: 90.7861\n",
      "Epoch 10.      Train Loss: 1.2723, Val Loss: 1.6632.      Train RMSE: 94.2044, Val RMSE: 91.1758\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 11.3439, Val Loss: 3.7963.      Train RMSE: 349428.0672, Val RMSE: 69060.8302\n",
      "Epoch 5.      Train Loss: 4.3058, Val Loss: 8.2380.      Train RMSE: 1090071.3873, Val RMSE: 3269016.5158\n",
      "Epoch 10.      Train Loss: 2.2268, Val Loss: 1.6722.      Train RMSE: 569952.5956, Val RMSE: 88.4885\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [4:26:59, 317.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:3\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 48.1347, Val Loss: 19.6122.      Train RMSE: 1708052.4123, Val RMSE: 3268806.8912\n",
      "Epoch 5.      Train Loss: 3.3058, Val Loss: 2.6076.      Train RMSE: 895569.3045, Val RMSE: 63.2555\n",
      "Epoch 10.      Train Loss: 2.9026, Val Loss: 3.2967.      Train RMSE: 962856.9414, Val RMSE: 26.5039\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 53.2769, Val Loss: 23.9481.      Train RMSE: 1366338.2648, Val RMSE: 3268911.7854\n",
      "Epoch 5.      Train Loss: 1.9803, Val Loss: 2.5503.      Train RMSE: 111.0956, Val RMSE: 92.2116\n",
      "Epoch 10.      Train Loss: 3.7815, Val Loss: 8.2610.      Train RMSE: 1757894.5696, Val RMSE: 3269016.5158\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 26.3564, Val Loss: 6.6092.      Train RMSE: 358308.0911, Val RMSE: 2983.6498\n",
      "Epoch 5.      Train Loss: 3.2040, Val Loss: 2.5365.      Train RMSE: 838379.9888, Val RMSE: 86.0396\n",
      "Epoch 10.      Train Loss: 3.7551, Val Loss: 8.2722.      Train RMSE: 1546329.9365, Val RMSE: 3269016.5158\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [4:29:34, 268.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 69.9242, Val Loss: 27.1991.      Train RMSE: 1708052.4123, Val RMSE: 3268806.8912\n",
      "Epoch 5.      Train Loss: 4.3036, Val Loss: 3.2962.      Train RMSE: 1678109.3489, Val RMSE: 23.9851\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 77.7860, Val Loss: 34.4278.      Train RMSE: 1366338.2648, Val RMSE: 3268911.7854\n",
      "Epoch 5.      Train Loss: 3.8475, Val Loss: 2.9186.      Train RMSE: 31867.4786, Val RMSE: 2.8935\n",
      "Epoch 10.      Train Loss: 3.6089, Val Loss: 8.2310.      Train RMSE: 1380362.9198, Val RMSE: 3269015.5535\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 41.6758, Val Loss: 10.0228.      Train RMSE: 369984.1273, Val RMSE: 2108.1152\n",
      "Epoch 5.      Train Loss: 5.2242, Val Loss: 2.8272.      Train RMSE: 2202032.7761, Val RMSE: 2.8935\n",
      "Epoch 10.      Train Loss: 4.0033, Val Loss: 0.3796.      Train RMSE: 1515208.7897, Val RMSE: 51.2069\n",
      "Epoch 15.      Train Loss: 1.6854, Val Loss: 0.5981.      Train RMSE: 2.3448, Val RMSE: 2.7974\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [4:32:48, 511.51s/it]\n"
     ]
    }
   ],
   "source": [
    "h_list = [10]#[4, 6, 8, 10]\n",
    "batch_size_list = [128, 256]\n",
    "lr_list = [5e-5, 1e-4, 5e-4, 1e-3]\n",
    "l2_lambda_list = [0.1, 1, 3, 5]\n",
    "\n",
    "d1=source1_tensor.shape[1]  # number of features of the 1st source\n",
    "d2=source2_tensor.shape[1]  # number of features of the 2nd source\n",
    "\n",
    "# Make sure the folder exists\n",
    "output_dir = 'validation_results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for h, batch_size, lr, l2_lambda in tqdm.tqdm(itertools.product(h_list, batch_size_list, lr_list, l2_lambda_list)):\n",
    "    # if [h, batch_size, lr, l2_lambda] in existing_results:\n",
    "    #     continue\n",
    "\n",
    "    # create dataset\n",
    "    train_loader, val_loader, test_loader = create_datasets(source1_array, source2_array, target_array, batch_size, h)\n",
    "\n",
    "    print(f\"\\n📊 h: {h}, batch_size:{batch_size}, lr: {lr}, l2_lambda:{l2_lambda}\")\n",
    "    \n",
    "    ensemble, losses = train_tme_ensemble(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        d1=d1,  # number of features of the 1st source\n",
    "        d2=d2,  # number of features of the 2nd source\n",
    "        h=h,  # lag length\n",
    "        num_models=3,#20,\n",
    "        device=device,\n",
    "        lr=lr,\n",
    "        weight_decay=0.1,\n",
    "        l2_lambda=l2_lambda,\n",
    "        max_epochs=40,\n",
    "        patience=5,\n",
    "        adam=True\n",
    "    )\n",
    "\n",
    "    y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, val_loader, device='cpu')\n",
    "\n",
    "    results_to_save = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'val_losses': losses\n",
    "    }\n",
    "\n",
    "    # Create a filename based on hyperparameters\n",
    "    filename = f\"h{h}_batch{batch_size}_lr{lr:.0e}_lambda{l2_lambda}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Save the dictionary to a JSON file\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results_to_save, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3332b650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 0.1, 'rmse': 1089726.1613200568, 'mae': 1089726.1569289148, 'val_losses': [0.7013087279972483, 0.6707661559225105, 0.6705304299343805]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 1.0, 'rmse': 36.60138394557098, 'mae': 32.128989496778956, 'val_losses': [1.6637232920429745, 1.6190716325015317, 1.6649700327486288]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 3.0, 'rmse': 2179361.534267373, 'mae': 2179361.5341919395, 'val_losses': [2.4473783847738484, 2.534669772279067, 2.48196539331655]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 5.0, 'rmse': 2179344.5025853366, 'mae': 2179344.502583613, 'val_losses': [0.33665505479105184, 0.5487079876856725, 2.8285509947778995]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 0.1, 'rmse': 34.380482429447696, 'mae': 26.47390019265506, 'val_losses': [0.004263333976268768, -0.0775618064598959, -0.07598065337563148]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 1.0, 'rmse': 19.641328548022358, 'mae': 15.821951348926465, 'val_losses': [0.08398216113936706, 0.09961788298287352, 0.14452660221179]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 3.0, 'rmse': 9.096391494090792, 'mae': 8.914076367311875, 'val_losses': [0.12600167077721752, 0.13637380509591493, 0.07466695750834512]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 5.0, 'rmse': 11.846714729395057, 'mae': 11.710406796042673, 'val_losses': [0.08561097016771797, 0.11317839338367836, 0.09640285644687896]}, {'h': 10, 'batch_size': 128, 'lr': 0.0005, 'l2_lambda': 0.1, 'rmse': 7371.843832436313, 'mae': 7371.454768352982, 'val_losses': [0.6802540442860517, 0.6802716377939357, 0.6800632888420683]}, {'h': 10, 'batch_size': 128, 'lr': 0.0005, 'l2_lambda': 1.0, 'rmse': 79.68430450533288, 'mae': 69.07313219629178, 'val_losses': [1.6323273343140963, 0.18419182086821462, 0.17276025465765937]}, {'h': 10, 'batch_size': 128, 'lr': 0.0005, 'l2_lambda': 3.0, 'rmse': 1089672.30554185, 'mae': 1089672.3055386231, 'val_losses': [0.3036820053687838, 0.3437256883280199, 0.31218698303230474]}, {'h': 10, 'batch_size': 128, 'lr': 0.0005, 'l2_lambda': 5.0, 'rmse': 18.83540702253546, 'mae': 9.728747173505784, 'val_losses': [0.32468328185257367, 0.34151489464718787, 0.3016753511839226]}, {'h': 10, 'batch_size': 128, 'lr': 5e-05, 'l2_lambda': 0.1, 'rmse': 69.98187132811283, 'mae': 28.314113841327455, 'val_losses': [-0.025143117078992187, 0.013102635741233826, -0.03629365477894173]}, {'h': 10, 'batch_size': 128, 'lr': 5e-05, 'l2_lambda': 1.0, 'rmse': 30.132333557273498, 'mae': 26.42163568316961, 'val_losses': [0.2797554149368747, 0.4397782613752318, 0.10462011824377247]}, {'h': 10, 'batch_size': 128, 'lr': 5e-05, 'l2_lambda': 3.0, 'rmse': 8.556378125537252, 'mae': 8.372314525409182, 'val_losses': [0.22772850590895433, 1.0314949960982214, 0.16105705630950262]}, {'h': 10, 'batch_size': 128, 'lr': 5e-05, 'l2_lambda': 5.0, 'rmse': 9.364026167015433, 'mae': 9.188425845889766, 'val_losses': [0.165138700091448, 1.4280872393826969, 0.13474790023670333]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 0.1, 'rmse': 96.24405841019112, 'mae': 58.149273079039546, 'val_losses': [0.677123174437734, 0.6798209275622837, 0.674311374543143]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 1.0, 'rmse': 29.093144756931395, 'mae': 24.9987616327635, 'val_losses': [1.6200325513472322, 1.6543025433040055, 1.646859597964365]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 3.0, 'rmse': 2179347.0094407885, 'mae': 2179347.0094252373, 'val_losses': [2.6075608554433605, 2.5503103987115328, 2.5364683573363256]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 5.0, 'rmse': 21.60882549964574, 'mae': 12.417172907161957, 'val_losses': [0.3255245570765167, 2.8296235325883647, 0.379599074909433]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 0.1, 'rmse': 152.26836106126325, 'mae': 20.20989772097929, 'val_losses': [-0.07272327166111743, -0.044305760779830276, 0.23937714481573613]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 1.0, 'rmse': 4.960035973650343, 'mae': 3.049843483836532, 'val_losses': [0.20360519089659707, 0.4889580885894963, 0.07885842135206597]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 3.0, 'rmse': 7.148934110758968, 'mae': 5.511608294993634, 'val_losses': [0.4268751119981047, 1.0671539570464463, 0.12813156269124296]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 5.0, 'rmse': 5.89015889183083, 'mae': 5.592879137453793, 'val_losses': [0.37920478059620155, 1.5507688375770068, 0.12428260020545272]}, {'h': 10, 'batch_size': 256, 'lr': 0.0005, 'l2_lambda': 0.1, 'rmse': 130.06021213530445, 'mae': 58.59685198314355, 'val_losses': [0.7017776865939624, 0.6913000706766472, 0.6812832130760443]}, {'h': 10, 'batch_size': 256, 'lr': 0.0005, 'l2_lambda': 1.0, 'rmse': 30.43102247250523, 'mae': 26.697840622818127, 'val_losses': [0.16984762251377106, 1.6511705395628193, 0.15816219041093452]}, {'h': 10, 'batch_size': 256, 'lr': 0.0005, 'l2_lambda': 3.0, 'rmse': 25.88466493481066, 'mae': 17.698247348801125, 'val_losses': [0.27889255006782343, 0.30916637096737254, 0.28144530096992115]}, {'h': 10, 'batch_size': 256, 'lr': 0.0005, 'l2_lambda': 5.0, 'rmse': 9530.315764176155, 'mae': 9528.759459357409, 'val_losses': [0.3314512091093376, 0.2678790448508302, 0.202126856343668]}, {'h': 10, 'batch_size': 256, 'lr': 5e-05, 'l2_lambda': 0.1, 'rmse': 117811.56256915265, 'mae': 114686.29921349291, 'val_losses': [0.047964701093122605, 3.8293014608445715, 0.7279603030593669]}, {'h': 10, 'batch_size': 256, 'lr': 5e-05, 'l2_lambda': 1.0, 'rmse': 19.570115444982193, 'mae': 16.35111885379237, 'val_losses': [1.3945028410583247, 4.229291532860428, 0.6333020251305377]}, {'h': 10, 'batch_size': 256, 'lr': 5e-05, 'l2_lambda': 3.0, 'rmse': 24.971855979009206, 'mae': 22.382182273167164, 'val_losses': [3.6903368859994607, 7.057284941438769, 0.8795328907302169]}, {'h': 10, 'batch_size': 256, 'lr': 5e-05, 'l2_lambda': 5.0, 'rmse': 22432.95842501505, 'mae': 18221.910417659583, 'val_losses': [6.116756654176556, 11.541008730403712, 1.1838267885270666]}]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing your saved results\n",
    "input_dir = 'validation_results'\n",
    "\n",
    "# Prepare a list to collect all results\n",
    "all_results = []\n",
    "existing_results = []\n",
    "\n",
    "# Loop over all files in the directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        \n",
    "        # Open and load the JSON file\n",
    "        with open(filepath, 'r') as f:\n",
    "            result = json.load(f)\n",
    "        \n",
    "        # Parse hyperparameters from the filename\n",
    "        name_parts = filename.replace('.json', '').split('_')\n",
    "        h = int(name_parts[0][1:])  # strip the 'h'\n",
    "        batch_size = int(name_parts[1][5:])  # strip 'batch'\n",
    "        lr = float(name_parts[2][2:].replace('e', 'e'))  # scientific notation stays\n",
    "        l2_lambda = float(name_parts[3][6:])  # strip 'lambda'\n",
    "\n",
    "        existing_results.append([h,batch_size,lr,l2_lambda])\n",
    "        \n",
    "        # Combine hyperparameters and results\n",
    "        entry = {\n",
    "            'h': h,\n",
    "            'batch_size': batch_size,\n",
    "            'lr': lr,\n",
    "            'l2_lambda': l2_lambda,\n",
    "            **result  # unpack the RMSE, MAE, etc.\n",
    "        }\n",
    "        all_results.append(entry)\n",
    "\n",
    "# Now `all_results` is a list of dictionaries\n",
    "print(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fa82f95c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>l2_lambda</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>val_losses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.960036e+00</td>\n",
       "      <td>3.049843e+00</td>\n",
       "      <td>[0.20360519089659707, 0.4889580885894963, 0.07...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.890159e+00</td>\n",
       "      <td>5.592879e+00</td>\n",
       "      <td>[0.37920478059620155, 1.5507688375770068, 0.12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.148934e+00</td>\n",
       "      <td>5.511608e+00</td>\n",
       "      <td>[0.4268751119981047, 1.0671539570464463, 0.128...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.556378e+00</td>\n",
       "      <td>8.372315e+00</td>\n",
       "      <td>[0.22772850590895433, 1.0314949960982214, 0.16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.096391e+00</td>\n",
       "      <td>8.914076e+00</td>\n",
       "      <td>[0.12600167077721752, 0.13637380509591493, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.364026e+00</td>\n",
       "      <td>9.188426e+00</td>\n",
       "      <td>[0.165138700091448, 1.4280872393826969, 0.1347...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.184671e+01</td>\n",
       "      <td>1.171041e+01</td>\n",
       "      <td>[0.08561097016771797, 0.11317839338367836, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.883541e+01</td>\n",
       "      <td>9.728747e+00</td>\n",
       "      <td>[0.32468328185257367, 0.34151489464718787, 0.3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.957012e+01</td>\n",
       "      <td>1.635112e+01</td>\n",
       "      <td>[1.3945028410583247, 4.229291532860428, 0.6333...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.964133e+01</td>\n",
       "      <td>1.582195e+01</td>\n",
       "      <td>[0.08398216113936706, 0.09961788298287352, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.160883e+01</td>\n",
       "      <td>1.241717e+01</td>\n",
       "      <td>[0.3255245570765167, 2.8296235325883647, 0.379...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.497186e+01</td>\n",
       "      <td>2.238218e+01</td>\n",
       "      <td>[3.6903368859994607, 7.057284941438769, 0.8795...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.588466e+01</td>\n",
       "      <td>1.769825e+01</td>\n",
       "      <td>[0.27889255006782343, 0.30916637096737254, 0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.909314e+01</td>\n",
       "      <td>2.499876e+01</td>\n",
       "      <td>[1.6200325513472322, 1.6543025433040055, 1.646...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.013233e+01</td>\n",
       "      <td>2.642164e+01</td>\n",
       "      <td>[0.2797554149368747, 0.4397782613752318, 0.104...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.043102e+01</td>\n",
       "      <td>2.669784e+01</td>\n",
       "      <td>[0.16984762251377106, 1.6511705395628193, 0.15...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.438048e+01</td>\n",
       "      <td>2.647390e+01</td>\n",
       "      <td>[0.004263333976268768, -0.0775618064598959, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.660138e+01</td>\n",
       "      <td>3.212899e+01</td>\n",
       "      <td>[1.6637232920429745, 1.6190716325015317, 1.664...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6.998187e+01</td>\n",
       "      <td>2.831411e+01</td>\n",
       "      <td>[-0.025143117078992187, 0.013102635741233826, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.968430e+01</td>\n",
       "      <td>6.907313e+01</td>\n",
       "      <td>[1.6323273343140963, 0.18419182086821462, 0.17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>9.624406e+01</td>\n",
       "      <td>5.814927e+01</td>\n",
       "      <td>[0.677123174437734, 0.6798209275622837, 0.6743...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.300602e+02</td>\n",
       "      <td>5.859685e+01</td>\n",
       "      <td>[0.7017776865939624, 0.6913000706766472, 0.681...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.522684e+02</td>\n",
       "      <td>2.020990e+01</td>\n",
       "      <td>[-0.07272327166111743, -0.044305760779830276, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>0.1</td>\n",
       "      <td>7.371844e+03</td>\n",
       "      <td>7.371455e+03</td>\n",
       "      <td>[0.6802540442860517, 0.6802716377939357, 0.680...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.530316e+03</td>\n",
       "      <td>9.528759e+03</td>\n",
       "      <td>[0.3314512091093376, 0.2678790448508302, 0.202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.243296e+04</td>\n",
       "      <td>1.822191e+04</td>\n",
       "      <td>[6.116756654176556, 11.541008730403712, 1.1838...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.178116e+05</td>\n",
       "      <td>1.146863e+05</td>\n",
       "      <td>[0.047964701093122605, 3.8293014608445715, 0.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.089672e+06</td>\n",
       "      <td>1.089672e+06</td>\n",
       "      <td>[0.3036820053687838, 0.3437256883280199, 0.312...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.089726e+06</td>\n",
       "      <td>1.089726e+06</td>\n",
       "      <td>[0.7013087279972483, 0.6707661559225105, 0.670...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.179345e+06</td>\n",
       "      <td>2.179345e+06</td>\n",
       "      <td>[0.33665505479105184, 0.5487079876856725, 2.82...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.179347e+06</td>\n",
       "      <td>2.179347e+06</td>\n",
       "      <td>[2.6075608554433605, 2.5503103987115328, 2.536...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.179362e+06</td>\n",
       "      <td>2.179362e+06</td>\n",
       "      <td>[2.4473783847738484, 2.534669772279067, 2.4819...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     h  batch_size       lr  l2_lambda          rmse           mae  \\\n",
       "21  10         256  0.00010        1.0  4.960036e+00  3.049843e+00   \n",
       "23  10         256  0.00010        5.0  5.890159e+00  5.592879e+00   \n",
       "22  10         256  0.00010        3.0  7.148934e+00  5.511608e+00   \n",
       "14  10         128  0.00005        3.0  8.556378e+00  8.372315e+00   \n",
       "6   10         128  0.00010        3.0  9.096391e+00  8.914076e+00   \n",
       "15  10         128  0.00005        5.0  9.364026e+00  9.188426e+00   \n",
       "7   10         128  0.00010        5.0  1.184671e+01  1.171041e+01   \n",
       "11  10         128  0.00050        5.0  1.883541e+01  9.728747e+00   \n",
       "29  10         256  0.00005        1.0  1.957012e+01  1.635112e+01   \n",
       "5   10         128  0.00010        1.0  1.964133e+01  1.582195e+01   \n",
       "19  10         256  0.00100        5.0  2.160883e+01  1.241717e+01   \n",
       "30  10         256  0.00005        3.0  2.497186e+01  2.238218e+01   \n",
       "26  10         256  0.00050        3.0  2.588466e+01  1.769825e+01   \n",
       "17  10         256  0.00100        1.0  2.909314e+01  2.499876e+01   \n",
       "13  10         128  0.00005        1.0  3.013233e+01  2.642164e+01   \n",
       "25  10         256  0.00050        1.0  3.043102e+01  2.669784e+01   \n",
       "4   10         128  0.00010        0.1  3.438048e+01  2.647390e+01   \n",
       "1   10         128  0.00100        1.0  3.660138e+01  3.212899e+01   \n",
       "12  10         128  0.00005        0.1  6.998187e+01  2.831411e+01   \n",
       "9   10         128  0.00050        1.0  7.968430e+01  6.907313e+01   \n",
       "16  10         256  0.00100        0.1  9.624406e+01  5.814927e+01   \n",
       "24  10         256  0.00050        0.1  1.300602e+02  5.859685e+01   \n",
       "20  10         256  0.00010        0.1  1.522684e+02  2.020990e+01   \n",
       "8   10         128  0.00050        0.1  7.371844e+03  7.371455e+03   \n",
       "27  10         256  0.00050        5.0  9.530316e+03  9.528759e+03   \n",
       "31  10         256  0.00005        5.0  2.243296e+04  1.822191e+04   \n",
       "28  10         256  0.00005        0.1  1.178116e+05  1.146863e+05   \n",
       "10  10         128  0.00050        3.0  1.089672e+06  1.089672e+06   \n",
       "0   10         128  0.00100        0.1  1.089726e+06  1.089726e+06   \n",
       "3   10         128  0.00100        5.0  2.179345e+06  2.179345e+06   \n",
       "18  10         256  0.00100        3.0  2.179347e+06  2.179347e+06   \n",
       "2   10         128  0.00100        3.0  2.179362e+06  2.179362e+06   \n",
       "\n",
       "                                           val_losses  \n",
       "21  [0.20360519089659707, 0.4889580885894963, 0.07...  \n",
       "23  [0.37920478059620155, 1.5507688375770068, 0.12...  \n",
       "22  [0.4268751119981047, 1.0671539570464463, 0.128...  \n",
       "14  [0.22772850590895433, 1.0314949960982214, 0.16...  \n",
       "6   [0.12600167077721752, 0.13637380509591493, 0.0...  \n",
       "15  [0.165138700091448, 1.4280872393826969, 0.1347...  \n",
       "7   [0.08561097016771797, 0.11317839338367836, 0.0...  \n",
       "11  [0.32468328185257367, 0.34151489464718787, 0.3...  \n",
       "29  [1.3945028410583247, 4.229291532860428, 0.6333...  \n",
       "5   [0.08398216113936706, 0.09961788298287352, 0.1...  \n",
       "19  [0.3255245570765167, 2.8296235325883647, 0.379...  \n",
       "30  [3.6903368859994607, 7.057284941438769, 0.8795...  \n",
       "26  [0.27889255006782343, 0.30916637096737254, 0.2...  \n",
       "17  [1.6200325513472322, 1.6543025433040055, 1.646...  \n",
       "13  [0.2797554149368747, 0.4397782613752318, 0.104...  \n",
       "25  [0.16984762251377106, 1.6511705395628193, 0.15...  \n",
       "4   [0.004263333976268768, -0.0775618064598959, -0...  \n",
       "1   [1.6637232920429745, 1.6190716325015317, 1.664...  \n",
       "12  [-0.025143117078992187, 0.013102635741233826, ...  \n",
       "9   [1.6323273343140963, 0.18419182086821462, 0.17...  \n",
       "16  [0.677123174437734, 0.6798209275622837, 0.6743...  \n",
       "24  [0.7017776865939624, 0.6913000706766472, 0.681...  \n",
       "20  [-0.07272327166111743, -0.044305760779830276, ...  \n",
       "8   [0.6802540442860517, 0.6802716377939357, 0.680...  \n",
       "27  [0.3314512091093376, 0.2678790448508302, 0.202...  \n",
       "31  [6.116756654176556, 11.541008730403712, 1.1838...  \n",
       "28  [0.047964701093122605, 3.8293014608445715, 0.7...  \n",
       "10  [0.3036820053687838, 0.3437256883280199, 0.312...  \n",
       "0   [0.7013087279972483, 0.6707661559225105, 0.670...  \n",
       "3   [0.33665505479105184, 0.5487079876856725, 2.82...  \n",
       "18  [2.6075608554433605, 2.5503103987115328, 2.536...  \n",
       "2   [2.4473783847738484, 2.534669772279067, 2.4819...  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_results).sort_values(\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae51f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "h_list = [10]#[4, 6, 8, 10]\n",
    "batch_size_list = [64, 128, 256]\n",
    "lr_list = [5e-5, 1e-4, 5e-4, 1e-3]\n",
    "l2_lambda_list = [0.1, 1, 3, 5]\n",
    "\n",
    "d1=source1_tensor.shape[1]  # number of features of the 1st source\n",
    "d2=source2_tensor.shape[1]  # number of features of the 2nd source\n",
    "\n",
    "# Make sure the folder exists\n",
    "output_dir = 'validation_results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# printing such hyperparams combination that are not already processed\n",
    "for h, batch_size, lr, l2_lambda in tqdm.tqdm(itertools.product(h_list, batch_size_list, lr_list, l2_lambda_list)):\n",
    "    if [h, batch_size, lr, l2_lambda] in existing_results:\n",
    "        continue\n",
    "    \n",
    "    print(h, batch_size, lr, l2_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99688073",
   "metadata": {},
   "source": [
    "## Training final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "42848c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0001, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/20\n",
      "Epoch 1.      Train Loss: 38.9569, Val Loss: 37.4437.      Train RMSE: 26094.3102, Val RMSE: 22026.0370\n",
      "Epoch 5.      Train Loss: 29.7270, Val Loss: 28.8966.      Train RMSE: 27907.1202, Val RMSE: 43077.5182\n",
      "Epoch 10.      Train Loss: 12.8008, Val Loss: 13.0383.      Train RMSE: 3268866.8770, Val RMSE: 3268806.8912\n",
      "Epoch 15.      Train Loss: 6.3414, Val Loss: 4.8534.      Train RMSE: 2.4810, Val RMSE: 2.8935\n",
      "Epoch 20.      Train Loss: 2.1606, Val Loss: 1.3952.      Train RMSE: 4.4286, Val RMSE: 5.5679\n",
      "Epoch 25.      Train Loss: 1.7247, Val Loss: 0.9727.      Train RMSE: 5.0741, Val RMSE: 4.6469\n",
      "Epoch 30.      Train Loss: 1.2720, Val Loss: 0.5490.      Train RMSE: 4.0380, Val RMSE: 3.4845\n",
      "Epoch 35.      Train Loss: 0.9748, Val Loss: 0.2911.      Train RMSE: 3.8131, Val RMSE: 3.2345\n",
      "Epoch 40.      Train Loss: 0.8675, Val Loss: 0.2202.      Train RMSE: 3.9682, Val RMSE: 3.3968\n",
      "Epoch 45.      Train Loss: 0.9534, Val Loss: 0.3144.      Train RMSE: 74.0263, Val RMSE: 56.0817\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/20\n",
      "Epoch 1.      Train Loss: 40.3558, Val Loss: 38.8082.      Train RMSE: 26094.1177, Val RMSE: 50407.6667\n",
      "Epoch 5.      Train Loss: 30.9998, Val Loss: 30.1697.      Train RMSE: 26094.2121, Val RMSE: 50407.6667\n",
      "Epoch 10.      Train Loss: 14.1036, Val Loss: 14.3414.      Train RMSE: 3268761.9813, Val RMSE: 3268911.7854\n",
      "Epoch 15.      Train Loss: 11.0652, Val Loss: 11.5047.      Train RMSE: 3268612.9724, Val RMSE: 3268911.7854\n",
      "Epoch 20.      Train Loss: 9.4997, Val Loss: 10.0390.      Train RMSE: 3268986.3641, Val RMSE: 3268911.4646\n",
      "Epoch 25.      Train Loss: 2.4231, Val Loss: 2.9075.      Train RMSE: 61.4492, Val RMSE: 72.3617\n",
      "Epoch 30.      Train Loss: 1.9068, Val Loss: 2.2457.      Train RMSE: 88.0863, Val RMSE: 89.7966\n",
      "Epoch 35.      Train Loss: 1.3556, Val Loss: 0.7421.      Train RMSE: 83.1163, Val RMSE: 74.8643\n",
      "Epoch 40.      Train Loss: 1.1539, Val Loss: 0.4890.      Train RMSE: 15.4771, Val RMSE: 10.9310\n",
      "Epoch 45.      Train Loss: 0.8139, Val Loss: 0.3373.      Train RMSE: 6.3771, Val RMSE: 5.6896\n",
      "Epoch 50.      Train Loss: 0.7186, Val Loss: 0.2302.      Train RMSE: 6.3817, Val RMSE: 5.3782\n",
      "Epoch 55.      Train Loss: 0.6568, Val Loss: 0.1826.      Train RMSE: 6.1304, Val RMSE: 5.6853\n",
      "Epoch 60.      Train Loss: 0.6227, Val Loss: 0.1486.      Train RMSE: 5.5716, Val RMSE: 6.4119\n",
      "Epoch 65.      Train Loss: 0.6124, Val Loss: 0.1103.      Train RMSE: 9.7006, Val RMSE: 5.7057\n",
      "Epoch 70.      Train Loss: 0.6110, Val Loss: 0.1175.      Train RMSE: 6.1527, Val RMSE: 7.0314\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/20\n",
      "Epoch 1.      Train Loss: 27.6461, Val Loss: 24.2263.      Train RMSE: 510579.3731, Val RMSE: 372722.0437\n",
      "Epoch 5.      Train Loss: 9.3257, Val Loss: 8.6084.      Train RMSE: 364099.4480, Val RMSE: 286464.7149\n",
      "Epoch 10.      Train Loss: 3.7792, Val Loss: 3.7864.      Train RMSE: 97051.1360, Val RMSE: 75288.6252\n",
      "Epoch 15.      Train Loss: 1.9802, Val Loss: 2.2553.      Train RMSE: 9866.7056, Val RMSE: 104.4559\n",
      "Epoch 20.      Train Loss: 1.4268, Val Loss: 1.7938.      Train RMSE: 94.9703, Val RMSE: 90.2899\n",
      "Epoch 25.      Train Loss: 1.2888, Val Loss: 0.6290.      Train RMSE: 3732.2312, Val RMSE: 44.4410\n",
      "Epoch 30.      Train Loss: 0.8102, Val Loss: 0.3954.      Train RMSE: 28.1611, Val RMSE: 29.4811\n",
      "Epoch 35.      Train Loss: 0.6221, Val Loss: 0.0789.      Train RMSE: 5.5929, Val RMSE: 3.7463\n",
      "Epoch 40.      Train Loss: 0.6111, Val Loss: 0.0916.      Train RMSE: 5.4717, Val RMSE: 4.7392\n",
      "Epoch 45.      Train Loss: 0.6120, Val Loss: 0.1500.      Train RMSE: 5.9681, Val RMSE: 6.0497\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 4/20\n",
      "Epoch 1.      Train Loss: 22.4601, Val Loss: 19.8319.      Train RMSE: 308261.7120, Val RMSE: 309320.8921\n",
      "Epoch 5.      Train Loss: 9.5506, Val Loss: 8.9963.      Train RMSE: 215215.5950, Val RMSE: 180759.9418\n",
      "Epoch 10.      Train Loss: 4.6056, Val Loss: 4.7278.      Train RMSE: 59139.9912, Val RMSE: 39923.0796\n",
      "Epoch 15.      Train Loss: 2.5795, Val Loss: 2.8831.      Train RMSE: 122.1522, Val RMSE: 84.3296\n",
      "Epoch 20.      Train Loss: 1.7757, Val Loss: 2.1304.      Train RMSE: 88.7230, Val RMSE: 85.4040\n",
      "Epoch 25.      Train Loss: 1.4713, Val Loss: 1.8543.      Train RMSE: 93.3755, Val RMSE: 89.4881\n",
      "Epoch 30.      Train Loss: 1.0635, Val Loss: 0.4784.      Train RMSE: 83.0141, Val RMSE: 65.0816\n",
      "Epoch 35.      Train Loss: 0.6930, Val Loss: 0.2241.      Train RMSE: 5.4071, Val RMSE: 6.0079\n",
      "Epoch 40.      Train Loss: 0.6451, Val Loss: 0.1109.      Train RMSE: 5.7872, Val RMSE: 4.4988\n",
      "Epoch 45.      Train Loss: 0.6200, Val Loss: 0.1661.      Train RMSE: 6.0117, Val RMSE: 8.7719\n",
      "Epoch 50.      Train Loss: 0.6136, Val Loss: 0.0536.      Train RMSE: 13.3894, Val RMSE: 3.6493\n",
      "Epoch 55.      Train Loss: 0.6125, Val Loss: 0.1588.      Train RMSE: 12.7123, Val RMSE: 8.1918\n",
      "Epoch 60.      Train Loss: 0.6112, Val Loss: 0.1179.      Train RMSE: 7.6986, Val RMSE: 5.2263\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 5/20\n",
      "Epoch 1.      Train Loss: 37.6111, Val Loss: 36.1699.      Train RMSE: 17138.1381, Val RMSE: 295.0022\n",
      "Epoch 5.      Train Loss: 28.9274, Val Loss: 28.1544.      Train RMSE: 17137.4509, Val RMSE: 269.8604\n",
      "Epoch 10.      Train Loss: 23.2292, Val Loss: 22.8429.      Train RMSE: 9894.4421, Val RMSE: 3.0320\n",
      "Epoch 15.      Train Loss: 3.3559, Val Loss: 3.6083.      Train RMSE: 47381.1926, Val RMSE: 11452.3727\n",
      "Epoch 20.      Train Loss: 2.0861, Val Loss: 1.2437.      Train RMSE: 106.6651, Val RMSE: 86.1537\n",
      "Epoch 25.      Train Loss: 1.4755, Val Loss: 0.8226.      Train RMSE: 47.9056, Val RMSE: 30.6107\n",
      "Epoch 30.      Train Loss: 0.9814, Val Loss: 0.4914.      Train RMSE: 13992.5260, Val RMSE: 6.7728\n",
      "Epoch 35.      Train Loss: 0.8638, Val Loss: 0.3522.      Train RMSE: 12186.1016, Val RMSE: 5.5062\n",
      "Epoch 40.      Train Loss: 0.7972, Val Loss: 0.3501.      Train RMSE: 9894.2222, Val RMSE: 7.7718\n",
      "Epoch 45.      Train Loss: 0.7644, Val Loss: 0.2791.      Train RMSE: 9894.2044, Val RMSE: 6.3361\n",
      "Epoch 50.      Train Loss: 0.7422, Val Loss: 0.2279.      Train RMSE: 9894.2060, Val RMSE: 4.8166\n",
      "Epoch 55.      Train Loss: 0.7228, Val Loss: 0.2717.      Train RMSE: 9894.2052, Val RMSE: 10.7980\n",
      "Epoch 60.      Train Loss: 0.7069, Val Loss: 0.1744.      Train RMSE: 9894.2052, Val RMSE: 5.0280\n",
      "Epoch 65.      Train Loss: 0.6957, Val Loss: 0.1766.      Train RMSE: 32.7914, Val RMSE: 4.8791\n",
      "Epoch 70.      Train Loss: 0.6815, Val Loss: 0.2746.      Train RMSE: 8.6480, Val RMSE: 11.1936\n",
      "Epoch 75.      Train Loss: 0.9353, Val Loss: 0.4929.      Train RMSE: 9894.2480, Val RMSE: 35.8656\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 6/20\n",
      "Epoch 1.      Train Loss: 32.9772, Val Loss: 31.6489.      Train RMSE: 120.8772, Val RMSE: 3.4022\n",
      "Epoch 5.      Train Loss: 25.2413, Val Loss: 24.5948.      Train RMSE: 2.4842, Val RMSE: 3.0204\n",
      "Epoch 10.      Train Loss: 3.6171, Val Loss: 3.7033.      Train RMSE: 2945.6118, Val RMSE: 89.7124\n",
      "Epoch 15.      Train Loss: 1.9340, Val Loss: 2.1656.      Train RMSE: 79.9516, Val RMSE: 81.3976\n",
      "Epoch 20.      Train Loss: 1.1965, Val Loss: 0.7299.      Train RMSE: 13992.5786, Val RMSE: 33.5211\n",
      "Epoch 25.      Train Loss: 0.9150, Val Loss: 0.4556.      Train RMSE: 32.7608, Val RMSE: 32.1123\n",
      "Epoch 30.      Train Loss: 0.6662, Val Loss: 0.1743.      Train RMSE: 4.9086, Val RMSE: 5.7454\n",
      "Epoch 35.      Train Loss: 0.6225, Val Loss: 0.1391.      Train RMSE: 5.3657, Val RMSE: 4.9634\n",
      "Epoch 40.      Train Loss: 0.6121, Val Loss: 0.1096.      Train RMSE: 6.5152, Val RMSE: 6.1176\n",
      "Epoch 45.      Train Loss: 0.6125, Val Loss: 0.1442.      Train RMSE: 10.9302, Val RMSE: 7.3919\n",
      "Epoch 50.      Train Loss: 0.6142, Val Loss: 0.1022.      Train RMSE: 7.1437, Val RMSE: 5.3199\n",
      "Epoch 55.      Train Loss: 0.6116, Val Loss: 0.1390.      Train RMSE: 6.9016, Val RMSE: 5.6279\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 7/20\n",
      "Epoch 1.      Train Loss: 31.6221, Val Loss: 28.7547.      Train RMSE: 796689.6039, Val RMSE: 772986.3090\n",
      "Epoch 5.      Train Loss: 14.6592, Val Loss: 13.6477.      Train RMSE: 942366.7390, Val RMSE: 832224.4227\n",
      "Epoch 10.      Train Loss: 6.8094, Val Loss: 6.6115.      Train RMSE: 552574.9474, Val RMSE: 502873.8494\n",
      "Epoch 15.      Train Loss: 3.2954, Val Loss: 3.5076.      Train RMSE: 226504.8159, Val RMSE: 199503.1627\n",
      "Epoch 20.      Train Loss: 1.8789, Val Loss: 2.2186.      Train RMSE: 75877.2533, Val RMSE: 51314.6940\n",
      "Epoch 25.      Train Loss: 1.3963, Val Loss: 1.7875.      Train RMSE: 10237.9666, Val RMSE: 96.0526\n",
      "Epoch 30.      Train Loss: 1.2199, Val Loss: 0.5898.      Train RMSE: 82.4910, Val RMSE: 38.0406\n",
      "Epoch 35.      Train Loss: 0.8037, Val Loss: 0.3243.      Train RMSE: 27.4261, Val RMSE: 25.1562\n",
      "Epoch 40.      Train Loss: 0.6185, Val Loss: 0.1105.      Train RMSE: 20.2089, Val RMSE: 5.8703\n",
      "Epoch 45.      Train Loss: 0.6121, Val Loss: 0.1726.      Train RMSE: 6.5584, Val RMSE: 8.2875\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 8/20\n",
      "Epoch 1.      Train Loss: 32.1744, Val Loss: 29.0419.      Train RMSE: 24235.8544, Val RMSE: 52356.2394\n",
      "Epoch 5.      Train Loss: 20.6162, Val Loss: 18.2641.      Train RMSE: 41977.6367, Val RMSE: 52356.2394\n",
      "Epoch 10.      Train Loss: 12.5957, Val Loss: 10.7449.      Train RMSE: 19788.4055, Val RMSE: 26178.1197\n",
      "Epoch 15.      Train Loss: 8.3684, Val Loss: 6.7851.      Train RMSE: 13992.5163, Val RMSE: 26178.1197\n",
      "Epoch 20.      Train Loss: 6.1743, Val Loss: 4.7357.      Train RMSE: 13992.5163, Val RMSE: 2.8935\n",
      "Epoch 25.      Train Loss: 8.0145, Val Loss: 3.7351.      Train RMSE: 3024445.3232, Val RMSE: 2.8935\n",
      "Epoch 30.      Train Loss: 1.1812, Val Loss: 0.6532.      Train RMSE: 4.7566, Val RMSE: 5.1834\n",
      "Epoch 35.      Train Loss: 1.0389, Val Loss: 0.5301.      Train RMSE: 5.7759, Val RMSE: 5.4199\n",
      "Epoch 40.      Train Loss: 0.8871, Val Loss: 0.4080.      Train RMSE: 5.8357, Val RMSE: 7.2361\n",
      "Epoch 45.      Train Loss: 0.7567, Val Loss: 0.2796.      Train RMSE: 5.6165, Val RMSE: 6.0681\n",
      "Epoch 50.      Train Loss: 0.6677, Val Loss: 0.1425.      Train RMSE: 7.8916, Val RMSE: 5.5899\n",
      "Epoch 55.      Train Loss: 0.6258, Val Loss: 0.1011.      Train RMSE: 12.0915, Val RMSE: 5.3066\n",
      "Epoch 60.      Train Loss: 0.6131, Val Loss: 0.1366.      Train RMSE: 10.1601, Val RMSE: 6.8139\n",
      "Epoch 65.      Train Loss: 0.6116, Val Loss: 0.0786.      Train RMSE: 7.7923, Val RMSE: 3.6723\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 9/20\n",
      "Epoch 1.      Train Loss: 23.0592, Val Loss: 20.3163.      Train RMSE: 14571.1095, Val RMSE: 2.8935\n",
      "Epoch 5.      Train Loss: 14.3858, Val Loss: 12.3280.      Train RMSE: 9917.5348, Val RMSE: 2.8935\n",
      "Epoch 10.      Train Loss: 6.0420, Val Loss: 5.8870.      Train RMSE: 89340.0085, Val RMSE: 85206.3262\n",
      "Epoch 15.      Train Loss: 6.5085, Val Loss: 5.0966.      Train RMSE: 9895.0436, Val RMSE: 2.8935\n",
      "Epoch 20.      Train Loss: 2.6000, Val Loss: 1.9151.      Train RMSE: 47.0638, Val RMSE: 30.4204\n",
      "Epoch 25.      Train Loss: 1.9725, Val Loss: 1.2310.      Train RMSE: 11.6073, Val RMSE: 7.3534\n",
      "Epoch 30.      Train Loss: 1.5444, Val Loss: 0.8466.      Train RMSE: 5.3786, Val RMSE: 4.3471\n",
      "Epoch 35.      Train Loss: 1.2451, Val Loss: 0.5696.      Train RMSE: 4.3993, Val RMSE: 3.5641\n",
      "Epoch 40.      Train Loss: 1.1234, Val Loss: 0.3799.      Train RMSE: 57310.2825, Val RMSE: 4.5671\n",
      "Epoch 45.      Train Loss: 0.9496, Val Loss: 0.2819.      Train RMSE: 3.7837, Val RMSE: 3.4530\n",
      "Epoch 50.      Train Loss: 1.3341, Val Loss: 1.9623.      Train RMSE: 16188.0301, Val RMSE: 64.6461\n",
      "Epoch 55.      Train Loss: 0.9692, Val Loss: 0.3350.      Train RMSE: 83.6985, Val RMSE: 66.7698\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 10/20\n",
      "Epoch 1.      Train Loss: 21.7051, Val Loss: 20.0578.      Train RMSE: 831607.0063, Val RMSE: 640330.0013\n",
      "Epoch 5.      Train Loss: 11.5350, Val Loss: 10.7943.      Train RMSE: 416538.2364, Val RMSE: 329659.9557\n",
      "Epoch 10.      Train Loss: 5.4236, Val Loss: 5.3231.      Train RMSE: 129031.8847, Val RMSE: 93736.8272\n",
      "Epoch 15.      Train Loss: 2.7583, Val Loss: 1.9838.      Train RMSE: 9895.1325, Val RMSE: 23935.0494\n",
      "Epoch 20.      Train Loss: 1.9500, Val Loss: 1.2125.      Train RMSE: 9894.8665, Val RMSE: 21328.1058\n",
      "Epoch 25.      Train Loss: 1.4038, Val Loss: 0.6817.      Train RMSE: 13992.6169, Val RMSE: 16991.2005\n",
      "Epoch 30.      Train Loss: 1.0909, Val Loss: 0.4337.      Train RMSE: 9894.2149, Val RMSE: 7.9148\n",
      "Epoch 35.      Train Loss: 0.9255, Val Loss: 0.2489.      Train RMSE: 6.0881, Val RMSE: 4.4944\n",
      "Epoch 40.      Train Loss: 0.8618, Val Loss: 0.2270.      Train RMSE: 4.2683, Val RMSE: 3.5505\n",
      "Epoch 45.      Train Loss: 0.8488, Val Loss: 0.1613.      Train RMSE: 3.9747, Val RMSE: 3.2509\n",
      "Epoch 50.      Train Loss: 0.6188, Val Loss: 0.0765.      Train RMSE: 5.1954, Val RMSE: 3.9852\n",
      "Epoch 55.      Train Loss: 0.6124, Val Loss: 0.1452.      Train RMSE: 6.0408, Val RMSE: 7.9757\n",
      "Epoch 60.      Train Loss: 0.6137, Val Loss: 0.1326.      Train RMSE: 7.0899, Val RMSE: 7.3334\n",
      "Epoch 65.      Train Loss: 0.6150, Val Loss: 0.1456.      Train RMSE: 6.3759, Val RMSE: 6.4777\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 11/20\n",
      "Epoch 1.      Train Loss: 35.7124, Val Loss: 31.3872.      Train RMSE: 443503.9489, Val RMSE: 431184.9995\n",
      "Epoch 5.      Train Loss: 18.4173, Val Loss: 16.1960.      Train RMSE: 396555.2587, Val RMSE: 354676.3015\n",
      "Epoch 10.      Train Loss: 6.6203, Val Loss: 6.2574.      Train RMSE: 144154.4061, Val RMSE: 137848.3489\n",
      "Epoch 15.      Train Loss: 2.9942, Val Loss: 3.1798.      Train RMSE: 36654.6696, Val RMSE: 4558.5710\n",
      "Epoch 20.      Train Loss: 1.7458, Val Loss: 2.0802.      Train RMSE: 3642.2564, Val RMSE: 89.1141\n",
      "Epoch 25.      Train Loss: 1.3784, Val Loss: 1.7613.      Train RMSE: 89.9371, Val RMSE: 86.8733\n",
      "Epoch 30.      Train Loss: 0.9670, Val Loss: 0.4815.      Train RMSE: 29.8041, Val RMSE: 26.1643\n",
      "Epoch 35.      Train Loss: 0.7760, Val Loss: 0.3017.      Train RMSE: 29.4758, Val RMSE: 25.5518\n",
      "Epoch 40.      Train Loss: 0.7214, Val Loss: 0.2919.      Train RMSE: 27.8415, Val RMSE: 34.5430\n",
      "Epoch 45.      Train Loss: 0.6124, Val Loss: 0.1060.      Train RMSE: 3962.4591, Val RMSE: 4.5516\n",
      "Epoch 50.      Train Loss: 0.6136, Val Loss: 0.2110.      Train RMSE: 13.1325, Val RMSE: 9.0415\n",
      "Epoch 55.      Train Loss: 0.6238, Val Loss: 0.1644.      Train RMSE: 7.0978, Val RMSE: 7.3703\n",
      "Epoch 60.      Train Loss: 0.6126, Val Loss: 0.1412.      Train RMSE: 19.3778, Val RMSE: 5.2531\n",
      "Epoch 65.      Train Loss: 0.6114, Val Loss: 0.2042.      Train RMSE: 10.3587, Val RMSE: 14.1698\n",
      "Epoch 70.      Train Loss: 0.6122, Val Loss: 0.1899.      Train RMSE: 10.3514, Val RMSE: 10.8349\n",
      "Epoch 75.      Train Loss: 0.6121, Val Loss: 0.0774.      Train RMSE: 6.4374, Val RMSE: 4.2115\n",
      "Epoch 80.      Train Loss: 0.6113, Val Loss: 0.1534.      Train RMSE: 6.9292, Val RMSE: 6.5756\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 12/20\n",
      "Epoch 1.      Train Loss: 30.7249, Val Loss: 29.6917.      Train RMSE: 3259645.6577, Val RMSE: 3263456.8332\n",
      "Epoch 5.      Train Loss: 20.4305, Val Loss: 20.1545.      Train RMSE: 3264295.5010, Val RMSE: 3260973.1383\n",
      "Epoch 10.      Train Loss: 13.3790, Val Loss: 13.5306.      Train RMSE: 3266740.0778, Val RMSE: 3268597.2531\n",
      "Epoch 15.      Train Loss: 8.4551, Val Loss: 4.8641.      Train RMSE: 2532204.4826, Val RMSE: 2.8935\n",
      "Epoch 20.      Train Loss: 1.5828, Val Loss: 1.0988.      Train RMSE: 17455.1705, Val RMSE: 27.2508\n",
      "Epoch 25.      Train Loss: 1.2400, Val Loss: 0.8190.      Train RMSE: 17137.5574, Val RMSE: 45.9611\n",
      "Epoch 30.      Train Loss: 0.9392, Val Loss: 0.4699.      Train RMSE: 6852.2881, Val RMSE: 30.1258\n",
      "Epoch 35.      Train Loss: 0.7807, Val Loss: 0.2911.      Train RMSE: 98.3614, Val RMSE: 21.2284\n",
      "Epoch 40.      Train Loss: 0.7273, Val Loss: 0.2533.      Train RMSE: 78.6959, Val RMSE: 23.2093\n",
      "Epoch 45.      Train Loss: 0.7206, Val Loss: 0.3246.      Train RMSE: 51.3764, Val RMSE: 44.5725\n",
      "Epoch 50.      Train Loss: 0.6115, Val Loss: 0.1298.      Train RMSE: 8.1249, Val RMSE: 6.6720\n",
      "Epoch 55.      Train Loss: 0.6108, Val Loss: 0.2115.      Train RMSE: 10.5892, Val RMSE: 11.4013\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 13/20\n",
      "Epoch 1.      Train Loss: 30.0429, Val Loss: 28.9118.      Train RMSE: 9895.8003, Val RMSE: 2.8962\n",
      "Epoch 5.      Train Loss: 9.3653, Val Loss: 7.5624.      Train RMSE: 66.7215, Val RMSE: 2.8936\n",
      "Epoch 10.      Train Loss: 5.8939, Val Loss: 4.4049.      Train RMSE: 2.4810, Val RMSE: 2.8936\n",
      "Epoch 15.      Train Loss: 2.6720, Val Loss: 3.4418.      Train RMSE: 280051.5244, Val RMSE: 207635.5305\n",
      "Epoch 20.      Train Loss: 1.2594, Val Loss: 0.6269.      Train RMSE: 13.8063, Val RMSE: 7.8872\n",
      "Epoch 25.      Train Loss: 1.0076, Val Loss: 0.3164.      Train RMSE: 4.9844, Val RMSE: 3.6240\n",
      "Epoch 30.      Train Loss: 0.6535, Val Loss: 0.1693.      Train RMSE: 5.0829, Val RMSE: 6.4512\n",
      "Epoch 35.      Train Loss: 1.4681, Val Loss: 2.1220.      Train RMSE: 38.9705, Val RMSE: 53.5444\n",
      "Epoch 40.      Train Loss: 0.9120, Val Loss: 0.2269.      Train RMSE: 38.0889, Val RMSE: 21.9017\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 14/20\n",
      "Epoch 1.      Train Loss: 41.3027, Val Loss: 39.6309.      Train RMSE: 22025.4297, Val RMSE: 22026.0370\n",
      "Epoch 5.      Train Loss: 30.9424, Val Loss: 29.9942.      Train RMSE: 22025.4667, Val RMSE: 22026.0370\n",
      "Epoch 10.      Train Loss: 23.8741, Val Loss: 23.3898.      Train RMSE: 22025.8699, Val RMSE: 22026.0370\n",
      "Epoch 15.      Train Loss: 20.3776, Val Loss: 20.1555.      Train RMSE: 22025.9701, Val RMSE: 22026.0370\n",
      "Epoch 20.      Train Loss: 18.9213, Val Loss: 18.8452.      Train RMSE: 21933.8905, Val RMSE: 22026.0370\n",
      "Epoch 25.      Train Loss: 11.2601, Val Loss: 0.3837.      Train RMSE: 17850.1101, Val RMSE: 41.4686\n",
      "Epoch 30.      Train Loss: 0.7519, Val Loss: 0.2984.      Train RMSE: 9894.2610, Val RMSE: 29.5212\n",
      "Epoch 35.      Train Loss: 0.6237, Val Loss: 0.0837.      Train RMSE: 9894.2141, Val RMSE: 4.7229\n",
      "Epoch 40.      Train Loss: 0.6143, Val Loss: 0.1807.      Train RMSE: 9842.6838, Val RMSE: 7.9818\n",
      "Epoch 45.      Train Loss: 0.6129, Val Loss: 0.1628.      Train RMSE: 9894.2177, Val RMSE: 9.3783\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 15/20\n",
      "Epoch 1.      Train Loss: 33.5103, Val Loss: 30.3316.      Train RMSE: 1003857.3474, Val RMSE: 880065.6360\n",
      "Epoch 5.      Train Loss: 17.9782, Val Loss: 16.8865.      Train RMSE: 574020.8487, Val RMSE: 426990.0812\n",
      "Epoch 10.      Train Loss: 6.3043, Val Loss: 6.1586.      Train RMSE: 218702.1219, Val RMSE: 171433.0167\n",
      "Epoch 15.      Train Loss: 2.7073, Val Loss: 2.9564.      Train RMSE: 33804.9615, Val RMSE: 1725.0797\n",
      "Epoch 20.      Train Loss: 1.4080, Val Loss: 0.8110.      Train RMSE: 94.3444, Val RMSE: 86.7492\n",
      "Epoch 25.      Train Loss: 1.1045, Val Loss: 0.5083.      Train RMSE: 62.1468, Val RMSE: 45.8632\n",
      "Epoch 30.      Train Loss: 0.7215, Val Loss: 0.2295.      Train RMSE: 6.2582, Val RMSE: 7.3760\n",
      "Epoch 35.      Train Loss: 0.6503, Val Loss: 0.1434.      Train RMSE: 5.7048, Val RMSE: 4.2164\n",
      "Epoch 40.      Train Loss: 0.6178, Val Loss: 0.0844.      Train RMSE: 6.4170, Val RMSE: 4.1524\n",
      "Epoch 45.      Train Loss: 0.6144, Val Loss: 0.1141.      Train RMSE: 10.3774, Val RMSE: 5.4939\n",
      "Epoch 50.      Train Loss: 0.6145, Val Loss: 0.1345.      Train RMSE: 6.9679, Val RMSE: 8.6585\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 16/20\n",
      "Epoch 1.      Train Loss: 34.1414, Val Loss: 32.0597.      Train RMSE: 670262.6105, Val RMSE: 564705.1596\n",
      "Epoch 5.      Train Loss: 17.8611, Val Loss: 16.7368.      Train RMSE: 773628.0609, Val RMSE: 599744.3244\n",
      "Epoch 10.      Train Loss: 8.2226, Val Loss: 8.0192.      Train RMSE: 389138.0627, Val RMSE: 296422.0507\n",
      "Epoch 15.      Train Loss: 4.8329, Val Loss: 4.1600.      Train RMSE: 137145.5416, Val RMSE: 110453.9509\n",
      "Epoch 20.      Train Loss: 2.7261, Val Loss: 2.0601.      Train RMSE: 9969.1255, Val RMSE: 91.9904\n",
      "Epoch 25.      Train Loss: 1.8255, Val Loss: 2.1429.      Train RMSE: 27106.2585, Val RMSE: 37021.5989\n",
      "Epoch 30.      Train Loss: 1.2044, Val Loss: 0.5685.      Train RMSE: 9894.3903, Val RMSE: 42.0079\n",
      "Epoch 35.      Train Loss: 0.9781, Val Loss: 0.3289.      Train RMSE: 9894.2153, Val RMSE: 4.6432\n",
      "Epoch 40.      Train Loss: 1.3268, Val Loss: 1.7975.      Train RMSE: 120.3882, Val RMSE: 79.2164\n",
      "Epoch 45.      Train Loss: 0.9543, Val Loss: 0.2843.      Train RMSE: 9894.4934, Val RMSE: 52.8230\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 17/20\n",
      "Epoch 1.      Train Loss: 37.5182, Val Loss: 36.0888.      Train RMSE: 22025.0628, Val RMSE: 22023.9194\n",
      "Epoch 5.      Train Loss: 28.8579, Val Loss: 28.0871.      Train RMSE: 24145.4632, Val RMSE: 22025.3309\n",
      "Epoch 10.      Train Loss: 23.2486, Val Loss: 22.8765.      Train RMSE: 22025.6665, Val RMSE: 22025.3309\n",
      "Epoch 15.      Train Loss: 20.5952, Val Loss: 20.4262.      Train RMSE: 22025.6687, Val RMSE: 22025.3309\n",
      "Epoch 20.      Train Loss: 2.4165, Val Loss: 3.1106.      Train RMSE: 71.9034, Val RMSE: 37.4693\n",
      "Epoch 25.      Train Loss: 1.7487, Val Loss: 2.1174.      Train RMSE: 107.5744, Val RMSE: 82.2083\n",
      "Epoch 30.      Train Loss: 1.2141, Val Loss: 0.7453.      Train RMSE: 29.6566, Val RMSE: 34.7917\n",
      "Epoch 35.      Train Loss: 0.9742, Val Loss: 0.4958.      Train RMSE: 9894.2480, Val RMSE: 25.6341\n",
      "Epoch 40.      Train Loss: 0.7436, Val Loss: 0.3086.      Train RMSE: 9894.2088, Val RMSE: 7.2627\n",
      "Epoch 45.      Train Loss: 0.6733, Val Loss: 0.2172.      Train RMSE: 4798.3741, Val RMSE: 8.7159\n",
      "Epoch 50.      Train Loss: 0.6305, Val Loss: 0.1406.      Train RMSE: 41.9651, Val RMSE: 6.5736\n",
      "Epoch 55.      Train Loss: 0.6132, Val Loss: 0.2085.      Train RMSE: 22.6489, Val RMSE: 11.0894\n",
      "Epoch 60.      Train Loss: 0.6130, Val Loss: 0.1285.      Train RMSE: 7.2880, Val RMSE: 6.4140\n",
      "Epoch 65.      Train Loss: 0.6131, Val Loss: 0.1694.      Train RMSE: 498.7538, Val RMSE: 7.9317\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 18/20\n",
      "Epoch 1.      Train Loss: 32.5840, Val Loss: 29.8765.      Train RMSE: 814032.2200, Val RMSE: 799991.8080\n",
      "Epoch 5.      Train Loss: 20.2327, Val Loss: 18.6148.      Train RMSE: 440100.0939, Val RMSE: 394638.3235\n",
      "Epoch 10.      Train Loss: 8.4869, Val Loss: 7.8724.      Train RMSE: 201663.7360, Val RMSE: 170165.3852\n",
      "Epoch 15.      Train Loss: 3.4202, Val Loss: 3.4649.      Train RMSE: 109332.1668, Val RMSE: 66420.6965\n",
      "Epoch 20.      Train Loss: 1.9781, Val Loss: 1.2721.      Train RMSE: 34794.5320, Val RMSE: 4650.6212\n",
      "Epoch 25.      Train Loss: 1.4857, Val Loss: 0.9354.      Train RMSE: 7356.4037, Val RMSE: 50.0644\n",
      "Epoch 30.      Train Loss: 1.0457, Val Loss: 0.5562.      Train RMSE: 1421.6026, Val RMSE: 6.5413\n",
      "Epoch 35.      Train Loss: 0.8427, Val Loss: 0.3516.      Train RMSE: 181.3254, Val RMSE: 6.6263\n",
      "Epoch 40.      Train Loss: 0.7152, Val Loss: 0.2229.      Train RMSE: 298.3734, Val RMSE: 6.1689\n",
      "Epoch 45.      Train Loss: 0.6493, Val Loss: 0.2464.      Train RMSE: 7.2913, Val RMSE: 10.7442\n",
      "Epoch 50.      Train Loss: 0.6196, Val Loss: 0.1344.      Train RMSE: 7.8870, Val RMSE: 6.2073\n",
      "Epoch 55.      Train Loss: 0.6149, Val Loss: 0.1451.      Train RMSE: 29.3817, Val RMSE: 8.3177\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 19/20\n",
      "Epoch 1.      Train Loss: 27.5914, Val Loss: 26.7698.      Train RMSE: 314604.2370, Val RMSE: 238242.5767\n",
      "Epoch 5.      Train Loss: 14.8416, Val Loss: 14.3975.      Train RMSE: 250029.0548, Val RMSE: 177414.9436\n",
      "Epoch 10.      Train Loss: 7.3238, Val Loss: 7.1286.      Train RMSE: 103206.6578, Val RMSE: 74025.8290\n",
      "Epoch 15.      Train Loss: 3.7173, Val Loss: 3.7698.      Train RMSE: 10780.4642, Val RMSE: 140.0650\n",
      "Epoch 20.      Train Loss: 5.1199, Val Loss: 3.7063.      Train RMSE: 2.5282, Val RMSE: 2.8935\n",
      "Epoch 25.      Train Loss: 1.4756, Val Loss: 1.9107.      Train RMSE: 84.9171, Val RMSE: 86.3462\n",
      "Epoch 30.      Train Loss: 0.9709, Val Loss: 0.5261.      Train RMSE: 29.1740, Val RMSE: 36.6386\n",
      "Epoch 35.      Train Loss: 0.6866, Val Loss: 0.0812.      Train RMSE: 17.3200, Val RMSE: 5.2497\n",
      "Epoch 40.      Train Loss: 0.6184, Val Loss: 0.1307.      Train RMSE: 5.4681, Val RMSE: 6.0707\n",
      "Epoch 45.      Train Loss: 0.6142, Val Loss: 0.0996.      Train RMSE: 5.9903, Val RMSE: 5.7508\n",
      "Epoch 50.      Train Loss: 0.6137, Val Loss: 0.1252.      Train RMSE: 6.1448, Val RMSE: 7.8752\n",
      "Epoch 55.      Train Loss: 0.6135, Val Loss: 0.1424.      Train RMSE: 6.8167, Val RMSE: 7.8013\n",
      "Epoch 60.      Train Loss: 0.6125, Val Loss: 0.1356.      Train RMSE: 7.7139, Val RMSE: 7.9730\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 20/20\n",
      "Epoch 1.      Train Loss: 39.5040, Val Loss: 37.9207.      Train RMSE: 94.3105, Val RMSE: 249.4485\n",
      "Epoch 5.      Train Loss: 29.8853, Val Loss: 29.0313.      Train RMSE: 21659.2336, Val RMSE: 22022.5056\n",
      "Epoch 10.      Train Loss: 23.6626, Val Loss: 23.2482.      Train RMSE: 22025.2655, Val RMSE: 22026.0370\n",
      "Epoch 15.      Train Loss: 20.6922, Val Loss: 20.5018.      Train RMSE: 94.3046, Val RMSE: 2.8957\n",
      "Epoch 20.      Train Loss: 8.5437, Val Loss: 9.1125.      Train RMSE: 3268911.6250, Val RMSE: 3269016.5158\n",
      "Epoch 25.      Train Loss: 1.4684, Val Loss: 0.8385.      Train RMSE: 53.9677, Val RMSE: 48.4302\n",
      "Epoch 30.      Train Loss: 1.0208, Val Loss: 0.4502.      Train RMSE: 13992.6066, Val RMSE: 4.0795\n",
      "Epoch 35.      Train Loss: 0.8106, Val Loss: 0.2832.      Train RMSE: 13992.6809, Val RMSE: 4.7685\n",
      "Epoch 40.      Train Loss: 0.6936, Val Loss: 0.1740.      Train RMSE: 13656.6784, Val RMSE: 4.6136\n",
      "Epoch 45.      Train Loss: 0.6366, Val Loss: 0.0474.      Train RMSE: 9.9673, Val RMSE: 2.9142\n",
      "Epoch 50.      Train Loss: 0.6163, Val Loss: 0.1610.      Train RMSE: 5.3318, Val RMSE: 6.2607\n",
      "Epoch 55.      Train Loss: 0.6131, Val Loss: 0.1903.      Train RMSE: 16.0791, Val RMSE: 8.8767\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "h, batch_size, lr, l2_lambda = 10, 256, 1e-4, 1\n",
    "\n",
    "train_loader, val_loader, test_loader = create_datasets(source1_array, source2_array, target_array, batch_size, h)\n",
    "\n",
    "print(f\"\\n📊 h: {h}, batch_size:{batch_size}, lr: {lr}, l2_lambda:{l2_lambda}\")\n",
    "\n",
    "ensemble, losses = train_tme_ensemble(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    d1=d1,  # number of features of the 1st source\n",
    "    d2=d2,  # number of features of the 2nd source\n",
    "    h=h,  # lag length\n",
    "    num_models=20,#20,\n",
    "    device=device,\n",
    "    lr=lr,\n",
    "    weight_decay=0.1,\n",
    "    l2_lambda=l2_lambda,\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    adam=True\n",
    ")\n",
    "\n",
    "y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, test_loader, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "aef179d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, test_loader, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "43920aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(65.15051503257351), 56.43250055867342)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_final = np.array(y_preds*df.iloc[-len(y_preds):]['mean_volume'])\n",
    "y_preds_final_median = np.array(y_preds_median*df.iloc[-len(y_preds_median):]['mean_volume'])\n",
    "y_true_final = df.iloc[-len(y_preds):]['total_volume'].values\n",
    "np.sqrt(mean_squared_error(y_true_final, y_preds_final)), mean_absolute_error(y_true_final, y_preds_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3ae71ceb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(42.22574373130067), 38.1972632905313)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_true_final, y_preds_final_median)), mean_absolute_error(y_true_final, y_preds_final_median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19b0895f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(25279443376.61549), 23415079202.145546)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_final = np.array(y_preds*df.iloc[-len(y_preds):]['mean_volume'])\n",
    "y_preds_final_median = np.array(y_preds_median*df.iloc[-len(y_preds_median):]['mean_volume'])\n",
    "y_true_final = df.iloc[-len(y_preds):]['total_volume'].values\n",
    "np.sqrt(mean_squared_error(y_true_final, y_preds_final)), mean_absolute_error(y_true_final, y_preds_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d1a602c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(18.93623096257617), 10.955864529347135)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(mean_squared_error(y_true_final, y_preds_final_median)), mean_absolute_error(y_true_final, y_preds_final_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c9e3c0",
   "metadata": {},
   "source": [
    "### Trial with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1385f656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0005, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/20\n",
      "Epoch 1, Train Loss: 33.9120, Val Loss: 28.0676\n",
      "Epoch 5, Train Loss: 1.7732, Val Loss: 2.7323\n",
      "Epoch 10, Train Loss: 1.6235, Val Loss: 2.5926\n",
      "Epoch 15, Train Loss: 1.5406, Val Loss: 2.6213\n",
      "Epoch 20, Train Loss: 1.5376, Val Loss: 2.6174\n",
      "Epoch 25, Train Loss: 1.5356, Val Loss: 2.6447\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/20\n",
      "Epoch 1, Train Loss: 17.6710, Val Loss: 12.3392\n",
      "Epoch 5, Train Loss: 9.1392, Val Loss: 18.6373\n",
      "Epoch 10, Train Loss: 18.4561, Val Loss: 18.4555\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/20\n",
      "Epoch 1, Train Loss: 17.9674, Val Loss: 12.0709\n",
      "Epoch 5, Train Loss: 2.2482, Val Loss: 3.0805\n",
      "Epoch 10, Train Loss: 1.5470, Val Loss: 2.6166\n",
      "Epoch 15, Train Loss: 1.5362, Val Loss: 2.5980\n",
      "Epoch 20, Train Loss: 1.5365, Val Loss: 2.6260\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 4/20\n",
      "Epoch 1, Train Loss: 32.3310, Val Loss: 21.2613\n",
      "Epoch 5, Train Loss: 3.3182, Val Loss: 3.6531\n",
      "Epoch 10, Train Loss: 18.5900, Val Loss: 18.5549\n",
      "Epoch 15, Train Loss: 1.5499, Val Loss: 2.6130\n",
      "Epoch 20, Train Loss: 1.5359, Val Loss: 2.6072\n",
      "Epoch 25, Train Loss: 1.5356, Val Loss: 2.6173\n",
      "Epoch 30, Train Loss: 1.5346, Val Loss: 2.5880\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 5/20\n",
      "Epoch 1, Train Loss: 34.7292, Val Loss: 28.6765\n",
      "Epoch 5, Train Loss: 14.5922, Val Loss: 18.8680\n",
      "Epoch 10, Train Loss: 1.8086, Val Loss: 2.8865\n",
      "Epoch 15, Train Loss: 1.7987, Val Loss: 2.8519\n",
      "Epoch 20, Train Loss: 1.7743, Val Loss: 2.8353\n",
      "Epoch 25, Train Loss: 8.4283, Val Loss: 18.6257\n",
      "Epoch 30, Train Loss: 1.6164, Val Loss: 2.6829\n",
      "Epoch 35, Train Loss: 1.5423, Val Loss: 2.6126\n",
      "Epoch 40, Train Loss: 1.5438, Val Loss: 2.6067\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 6/20\n",
      "Epoch 1, Train Loss: 34.4178, Val Loss: 28.4199\n",
      "Epoch 5, Train Loss: 18.8079, Val Loss: 18.6934\n",
      "Epoch 10, Train Loss: 1.6620, Val Loss: 2.7419\n",
      "Epoch 15, Train Loss: 1.5958, Val Loss: 2.5979\n",
      "Epoch 20, Train Loss: 1.5466, Val Loss: 2.5775\n",
      "Epoch 25, Train Loss: 1.5353, Val Loss: 2.6277\n",
      "Epoch 30, Train Loss: 1.7346, Val Loss: 2.6132\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 7/20\n",
      "Epoch 1, Train Loss: 16.1701, Val Loss: 10.6025\n",
      "Epoch 5, Train Loss: 1.7773, Val Loss: 2.7460\n",
      "Epoch 10, Train Loss: 18.4323, Val Loss: 18.4260\n",
      "Epoch 15, Train Loss: 10.3708, Val Loss: 2.5882\n",
      "Epoch 20, Train Loss: 1.5370, Val Loss: 2.6080\n",
      "Epoch 25, Train Loss: 1.5362, Val Loss: 2.5835\n",
      "Epoch 30, Train Loss: 18.4271, Val Loss: 18.4265\n",
      "Epoch 35, Train Loss: 18.4231, Val Loss: 18.4228\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 8/20\n",
      "Epoch 1, Train Loss: 30.5871, Val Loss: 25.7182\n",
      "Epoch 5, Train Loss: 15.8587, Val Loss: 2.7364\n",
      "Epoch 10, Train Loss: 1.5387, Val Loss: 2.5934\n",
      "Epoch 15, Train Loss: 1.5357, Val Loss: 2.6030\n",
      "Epoch 20, Train Loss: 1.5343, Val Loss: 2.6118\n",
      "Epoch 25, Train Loss: 1.5372, Val Loss: 2.5869\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 9/20\n",
      "Epoch 1, Train Loss: 18.3499, Val Loss: 12.0188\n",
      "Epoch 5, Train Loss: 1.8175, Val Loss: 2.7609\n",
      "Epoch 10, Train Loss: 18.4300, Val Loss: 18.4290\n",
      "Epoch 15, Train Loss: 18.4258, Val Loss: 18.4256\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 10/20\n",
      "Epoch 1, Train Loss: 17.2630, Val Loss: 11.4616\n",
      "Epoch 5, Train Loss: 1.7828, Val Loss: 2.7003\n",
      "Epoch 10, Train Loss: 1.5351, Val Loss: 2.6024\n",
      "Epoch 15, Train Loss: 1.5343, Val Loss: 2.6189\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 11/20\n",
      "Epoch 1, Train Loss: 31.6356, Val Loss: 26.4631\n",
      "Epoch 5, Train Loss: 1.6306, Val Loss: 2.6092\n",
      "Epoch 10, Train Loss: 1.5341, Val Loss: 2.6239\n",
      "Epoch 15, Train Loss: 1.5349, Val Loss: 2.6412\n",
      "Epoch 20, Train Loss: 1.5366, Val Loss: 2.6186\n",
      "Epoch 25, Train Loss: 1.5352, Val Loss: 2.5587\n",
      "Epoch 30, Train Loss: 1.5343, Val Loss: 2.5750\n",
      "Epoch 35, Train Loss: 1.5340, Val Loss: 2.5765\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 12/20\n",
      "Epoch 1, Train Loss: 21.0690, Val Loss: 14.7213\n",
      "Epoch 5, Train Loss: 3.7195, Val Loss: 4.4425\n",
      "Epoch 10, Train Loss: 2.5643, Val Loss: 3.5700\n",
      "Epoch 15, Train Loss: 1.9334, Val Loss: 2.9026\n",
      "Epoch 20, Train Loss: 1.5661, Val Loss: 2.6343\n",
      "Epoch 25, Train Loss: 1.5498, Val Loss: 2.6347\n",
      "Epoch 30, Train Loss: 1.5378, Val Loss: 2.6424\n",
      "Epoch 35, Train Loss: 1.5353, Val Loss: 2.6175\n",
      "Epoch 40, Train Loss: 1.5366, Val Loss: 2.6103\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 13/20\n",
      "Epoch 1, Train Loss: 31.8196, Val Loss: 27.0848\n",
      "Epoch 5, Train Loss: 3.1488, Val Loss: 4.1652\n",
      "Epoch 10, Train Loss: 2.8850, Val Loss: 3.9440\n",
      "Epoch 15, Train Loss: 2.5106, Val Loss: 3.5386\n",
      "Epoch 20, Train Loss: 4.5573, Val Loss: 3.4866\n",
      "Epoch 25, Train Loss: 1.5660, Val Loss: 2.6687\n",
      "Epoch 30, Train Loss: 1.5402, Val Loss: 2.5922\n",
      "Epoch 35, Train Loss: 2.0623, Val Loss: 3.2403\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 14/20\n",
      "Epoch 1, Train Loss: 27.7899, Val Loss: 19.5610\n",
      "Epoch 5, Train Loss: 3.1816, Val Loss: 3.5549\n",
      "Epoch 10, Train Loss: 4.9998, Val Loss: 2.7052\n",
      "Epoch 15, Train Loss: 1.5368, Val Loss: 2.5905\n",
      "Epoch 20, Train Loss: 1.9794, Val Loss: 2.6222\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 15/20\n",
      "Epoch 1, Train Loss: 33.3793, Val Loss: 27.7721\n",
      "Epoch 5, Train Loss: 5.1955, Val Loss: 4.0549\n",
      "Epoch 10, Train Loss: 11.1758, Val Loss: 3.1658\n",
      "Epoch 15, Train Loss: 1.9924, Val Loss: 3.0557\n",
      "Epoch 20, Train Loss: 1.8296, Val Loss: 2.8715\n",
      "Epoch 25, Train Loss: 1.6402, Val Loss: 2.7320\n",
      "Epoch 30, Train Loss: 1.5466, Val Loss: 2.6180\n",
      "Epoch 35, Train Loss: 1.5357, Val Loss: 2.5908\n",
      "Epoch 40, Train Loss: 1.5342, Val Loss: 2.5820\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 16/20\n",
      "Epoch 1, Train Loss: 33.1630, Val Loss: 27.7555\n",
      "Epoch 5, Train Loss: 15.2069, Val Loss: 2.7978\n",
      "Epoch 10, Train Loss: 18.4271, Val Loss: 18.4266\n",
      "Epoch 15, Train Loss: 18.4248, Val Loss: 18.4247\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 17/20\n",
      "Epoch 1, Train Loss: 31.2952, Val Loss: 22.7583\n",
      "Epoch 5, Train Loss: 4.7125, Val Loss: 4.9693\n",
      "Epoch 10, Train Loss: 19.0931, Val Loss: 18.9777\n",
      "Epoch 15, Train Loss: 18.4735, Val Loss: 18.4570\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 18/20\n",
      "Epoch 1, Train Loss: 34.4165, Val Loss: 28.4572\n",
      "Epoch 5, Train Loss: 18.6780, Val Loss: 18.5507\n",
      "Epoch 10, Train Loss: 1.5417, Val Loss: 2.6023\n",
      "Epoch 15, Train Loss: 1.5368, Val Loss: 2.5906\n",
      "Epoch 20, Train Loss: 1.5354, Val Loss: 2.5991\n",
      "Epoch 25, Train Loss: 1.5343, Val Loss: 2.5732\n",
      "Epoch 30, Train Loss: 18.4243, Val Loss: 18.4240\n",
      "Epoch 35, Train Loss: 1.5790, Val Loss: 2.6244\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 19/20\n",
      "Epoch 1, Train Loss: 22.8134, Val Loss: 16.1006\n",
      "Epoch 5, Train Loss: 3.9350, Val Loss: 4.7056\n",
      "Epoch 10, Train Loss: 2.8469, Val Loss: 3.8893\n",
      "Epoch 15, Train Loss: 2.4091, Val Loss: 3.4263\n",
      "Epoch 20, Train Loss: 1.8256, Val Loss: 2.8418\n",
      "Epoch 25, Train Loss: 1.5494, Val Loss: 2.6132\n",
      "Epoch 30, Train Loss: 13.9980, Val Loss: 18.4347\n",
      "Epoch 35, Train Loss: 1.5348, Val Loss: 2.6192\n",
      "Epoch 40, Train Loss: 18.4259, Val Loss: 18.4241\n",
      "Epoch 45, Train Loss: 18.4219, Val Loss: 18.4218\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 20/20\n",
      "Epoch 1, Train Loss: 22.1131, Val Loss: 15.3331\n",
      "Epoch 5, Train Loss: 3.4534, Val Loss: 3.9445\n",
      "Epoch 10, Train Loss: 1.6758, Val Loss: 2.7127\n",
      "Epoch 15, Train Loss: 1.5393, Val Loss: 2.6069\n",
      "Epoch 20, Train Loss: 4.1762, Val Loss: 3.1613\n",
      "Epoch 25, Train Loss: 1.5576, Val Loss: 2.6121\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "h, batch_size, lr, l2_lambda = 10, 256, 5e-4, 1\n",
    "\n",
    "train_loader, val_loader, test_loader = create_datasets(source1_array, source2_array, target_array, batch_size, h)\n",
    "\n",
    "print(f\"\\n📊 h: {h}, batch_size:{batch_size}, lr: {lr}, l2_lambda:{l2_lambda}\")\n",
    "\n",
    "ensemble, losses = train_tme_ensemble(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    d1=d1,  # number of features of the 1st source\n",
    "    d2=d2,  # number of features of the 2nd source\n",
    "    h=h,  # lag length\n",
    "    num_models=20,#20,\n",
    "    device=device,\n",
    "    lr=lr,\n",
    "    weight_decay=0.1,\n",
    "    l2_lambda=l2_lambda,\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    adam=True\n",
    ")\n",
    "\n",
    "y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, test_loader, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0cef44ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(255106244.2784626), 236295199.18338215)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds_final = np.array(y_preds*df.iloc[-len(y_preds):]['mean_volume'])\n",
    "y_preds_final_median = np.array(y_preds_median*df.iloc[-len(y_preds_median):]['mean_volume'])\n",
    "y_true_final = df.iloc[-len(y_preds):]['total_volume'].values\n",
    "np.sqrt(mean_squared_error(y_true_final, y_preds_final)), mean_absolute_error(y_true_final, y_preds_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8883cffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39995111.43428375, 39995111.3915224 , 39995111.40449377, ...,\n",
       "       39995111.41738187, 39995111.4258525 , 39995111.426695  ])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "19fba3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, test_loader, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "51189d1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39995111.43428375, 39995111.3915224 , 39995111.40449377, ...,\n",
       "       39995111.41738187, 39995111.4258525 , 39995111.426695  ])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c6d55fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, val_loader, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cc1ce113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39995116.39778644, 39995116.37797287, 39995116.7019389 , ...,\n",
       "       39995111.36725745, 39995111.35094261, 39995111.38108386])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26941673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_fin_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
