{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81855d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy import cumsum, log, polyfit, sqrt, std, subtract\n",
    "from numpy.random import randn\n",
    "import pandas as pd\n",
    "# from pandas_datareader import data as web\n",
    "import seaborn as sns\n",
    "from pylab import rcParams \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from arch import arch_model\n",
    "from numpy.linalg import LinAlgError\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as tsa\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, q_stat, adfuller\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import probplot, moment\n",
    "from arch import arch_model\n",
    "from arch.univariate import ConstantMean, GARCH, Normal\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from itertools import product\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4fd1a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_txn_data, preprocess_txn_data, compute_lob_features, create_lob_dataset, merge_txn_and_lob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faa3ced1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"darkgrid\", color_codes=True)\n",
    "rcParams['figure.figsize'] = 8,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a919d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10d570f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trx Data loaded successfully.\n",
      "preprocessed lob Data loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>buy_volume</th>\n",
       "      <th>sell_volume</th>\n",
       "      <th>buy_txn</th>\n",
       "      <th>sell_txn</th>\n",
       "      <th>volume_imbalance</th>\n",
       "      <th>txn_imbalance</th>\n",
       "      <th>total_volume</th>\n",
       "      <th>mean_volume</th>\n",
       "      <th>deseasoned_total_volume</th>\n",
       "      <th>log_deseasoned_total_volume</th>\n",
       "      <th>ask_volume</th>\n",
       "      <th>bid_volume</th>\n",
       "      <th>ask_slope_1</th>\n",
       "      <th>ask_slope_5</th>\n",
       "      <th>ask_slope_10</th>\n",
       "      <th>bid_slope_1</th>\n",
       "      <th>bid_slope_5</th>\n",
       "      <th>bid_slope_10</th>\n",
       "      <th>spread</th>\n",
       "      <th>lob_volume_imbalance</th>\n",
       "      <th>slope_imbalance_1</th>\n",
       "      <th>slope_imbalance_5</th>\n",
       "      <th>slope_imbalance_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5819</th>\n",
       "      <td>2018-06-04 22:00:05+00:00</td>\n",
       "      <td>0.059804</td>\n",
       "      <td>0.730357</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.670553</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.790162</td>\n",
       "      <td>4.380444</td>\n",
       "      <td>0.180384</td>\n",
       "      <td>-1.712667</td>\n",
       "      <td>2695.804973</td>\n",
       "      <td>586356.113693</td>\n",
       "      <td>1761.630667</td>\n",
       "      <td>2695.804973</td>\n",
       "      <td>2695.804973</td>\n",
       "      <td>3.972121</td>\n",
       "      <td>53.502450</td>\n",
       "      <td>160.246934</td>\n",
       "      <td>6.19</td>\n",
       "      <td>583660.308720</td>\n",
       "      <td>1757.658546</td>\n",
       "      <td>2642.302523</td>\n",
       "      <td>2535.558040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5820</th>\n",
       "      <td>2018-06-04 22:01:05+00:00</td>\n",
       "      <td>0.089359</td>\n",
       "      <td>0.849477</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.760118</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.938836</td>\n",
       "      <td>3.692009</td>\n",
       "      <td>0.254289</td>\n",
       "      <td>-1.369285</td>\n",
       "      <td>2699.165417</td>\n",
       "      <td>586350.938081</td>\n",
       "      <td>1765.312385</td>\n",
       "      <td>2699.165417</td>\n",
       "      <td>2699.165417</td>\n",
       "      <td>4.017044</td>\n",
       "      <td>52.408273</td>\n",
       "      <td>155.071322</td>\n",
       "      <td>4.97</td>\n",
       "      <td>583651.772664</td>\n",
       "      <td>1761.295341</td>\n",
       "      <td>2646.757144</td>\n",
       "      <td>2544.094095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5821</th>\n",
       "      <td>2018-06-04 22:02:05+00:00</td>\n",
       "      <td>0.313458</td>\n",
       "      <td>0.508952</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.195494</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.822410</td>\n",
       "      <td>3.324900</td>\n",
       "      <td>0.247349</td>\n",
       "      <td>-1.396955</td>\n",
       "      <td>2657.946212</td>\n",
       "      <td>586317.596946</td>\n",
       "      <td>1723.843180</td>\n",
       "      <td>2657.946212</td>\n",
       "      <td>2657.946212</td>\n",
       "      <td>3.831055</td>\n",
       "      <td>46.578294</td>\n",
       "      <td>158.194750</td>\n",
       "      <td>4.90</td>\n",
       "      <td>583659.650734</td>\n",
       "      <td>1720.012125</td>\n",
       "      <td>2611.367918</td>\n",
       "      <td>2499.751462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5822</th>\n",
       "      <td>2018-06-04 22:03:05+00:00</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.199219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.198227</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.200211</td>\n",
       "      <td>4.128645</td>\n",
       "      <td>0.048493</td>\n",
       "      <td>-3.026331</td>\n",
       "      <td>2650.599402</td>\n",
       "      <td>586308.612876</td>\n",
       "      <td>1718.061157</td>\n",
       "      <td>2650.599402</td>\n",
       "      <td>2650.599402</td>\n",
       "      <td>3.631836</td>\n",
       "      <td>51.036074</td>\n",
       "      <td>160.641345</td>\n",
       "      <td>4.32</td>\n",
       "      <td>583658.013474</td>\n",
       "      <td>1714.429321</td>\n",
       "      <td>2599.563327</td>\n",
       "      <td>2489.958056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823</th>\n",
       "      <td>2018-06-04 22:04:05+00:00</td>\n",
       "      <td>0.172042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172042</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.172042</td>\n",
       "      <td>6.271124</td>\n",
       "      <td>0.027434</td>\n",
       "      <td>-3.595966</td>\n",
       "      <td>2650.082079</td>\n",
       "      <td>586314.173248</td>\n",
       "      <td>1715.979046</td>\n",
       "      <td>2650.082079</td>\n",
       "      <td>2650.082079</td>\n",
       "      <td>3.704804</td>\n",
       "      <td>51.092926</td>\n",
       "      <td>160.489197</td>\n",
       "      <td>4.32</td>\n",
       "      <td>583664.091169</td>\n",
       "      <td>1712.274243</td>\n",
       "      <td>2598.989153</td>\n",
       "      <td>2489.592882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      datetime  buy_volume  sell_volume  buy_txn  sell_txn  \\\n",
       "5819 2018-06-04 22:00:05+00:00    0.059804     0.730357      5.0      10.0   \n",
       "5820 2018-06-04 22:01:05+00:00    0.089359     0.849477      3.0       4.0   \n",
       "5821 2018-06-04 22:02:05+00:00    0.313458     0.508952      2.0       4.0   \n",
       "5822 2018-06-04 22:03:05+00:00    0.000992     0.199219      1.0       4.0   \n",
       "5823 2018-06-04 22:04:05+00:00    0.172042     0.000000      7.0       0.0   \n",
       "\n",
       "      volume_imbalance  txn_imbalance  total_volume  mean_volume  \\\n",
       "5819          0.670553            5.0      0.790162     4.380444   \n",
       "5820          0.760118            1.0      0.938836     3.692009   \n",
       "5821          0.195494            2.0      0.822410     3.324900   \n",
       "5822          0.198227            3.0      0.200211     4.128645   \n",
       "5823          0.172042            7.0      0.172042     6.271124   \n",
       "\n",
       "      deseasoned_total_volume  log_deseasoned_total_volume   ask_volume  \\\n",
       "5819                 0.180384                    -1.712667  2695.804973   \n",
       "5820                 0.254289                    -1.369285  2699.165417   \n",
       "5821                 0.247349                    -1.396955  2657.946212   \n",
       "5822                 0.048493                    -3.026331  2650.599402   \n",
       "5823                 0.027434                    -3.595966  2650.082079   \n",
       "\n",
       "         bid_volume  ask_slope_1  ask_slope_5  ask_slope_10  bid_slope_1  \\\n",
       "5819  586356.113693  1761.630667  2695.804973   2695.804973     3.972121   \n",
       "5820  586350.938081  1765.312385  2699.165417   2699.165417     4.017044   \n",
       "5821  586317.596946  1723.843180  2657.946212   2657.946212     3.831055   \n",
       "5822  586308.612876  1718.061157  2650.599402   2650.599402     3.631836   \n",
       "5823  586314.173248  1715.979046  2650.082079   2650.082079     3.704804   \n",
       "\n",
       "      bid_slope_5  bid_slope_10  spread  lob_volume_imbalance  \\\n",
       "5819    53.502450    160.246934    6.19         583660.308720   \n",
       "5820    52.408273    155.071322    4.97         583651.772664   \n",
       "5821    46.578294    158.194750    4.90         583659.650734   \n",
       "5822    51.036074    160.641345    4.32         583658.013474   \n",
       "5823    51.092926    160.489197    4.32         583664.091169   \n",
       "\n",
       "      slope_imbalance_1  slope_imbalance_5  slope_imbalance_10  \n",
       "5819        1757.658546        2642.302523         2535.558040  \n",
       "5820        1761.295341        2646.757144         2544.094095  \n",
       "5821        1720.012125        2611.367918         2499.751462  \n",
       "5822        1714.429321        2599.563327         2489.958056  \n",
       "5823        1712.274243        2598.989153         2489.592882  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trx_df = read_txn_data(use_load=False)\n",
    "trx_df = preprocess_txn_data(trx_df, freq='1min')\n",
    "trx_df['log_deseasoned_total_volume'] = np.log(trx_df['deseasoned_total_volume'] + 1e-07)\n",
    "\n",
    "lob_df = create_lob_dataset(use_load=False)\n",
    "\n",
    "df_merged = merge_txn_and_lob(trx_df, lob_df)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033452ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64d3e4a7",
   "metadata": {},
   "source": [
    "## TME implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60752e98",
   "metadata": {},
   "source": [
    "### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc7830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(12345)\n",
    "\n",
    "h = 5  # window length\n",
    "batch_size = 128\n",
    "\n",
    "# -----------------------------\n",
    "df = df_merged.sort_values('datetime').reset_index(drop=True)\n",
    "# STEP 1: Create time-of-day feature\n",
    "df['time_of_day'] = df['datetime'].dt.strftime('%H:%M')\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 2: Split indices (AFTER creating lags!)\n",
    "n_total = len(df)\n",
    "n_train = int(0.7 * n_total)\n",
    "n_val = int(0.1 * n_total)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 3: Create deseasonalizing map using per-time volume means from train only\n",
    "train_deseason_df = df.iloc[:n_train]\n",
    "mean_volume_by_time = train_deseason_df.groupby('time_of_day')['total_volume'].mean()\n",
    "df['mean_volume'] = df['time_of_day'].map(mean_volume_by_time)\n",
    "\n",
    "df['deseasoned_total_volume'] = df['total_volume'] / df['mean_volume']\n",
    "df['log_deseasoned_total_volume'] = np.log(df['deseasoned_total_volume'] + 1e-7)\n",
    "df['target'] = df['deseasoned_total_volume']\n",
    "\n",
    "del train_deseason_df\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 4: Define the source-specific features\n",
    "source1_cols = ['buy_volume', 'sell_volume', 'buy_txn', 'sell_txn', 'volume_imbalance', 'txn_imbalance']\n",
    "source2_cols = ['ask_volume', 'bid_volume', 'ask_slope_1', 'ask_slope_5', 'ask_slope_10', 'bid_slope_1', 'bid_slope_5', 'bid_slope_10', 'spread',\n",
    "       'lob_volume_imbalance', 'slope_imbalance_1', 'slope_imbalance_5', 'slope_imbalance_10']\n",
    "# target_col = 'log_deseasoned_total_volume'\n",
    "target_col = 'target'\n",
    "target_direct_col = 'total_volume'\n",
    "weight_col = 'mean_volume'\n",
    "datetime_col = 'datetime'\n",
    "\n",
    "# # Normalize source1 and source2 features using training data only\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "scaler2 = StandardScaler()\n",
    "\n",
    "# Fit only on training portion\n",
    "source1_train_raw = df[source1_cols].iloc[:n_train]\n",
    "source2_train_raw = df[source2_cols].iloc[:n_train]\n",
    "\n",
    "scaler1.fit(source1_train_raw)\n",
    "scaler2.fit(source2_train_raw)\n",
    "\n",
    "# Apply normalization to the whole dataset\n",
    "df[source1_cols] = scaler1.transform(df[source1_cols])\n",
    "df[source2_cols] = scaler2.transform(df[source2_cols])\n",
    "\n",
    "\n",
    "# --- Create rolling windows efficiently ---\n",
    "source1_array = df[source1_cols].values  # shape (N, F1)\n",
    "source2_array = df[source2_cols].values  # shape (N, F2)\n",
    "target_array = df[target_col].values + 1e-7  # shape (N,)\n",
    "target_direct_array = df[target_direct_col].values + 1e-7  # shape (N,)\n",
    "weight_array = df[weight_col].values  # shape (N,)\n",
    "timestamps_array = df[datetime_col].values\n",
    "\n",
    "\n",
    "# Create sliding windows ([:-1] in windows and [h:] in targets make sure the targets are matched with corresponding features)\n",
    "source1_windows = sliding_window_view(source1_array, window_shape=(h,), axis=0)[:-1]  # shape (N - h, F1, h)\n",
    "source2_windows = sliding_window_view(source2_array, window_shape=(h,), axis=0)[:-1]  # shape (N - h, F2, h)\n",
    "y = target_array[h:]\n",
    "w = weight_array[h:]\n",
    "timestamps = timestamps_array[h:]\n",
    "\n",
    "# Convert to tensors\n",
    "source1_tensor = torch.tensor(source1_windows, dtype=torch.float32)\n",
    "source2_tensor = torch.tensor(source2_windows, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "w_tensor = torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "# --- Time-based split (preserving time order) ---\n",
    "n_total = len(y_tensor)\n",
    "n_train = int(n_total * 0.7)\n",
    "n_val = int(n_total * 0.1)\n",
    "\n",
    "source1_train, source1_val, source1_test = source1_tensor[:n_train], source1_tensor[n_train:n_train + n_val], source1_tensor[n_train + n_val:]\n",
    "source2_train, source2_val, source2_test = source2_tensor[:n_train], source2_tensor[n_train:n_train + n_val], source2_tensor[n_train + n_val:]\n",
    "y_train, y_val, y_test = y_tensor[:n_train], y_tensor[n_train:n_train + n_val], y_tensor[n_train + n_val:]\n",
    "w_train, w_val, w_test = w_tensor[:n_train], w_tensor[n_train:n_train + n_val], w_tensor[n_train + n_val:]\n",
    "\n",
    "# (Optional) timestamps split for tracking\n",
    "timestamps_train = timestamps[:n_train]\n",
    "timestamps_val = timestamps[n_train:n_train + n_val]\n",
    "timestamps_test = timestamps[n_train + n_val:]\n",
    "\n",
    "# Dataset ready for PyTorch training\n",
    "train_dataset = torch.utils.data.TensorDataset(source1_train, source2_train, y_train, w_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(source1_val, source2_val, y_val, w_val)\n",
    "test_dataset = torch.utils.data.TensorDataset(source1_test, source2_test, y_test, w_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "## function for dataset creation for hyperparams search\n",
    "def create_datasets(source1_array, source2_array, target_array, weight_array, batch_size, h):\n",
    "       # Create sliding windows ([:-1] in windows and [h:] in targets make sure the targets are matched with corresponding features)\n",
    "       source1_windows = sliding_window_view(source1_array, window_shape=(h,), axis=0)[:-1]  # shape (N - h, F1, h)\n",
    "       source2_windows = sliding_window_view(source2_array, window_shape=(h,), axis=0)[:-1]  # shape (N - h, F2, h)\n",
    "       y = target_array[h:]\n",
    "       w = weight_array[h:]\n",
    "\n",
    "       # Convert to tensors\n",
    "       source1_tensor = torch.tensor(source1_windows, dtype=torch.float32)\n",
    "       source2_tensor = torch.tensor(source2_windows, dtype=torch.float32)\n",
    "       y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "       w_tensor = torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "       # --- Time-based split (preserving time order) ---\n",
    "       n_total = len(y_tensor)\n",
    "       n_train = int(n_total * 0.7)\n",
    "       n_val = int(n_total * 0.1)\n",
    "\n",
    "       source1_train, source1_val, source1_test = source1_tensor[:n_train], source1_tensor[n_train:n_train + n_val], source1_tensor[n_train + n_val:]\n",
    "       source2_train, source2_val, source2_test = source2_tensor[:n_train], source2_tensor[n_train:n_train + n_val], source2_tensor[n_train + n_val:]\n",
    "       y_train, y_val, y_test = y_tensor[:n_train], y_tensor[n_train:n_train + n_val], y_tensor[n_train + n_val:]\n",
    "       w_train, w_val, w_test = w_tensor[:n_train], w_tensor[n_train:n_train + n_val], w_tensor[n_train + n_val:]\n",
    "\n",
    "       # Dataset ready for PyTorch training\n",
    "       train_dataset = torch.utils.data.TensorDataset(source1_train, source2_train, y_train, w_train)\n",
    "       val_dataset = torch.utils.data.TensorDataset(source1_val, source2_val, y_val, w_val)\n",
    "       test_dataset = torch.utils.data.TensorDataset(source1_test, source2_test, y_test, w_test)\n",
    "\n",
    "       train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "       val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "       test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "       return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6670de",
   "metadata": {},
   "source": [
    "### TME components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "023c188a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearRegressor(nn.Module):\n",
    "    def __init__(self, d, h, latent_variable):\n",
    "        \"\"\"\n",
    "        d: number of features in the source data\n",
    "        h: number of lags in the source data\n",
    "        latent_variable (bool): if True the class is devoted for modeling latent variable z, if False => y|s_i\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_variable = latent_variable\n",
    "\n",
    "        # Mean parameters\n",
    "        self.L_mu = nn.Parameter(torch.empty(d))\n",
    "        self.R_mu = nn.Parameter(torch.empty(h))\n",
    "        self.b_mu = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        # Xavier init for 1D weight tensors\n",
    "        # nn.init.xavier_uniform_(self.L_mu.unsqueeze(0))\n",
    "        # nn.init.xavier_uniform_(self.R_mu.unsqueeze(0))\n",
    "\n",
    "        nn.init.xavier_normal_(self.L_mu.unsqueeze(0))\n",
    "        nn.init.xavier_normal_(self.R_mu.unsqueeze(0))\n",
    "\n",
    "        if not self.latent_variable:\n",
    "            self.L_sigma = nn.Parameter(torch.empty(d))\n",
    "            self.R_sigma = nn.Parameter(torch.empty(h))\n",
    "            self.b_sigma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "            # nn.init.xavier_uniform_(self.L_sigma.unsqueeze(0))\n",
    "            # nn.init.xavier_uniform_(self.R_sigma.unsqueeze(0))\n",
    "\n",
    "            nn.init.xavier_normal_(self.L_sigma.unsqueeze(0))\n",
    "            nn.init.xavier_normal_(self.R_sigma.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):  # x: (B, d, h)\n",
    "        mu = torch.einsum('bdh,d,h->b', x, self.L_mu, self.R_mu) + self.b_mu  # [B]\n",
    "        if self.latent_variable:\n",
    "            return mu\n",
    "        log_var = torch.einsum('bdh,d,h->b', x, self.L_sigma, self.R_sigma) + self.b_sigma\n",
    "        log_var = torch.clamp(log_var, min=-10, max=10)\n",
    "        var = torch.exp(log_var)  # Ensure positivity\n",
    "        return mu, var\n",
    "    \n",
    "\n",
    "class TME(nn.Module):\n",
    "    def __init__(self, d1, d2, h):\n",
    "        super().__init__()\n",
    "        self.target1 = BilinearRegressor(d1, h, latent_variable=False)\n",
    "        self.target2 = BilinearRegressor(d2, h, latent_variable=False)\n",
    "        self.latent1 = BilinearRegressor(d1, h, latent_variable=True)\n",
    "        self.latent2 = BilinearRegressor(d2, h, latent_variable=True)\n",
    "\n",
    "    def forward(self, x1, x2, return_all=False):\n",
    "        # x1: (B, d1, h), x2: (B, d2, h)\n",
    "        mu1, var1 = self.target1(x1)  # [B], [B]\n",
    "        mu2, var2 = self.target2(x2)\n",
    "\n",
    "        logit1 = self.latent1(x1)\n",
    "        logit2 = self.latent2(x2)\n",
    "\n",
    "        logits = torch.stack([logit1, logit2], dim=1)  # [B, num_sources]\n",
    "        probs = F.softmax(logits, dim=1)     # [B, num_sources]\n",
    "\n",
    "        \n",
    "        # Clamp to avoid numerical instability\n",
    "        mu1 = torch.clamp(mu1, -20, 20)\n",
    "        mu2 = torch.clamp(mu2, -20, 20)\n",
    "        var1 = torch.clamp(var1, min=1e-5, max=20)\n",
    "        var2 = torch.clamp(var2, min=1e-5, max=20)\n",
    "\n",
    "        # Mixture of expected values under log-normal\n",
    "        exp1 = torch.exp(mu1 + 0.5 * var1)\n",
    "        exp2 = torch.exp(mu2 + 0.5 * var2)\n",
    "        final_pred = probs[:, 0] * exp1 + probs[:, 1] * exp2  # [B]\n",
    "\n",
    "        var1_lognormal = torch.exp(2*mu1 + var1) * (torch.exp(var1) - 1)\n",
    "        var2_lognormal = torch.exp(2*mu2 + var2) * (torch.exp(var2) - 1)\n",
    "        aleatoric_var = probs[:, 0] * var1_lognormal + probs[:, 1] * var2_lognormal  # [B]\n",
    "        epistemic_var = probs[:, 0] * torch.square(exp1) + probs[:, 1] * torch.square(exp2)  # [B]\n",
    "\n",
    "        if return_all:\n",
    "            return final_pred, aleatoric_var, epistemic_var, mu1, var1, mu2, var2, probs\n",
    "        return final_pred\n",
    "\n",
    "\n",
    "def tme_loss(y, mu1, var1, mu2, var2, probs, model, l2_lambda=0.1):\n",
    "    \"\"\"\n",
    "    Implements:\n",
    "        -ln ∑_s [ lognormal(y_t | μ_s, σ_s^2) * P(z_t = s | x) ] + λ * ||θ||^2\n",
    "    \"\"\"\n",
    "\n",
    "    eps = 1e-8  # for numerical stability\n",
    "    log_y = torch.log(y)# + eps)\n",
    "\n",
    "    # Log-normal density terms (not in log-space)\n",
    "    def lognormal_pdf(y, log_y, mu, var):\n",
    "        coef = 1.0 / (y * torch.sqrt(2 * torch.pi * var))# + eps))\n",
    "        exponent = torch.exp(- (log_y - mu) ** 2 / (2 * var))# + eps))\n",
    "        return coef * exponent\n",
    "\n",
    "    p1 = lognormal_pdf(y, log_y, mu1, var1)\n",
    "    p2 = lognormal_pdf(y, log_y, mu2, var2)\n",
    "\n",
    "    # Combine with selector probabilities\n",
    "    # print(probs[:,1])\n",
    "    weighted_sum = probs[:,0] * p1 + probs[:,1] * p2\n",
    "\n",
    "    # Negative log-likelihood (mean over batch)\n",
    "    nll = -torch.log(weighted_sum + eps).mean() #maybe mean or sum\n",
    "\n",
    "    # L2 Regularization (Gaussian prior on θ)\n",
    "    l2_penalty = sum((p**2).sum() for p in model.parameters())\n",
    "    reg = l2_lambda * l2_penalty\n",
    "\n",
    "    return nll + reg\n",
    "\n",
    "\n",
    "def train_tme_model(model, train_loader, val_loader, lr=5e-4, weight_decay=0.1, l2_lambda=0.1,\n",
    "                    max_epochs=100, patience=10, device='cpu', adam=False, direct_target=False, stop_criteria='RMSE', output_everything=False):\n",
    "    model.to(device)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=lr)#, weight_decay=0.1)#, momentum=0.9)\n",
    "    if adam:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_rmse = float('inf')\n",
    "    best_state_dict = None\n",
    "    best_epoch = -1\n",
    "    patience_counter = 0\n",
    "\n",
    "    all_state_dicts = []  # Store model states\n",
    "    all_train_losses = []  # Per-epoch train loss\n",
    "    all_val_losses = []  # Per-epoch val loss\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for x1, x2, y, w in train_loader:\n",
    "            x1, x2, y, w = x1.to(device), x2.to(device), y.to(device), w.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            final_pred, aleatoric_var, epistemic_var, mu1, var1, mu2, var2, probs = model(x1, x2, return_all=True)\n",
    "\n",
    "            loss = tme_loss(y, mu1, var1, mu2, var2, probs, model, l2_lambda=l2_lambda)\n",
    "            loss.backward()\n",
    "\n",
    "            # total_norm = 0\n",
    "            # for p in model.parameters():\n",
    "            #     if p.grad is not None:\n",
    "            #         param_norm = p.grad.data.norm(2)\n",
    "            #         total_norm += param_norm.item() ** 2\n",
    "            # total_norm = total_norm ** 0.5\n",
    "            # print(f\"Gradient norm: {total_norm:.4f}\")\n",
    "\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        y_preds_val = []\n",
    "        y_true_val = []\n",
    "        w_val = []\n",
    "\n",
    "        train_losses = []\n",
    "        y_preds_train = []\n",
    "        y_true_train = []\n",
    "        w_train = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x1, x2, y, w in val_loader:\n",
    "                x1, x2, y, w = x1.to(device), x2.to(device), y.to(device), w.to(device)\n",
    "\n",
    "                final_pred, aleatoric_var, epistemic_var, mu1, var1, mu2, var2, probs = model(x1, x2, return_all=True)\n",
    "\n",
    "                val_loss = tme_loss(y, mu1, var1, mu2, var2, probs, model, l2_lambda=l2_lambda)\n",
    "                val_losses.append(val_loss.item())\n",
    "                y_preds_val.append(final_pred.detach().cpu())\n",
    "                y_true_val.append(y.detach().cpu())\n",
    "                w_val.append(w.detach().cpu())\n",
    "\n",
    "            for x1, x2, y, w in train_loader:\n",
    "                x1, x2, y, w = x1.to(device), x2.to(device), y.to(device), w.to(device)\n",
    "\n",
    "                final_pred, aleatoric_var, epistemic_var, mu1, var1, mu2, var2, probs = model(x1, x2, return_all=True)\n",
    "\n",
    "                train_loss = tme_loss(y, mu1, var1, mu2, var2, probs, model, l2_lambda=l2_lambda)\n",
    "                train_losses.append(train_loss.item())\n",
    "                y_preds_train.append(final_pred.detach().cpu())\n",
    "                y_true_train.append(y.detach().cpu())\n",
    "                w_train.append(w.detach().cpu())\n",
    "            \n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        y_preds_val = torch.cat(y_preds_val).numpy()\n",
    "        y_true_val = torch.cat(y_true_val).numpy()\n",
    "        w_val = torch.cat(w_val).numpy()\n",
    "        if direct_target:\n",
    "            rmse_val = np.sqrt(mean_squared_error(y_true_val, y_preds_val))\n",
    "        else:\n",
    "            rmse_val = np.sqrt(mean_squared_error(y_true_val*w_val, y_preds_val*w_val))\n",
    "\n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "        y_preds_train = torch.cat(y_preds_train).numpy()\n",
    "        y_true_train = torch.cat(y_true_train).numpy()\n",
    "        w_train = torch.cat(w_train).numpy()\n",
    "        if direct_target:\n",
    "            rmse_train = np.sqrt(mean_squared_error(y_true_train, y_preds_train))\n",
    "        else:    \n",
    "            rmse_train = np.sqrt(mean_squared_error(y_true_train*w_train, y_preds_train*w_train))\n",
    "\n",
    "\n",
    "        all_train_losses.append(avg_train_loss)\n",
    "        all_val_losses.append(avg_val_loss)\n",
    "\n",
    "        if output_everything:\n",
    "            all_state_dicts.append(model.state_dict())  # Save a copy of model state at each epoch\n",
    "\n",
    "        if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}.      Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}.      Train RMSE: {rmse_train:.4f}, Val RMSE: {rmse_val:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if (stop_criteria=='RMSE'):\n",
    "            criteria = rmse_val < best_val_rmse - 1e-2\n",
    "        else:\n",
    "            criteria = avg_val_loss < best_val_loss - 1e-2\n",
    "        \n",
    "        if criteria:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_val_rmse = rmse_val\n",
    "            best_state_dict = model.state_dict()\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Restore best model\n",
    "    model.load_state_dict(best_state_dict)\n",
    "\n",
    "    if output_everything:\n",
    "        # Save models from [best_epoch-3, best_epoch] onward\n",
    "        start_idx = max(0, best_epoch - 3)\n",
    "        selected_models = all_state_dicts[start_idx:]\n",
    "        return model, best_val_loss, {\n",
    "            'all_train_losses': all_train_losses,\n",
    "            'all_val_losses': all_val_losses,\n",
    "            'selected_model_states': selected_models,\n",
    "            'best_epoch': best_epoch\n",
    "        }\n",
    "    else:\n",
    "        return model, best_val_loss, {}\n",
    "\n",
    "\n",
    "def train_tme_ensemble(train_loader, val_loader, d1, d2, h, num_models=20, device='cpu', adam=False, direct_target=False, output_everything=False, **train_kwargs):\n",
    "    ensemble = []\n",
    "    val_losses = []\n",
    "    all_train_losses = {}\n",
    "    all_val_losses = {}\n",
    "\n",
    "    # Generate 5 large random seeds (e.g., 32-bit)\n",
    "    seeds = [random.randint(1, 2**31 - 1) for _ in range(num_models)]\n",
    "\n",
    "    for i, seed in enumerate(seeds):\n",
    "        print(f\"\\n🌱 Training ensemble model {i + 1}/{num_models}\")\n",
    "        print(f\"Model {i+1} initialized with seed: {seed}\")\n",
    "\n",
    "        # Set seed for reproducibility\n",
    "        torch.manual_seed(seed)\n",
    "        model = TME(d1, d2, h)  # Initialize new model\n",
    "\n",
    "        # Train the model using your function\n",
    "        trained_model, best_val_loss, model_results = train_tme_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            device=device,\n",
    "            adam=adam,\n",
    "            direct_target=direct_target,\n",
    "            output_everything=output_everything,\n",
    "            **train_kwargs\n",
    "        )\n",
    "\n",
    "        if output_everything:\n",
    "            for selected_model in model_results['selected_model_states']:\n",
    "                trained_model.load_state_dict(selected_model)\n",
    "                ensemble.append(trained_model)\n",
    "            # ensemble = ensemble + [trained_model.load_state_dict(selected_model) for selected_model in model_results['selected_model_states']]\n",
    "            all_train_losses[i] = model_results['all_train_losses']\n",
    "            all_val_losses[i] = model_results['all_val_losses']\n",
    "            val_losses.append(best_val_loss)\n",
    "        else:\n",
    "            # Save the model and its validation loss\n",
    "            ensemble.append(trained_model)\n",
    "            val_losses.append(best_val_loss)\n",
    "\n",
    "    return ensemble, val_losses, all_train_losses, all_val_losses\n",
    "\n",
    "\n",
    "# def evaluate_tme_ensemble(ensemble, test_loader, all_preds=False, device='cpu', direct_target=False, output_everything=False):\n",
    "#     all_preds = []\n",
    "#     all_preds_median = []\n",
    "#     y_trues = []\n",
    "#     w_trues = []\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         for x1, x2, y, w in test_loader:\n",
    "#             x1, x2, y, w = x1.to(device).to(torch.float64), x2.to(device).to(torch.float64), y.to(device).to(torch.float64), w.to(device).to(torch.float64)\n",
    "#             batch_preds = []\n",
    "#             batch_aleatoric = []\n",
    "#             batch_epistemic = []\n",
    "\n",
    "#             for model in ensemble:\n",
    "#                 model.eval()\n",
    "#                 model.to(device).to(torch.float64)\n",
    "#                 if output_everything:\n",
    "#                     final_pred, aleatoric_var, epistemic_var, mu1, var1, mu2, var2, probs = model(x1, x2, return_all=True)\n",
    "#                     batch_aleatoric.append(aleatoric_var.cpu())\n",
    "#                     batch_epistemic.append(epistemic_var.cpu())\n",
    "#                 else:\n",
    "#                     final_pred = model(x1, x2)\n",
    "\n",
    "#                 batch_preds.append(final_pred.cpu())\n",
    "                \n",
    "\n",
    "#             # Average predictions from all models\n",
    "#             avg_pred = torch.stack(batch_preds).mean(dim=0)\n",
    "#             median_pred = torch.stack(batch_preds).median(dim=0).values\n",
    "#             all_preds.append(avg_pred)\n",
    "#             all_preds_median.append(median_pred)\n",
    "#             y_trues.append(y.cpu())\n",
    "#             w_trues.append(w.cpu())\n",
    "\n",
    "#     y_preds = torch.cat(all_preds).numpy()\n",
    "#     y_preds_median = torch.cat(all_preds_median).numpy()\n",
    "#     y_trues = torch.cat(y_trues).numpy()\n",
    "#     w_trues = torch.cat(w_trues).numpy()\n",
    "\n",
    "#     if direct_target:\n",
    "#         rmse = np.sqrt(mean_squared_error(y_trues, y_preds))\n",
    "#         mae = mean_absolute_error(y_trues, y_preds)\n",
    "#     else:\n",
    "#         rmse = np.sqrt(mean_squared_error(y_trues*w_trues, y_preds*w_trues))\n",
    "#         mae = mean_absolute_error(y_trues*w_trues, y_preds*w_trues)\n",
    "\n",
    "#     # print(f\"📊 Ensemble Test RMSE: {rmse:.4f}\")\n",
    "#     # print(f\"📊 Ensemble Test MAE: {mae:.4f}\")\n",
    "#     return y_preds, y_preds_median, rmse, mae\n",
    "\n",
    "\n",
    "def evaluate_tme_ensemble(ensemble, test_loader, all_preds=False, device='cpu', direct_target=False, output_everything=False):\n",
    "    all_preds_mean = []\n",
    "    all_preds_median = []\n",
    "    y_trues = []\n",
    "    w_trues = []\n",
    "\n",
    "    if output_everything:\n",
    "        all_model_preds = []       # Shape: (num_models, N)\n",
    "        all_epistemic_vars = []    # Shape: (num_models, N)\n",
    "        all_aleatoric_vars = []    # Shape: (num_models, N)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, y, w in test_loader:\n",
    "            x1, x2 = x1.to(device).to(torch.float64), x2.to(device).to(torch.float64)\n",
    "            y, w = y.to(device).to(torch.float64), w.to(device).to(torch.float64)\n",
    "\n",
    "            batch_preds = []\n",
    "            batch_aleatoric = []\n",
    "            batch_epistemic = []\n",
    "\n",
    "            for model in ensemble:\n",
    "                model.eval()\n",
    "                model.to(device).to(torch.float64)\n",
    "\n",
    "                if output_everything:\n",
    "                    final_pred, aleatoric_var, epistemic_var, *_ = model(x1, x2, return_all=True)\n",
    "                    batch_preds.append(final_pred.cpu())\n",
    "                    batch_aleatoric.append(aleatoric_var.cpu())\n",
    "                    batch_epistemic.append(epistemic_var.cpu())\n",
    "                else:\n",
    "                    final_pred = model(x1, x2)\n",
    "                    batch_preds.append(final_pred.cpu())\n",
    "\n",
    "            preds_stack = torch.stack(batch_preds)  # (num_models, batch_size)\n",
    "            avg_pred = preds_stack.mean(dim=0)       # (batch_size,)\n",
    "            median_pred = preds_stack.median(dim=0).values  # (batch_size,)\n",
    "\n",
    "            all_preds_mean.append(avg_pred)\n",
    "            all_preds_median.append(median_pred)\n",
    "            y_trues.append(y.cpu())\n",
    "            w_trues.append(w.cpu())\n",
    "\n",
    "            if output_everything:\n",
    "                all_model_preds.append(preds_stack.T)                  # (batch_size, num_models)\n",
    "                all_epistemic_vars.append(torch.stack(batch_epistemic).T)\n",
    "                all_aleatoric_vars.append(torch.stack(batch_aleatoric).T)\n",
    "\n",
    "    # Concatenate across batches\n",
    "    y_preds_mean = torch.cat(all_preds_mean).numpy()\n",
    "    y_preds_median = torch.cat(all_preds_median).numpy()\n",
    "    y_trues = torch.cat(y_trues).numpy()\n",
    "    w_trues = torch.cat(w_trues).numpy()\n",
    "\n",
    "    if direct_target:\n",
    "        rmse = np.sqrt(mean_squared_error(y_trues, y_preds_mean))\n",
    "        mae = mean_absolute_error(y_trues, y_preds_mean)\n",
    "    else:\n",
    "        rmse = np.sqrt(mean_squared_error(y_trues * w_trues, y_preds_mean * w_trues))\n",
    "        mae = mean_absolute_error(y_trues * w_trues, y_preds_mean * w_trues)\n",
    "\n",
    "    if output_everything:\n",
    "        y_preds = torch.cat(all_model_preds, dim=0)           # (N, num_models)\n",
    "        epistemic_var = torch.cat(all_epistemic_vars, dim=0)  # (N, num_models)\n",
    "        aleatoric_var = torch.cat(all_aleatoric_vars, dim=0)  # (N, num_models)\n",
    "\n",
    "        return {\n",
    "            'y_trues': y_trues,\n",
    "            'y_preds': y_preds,\n",
    "            'epistemic_var': epistemic_var,\n",
    "            'aleatoric_var': aleatoric_var,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae\n",
    "        }\n",
    "\n",
    "    return {\"y_preds_mean\":y_preds_mean, \"y_preds_median\":y_preds_median, \"rmse\":rmse, \"mae\":mae}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5ccd7cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 29.4278, Val Loss: 30.4283.      Train RMSE: 11562.0375, Val RMSE: 32.7399\n",
      "Epoch 5.      Train Loss: 3.4841, Val Loss: 4.5213.      Train RMSE: 16.9248, Val RMSE: 14.5841\n",
      "Epoch 10.      Train Loss: 3.4584, Val Loss: 4.4958.      Train RMSE: 16.9252, Val RMSE: 14.5844\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "h, batch_size, lr, l2_lambda = 10, 256, 1e-3, 5\n",
    "\n",
    "d1=source1_tensor.shape[1]  # number of features of the 1st source\n",
    "d2=source2_tensor.shape[1]  # number of features of the 2nd source\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = create_datasets(source1_array, source2_array, target_direct_array, weight_array, batch_size, h)\n",
    "\n",
    "print(f\"\\n📊 h: {h}, batch_size:{batch_size}, lr: {lr}, l2_lambda:{l2_lambda}\")\n",
    "\n",
    "ensemble, losses = train_tme_ensemble(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    d1=d1,  # number of features of the 1st source\n",
    "    d2=d2,  # number of features of the 2nd source\n",
    "    h=h,  # lag length\n",
    "    num_models=1,#20,\n",
    "    device=device,\n",
    "    lr=lr,\n",
    "    weight_decay=0.1,\n",
    "    l2_lambda=l2_lambda,\n",
    "    max_epochs=100,\n",
    "    patience=7,\n",
    "    adam=True,\n",
    "    direct_target=True,\n",
    "    stop_criteria='RMSE'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2709075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tme_ensemble(ensemble, test_loader, all_preds=False, device='cpu', direct_target=False):\n",
    "    all_preds = []\n",
    "    all_preds_median = []\n",
    "    y_trues = []\n",
    "    w_trues = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, y, w in test_loader:\n",
    "            x1, x2, y, w = x1.to(device).to(torch.float64), x2.to(device).to(torch.float64), y.to(device).to(torch.float64), w.to(device).to(torch.float64)\n",
    "            batch_preds = []\n",
    "\n",
    "            for model in ensemble:\n",
    "                model.eval()\n",
    "                model.to(device).to(torch.float64)\n",
    "                pred = model(x1, x2)\n",
    "                batch_preds.append(pred.cpu())\n",
    "\n",
    "            # Average predictions from all models\n",
    "            avg_pred = torch.stack(batch_preds).mean(dim=0)\n",
    "            median_pred = torch.stack(batch_preds).median(dim=0).values\n",
    "            all_preds.append(avg_pred)\n",
    "            all_preds_median.append(median_pred)\n",
    "            y_trues.append(y.cpu())\n",
    "            w_trues.append(w.cpu())\n",
    "\n",
    "    y_preds = torch.cat(all_preds).numpy()\n",
    "    y_preds_median = torch.cat(all_preds_median).numpy()\n",
    "    y_trues = torch.cat(y_trues).numpy()\n",
    "    w_trues = torch.cat(w_trues).numpy()\n",
    "\n",
    "    if direct_target:\n",
    "        rmse = np.sqrt(mean_squared_error(y_trues, y_preds))\n",
    "        mae = mean_absolute_error(y_trues, y_preds)\n",
    "    else:\n",
    "        rmse = np.sqrt(mean_squared_error(y_trues*w_trues, y_preds*w_trues))\n",
    "        mae = mean_absolute_error(y_trues*w_trues, y_preds*w_trues)\n",
    "\n",
    "    # print(f\"📊 Ensemble Test RMSE: {rmse:.4f}\")\n",
    "    # print(f\"📊 Ensemble Test MAE: {mae:.4f}\")\n",
    "    return y_preds, y_preds_median, rmse, mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "77383a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, test_loader, device='cpu', direct_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2365b0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.205035357978883 3.569523536259337\n"
     ]
    }
   ],
   "source": [
    "print(rmse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885c4f51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3e0413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99f4fd98",
   "metadata": {},
   "source": [
    "## Hyperparams search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "10732928",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:128, lr: 0.0001, l2_lambda:0.1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 5.1618, Val Loss: 6.0561.      Train RMSE: 58873010397.7560, Val RMSE: 536558.4730\n",
      "Epoch 5.      Train Loss: 3.7214, Val Loss: 4.6016.      Train RMSE: 60456939375.7285, Val RMSE: 181981.4426\n",
      "Epoch 10.      Train Loss: 2.9128, Val Loss: 3.2115.      Train RMSE: 2479866832.7572, Val RMSE: 2452.9113\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [03:20, 200.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:128, lr: 0.0001, l2_lambda:0.1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 5.1618, Val Loss: 6.0561.      Train RMSE: 58873010397.7560, Val RMSE: 536558.4730\n",
      "Epoch 5.      Train Loss: 3.7214, Val Loss: 4.6016.      Train RMSE: 60456939375.7285, Val RMSE: 181981.4426\n",
      "Epoch 10.      Train Loss: 2.9128, Val Loss: 3.2115.      Train RMSE: 2479866832.7572, Val RMSE: 2452.9113\n",
      "Epoch 15.      Train Loss: 2.7260, Val Loss: 2.8465.      Train RMSE: 7228064.8664, Val RMSE: 3788.5306\n",
      "Epoch 20.      Train Loss: 2.6810, Val Loss: 2.7901.      Train RMSE: 6771733.9107, Val RMSE: 3898.8415\n",
      "Epoch 25.      Train Loss: 2.6724, Val Loss: 2.7840.      Train RMSE: 5930952.3091, Val RMSE: 4183.3360\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [10:19, 329.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:128, lr: 0.0001, l2_lambda:1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 21.0510, Val Loss: 21.9561.      Train RMSE: 34036611306.5554, Val RMSE: 66218.5743\n",
      "Epoch 5.      Train Loss: 9.8338, Val Loss: 10.8203.      Train RMSE: 43450910.1143, Val RMSE: 273.3316\n",
      "Epoch 10.      Train Loss: 5.0610, Val Loss: 6.0227.      Train RMSE: 16.7810, Val RMSE: 14.4831\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [13:31, 266.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:128, lr: 0.0001, l2_lambda:1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 21.0510, Val Loss: 21.9561.      Train RMSE: 34036611306.5554, Val RMSE: 66218.5743\n",
      "Epoch 5.      Train Loss: 9.8338, Val Loss: 10.8203.      Train RMSE: 43450910.1143, Val RMSE: 273.3316\n",
      "Epoch 10.      Train Loss: 5.0610, Val Loss: 6.0227.      Train RMSE: 16.7810, Val RMSE: 14.4831\n",
      "Epoch 15.      Train Loss: 3.7281, Val Loss: 4.6684.      Train RMSE: 16.8565, Val RMSE: 14.5225\n",
      "Epoch 20.      Train Loss: 3.3359, Val Loss: 4.2764.      Train RMSE: 16.8615, Val RMSE: 14.5257\n",
      "Epoch 25.      Train Loss: 3.2973, Val Loss: 4.2370.      Train RMSE: 16.8615, Val RMSE: 14.5256\n",
      "Epoch 30.      Train Loss: 3.2973, Val Loss: 4.2380.      Train RMSE: 16.8617, Val RMSE: 14.5258\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [34:39, 661.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:128, lr: 0.0001, l2_lambda:3, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 56.2748, Val Loss: 57.1796.      Train RMSE: 32347243642.5246, Val RMSE: 39701.8972\n",
      "Epoch 5.      Train Loss: 22.9533, Val Loss: 23.9884.      Train RMSE: 673984.3130, Val RMSE: 243.7586\n",
      "Epoch 10.      Train Loss: 8.7143, Val Loss: 9.7457.      Train RMSE: 16.9055, Val RMSE: 14.5710\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [42:59, 603.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:128, lr: 0.0001, l2_lambda:3, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 56.2748, Val Loss: 57.1796.      Train RMSE: 32347243642.5246, Val RMSE: 39701.8972\n",
      "Epoch 5.      Train Loss: 22.9533, Val Loss: 23.9884.      Train RMSE: 673984.3130, Val RMSE: 243.7586\n",
      "Epoch 10.      Train Loss: 8.7143, Val Loss: 9.7457.      Train RMSE: 16.9055, Val RMSE: 14.5710\n",
      "Epoch 15.      Train Loss: 4.7135, Val Loss: 5.7306.      Train RMSE: 16.9138, Val RMSE: 14.5733\n",
      "Epoch 20.      Train Loss: 3.5409, Val Loss: 4.5618.      Train RMSE: 16.9137, Val RMSE: 14.5735\n",
      "Epoch 25.      Train Loss: 3.4256, Val Loss: 4.4457.      Train RMSE: 16.9138, Val RMSE: 14.5737\n",
      "Epoch 30.      Train Loss: 3.4257, Val Loss: 4.4472.      Train RMSE: 16.9139, Val RMSE: 14.5738\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [1:05:07, 849.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:128, lr: 0.0001, l2_lambda:5, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 91.4425, Val Loss: 92.3498.      Train RMSE: 32344178334.7135, Val RMSE: 32649.8172\n",
      "Epoch 5.      Train Loss: 35.9989, Val Loss: 37.0473.      Train RMSE: 673006.0096, Val RMSE: 239.9122\n",
      "Epoch 10.      Train Loss: 12.2696, Val Loss: 13.3206.      Train RMSE: 16.9213, Val RMSE: 14.5840\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [1:15:04, 767.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:128, lr: 0.0001, l2_lambda:5, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 91.4425, Val Loss: 92.3498.      Train RMSE: 32344178334.7135, Val RMSE: 32649.8172\n",
      "Epoch 5.      Train Loss: 35.9989, Val Loss: 37.0473.      Train RMSE: 673006.0096, Val RMSE: 239.9122\n",
      "Epoch 10.      Train Loss: 12.2696, Val Loss: 13.3206.      Train RMSE: 16.9213, Val RMSE: 14.5840\n",
      "Epoch 15.      Train Loss: 5.6041, Val Loss: 6.6404.      Train RMSE: 16.9256, Val RMSE: 14.5844\n",
      "Epoch 20.      Train Loss: 3.6502, Val Loss: 4.6889.      Train RMSE: 16.9256, Val RMSE: 14.5846\n",
      "Epoch 25.      Train Loss: 3.4583, Val Loss: 4.4962.      Train RMSE: 16.9257, Val RMSE: 14.5847\n",
      "Epoch 30.      Train Loss: 3.4584, Val Loss: 4.4975.      Train RMSE: 16.9258, Val RMSE: 14.5848\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [1:28:39, 782.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:128, lr: 0.001, l2_lambda:0.1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 3.0474, Val Loss: 3.5786.      Train RMSE: 45741527974.4060, Val RMSE: 6532.2404\n",
      "Epoch 5.      Train Loss: 2.6697, Val Loss: 2.7976.      Train RMSE: 6013823.9333, Val RMSE: 5468.8891\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [1:30:17, 568.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:128, lr: 0.001, l2_lambda:0.1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 3.0474, Val Loss: 3.5786.      Train RMSE: 45741527974.4060, Val RMSE: 6532.2404\n",
      "Epoch 5.      Train Loss: 2.6697, Val Loss: 2.7976.      Train RMSE: 6013823.9333, Val RMSE: 5468.8891\n",
      "Epoch 10.      Train Loss: 2.6697, Val Loss: 2.7974.      Train RMSE: 6094384.8426, Val RMSE: 5396.4600\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [1:33:00, 443.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:128, lr: 0.001, l2_lambda:1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 5.6689, Val Loss: 6.6441.      Train RMSE: 47.2214, Val RMSE: 14.4360\n",
      "Epoch 5.      Train Loss: 3.2973, Val Loss: 4.2372.      Train RMSE: 16.8622, Val RMSE: 14.5263\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [1:34:07, 328.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:128, lr: 0.001, l2_lambda:1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 5.6689, Val Loss: 6.6441.      Train RMSE: 47.2214, Val RMSE: 14.4360\n",
      "Epoch 5.      Train Loss: 3.2973, Val Loss: 4.2372.      Train RMSE: 16.8622, Val RMSE: 14.5263\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [1:36:01, 263.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:128, lr: 0.001, l2_lambda:3, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 10.5141, Val Loss: 11.5576.      Train RMSE: 16.9020, Val RMSE: 14.5582\n",
      "Epoch 5.      Train Loss: 3.4256, Val Loss: 4.4471.      Train RMSE: 16.9141, Val RMSE: 14.5740\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [1:37:12, 204.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:128, lr: 0.001, l2_lambda:3, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 10.5141, Val Loss: 11.5576.      Train RMSE: 16.9020, Val RMSE: 14.5582\n",
      "Epoch 5.      Train Loss: 3.4256, Val Loss: 4.4471.      Train RMSE: 16.9141, Val RMSE: 14.5740\n",
      "Epoch 10.      Train Loss: 3.4257, Val Loss: 4.4479.      Train RMSE: 16.9142, Val RMSE: 14.5741\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [1:39:56, 192.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:128, lr: 0.001, l2_lambda:5, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 15.2600, Val Loss: 16.3218.      Train RMSE: 16.9133, Val RMSE: 14.5772\n",
      "Epoch 5.      Train Loss: 3.4582, Val Loss: 4.4970.      Train RMSE: 16.9256, Val RMSE: 14.5846\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [1:41:09, 156.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:128, lr: 0.001, l2_lambda:5, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 15.2600, Val Loss: 16.3218.      Train RMSE: 16.9133, Val RMSE: 14.5772\n",
      "Epoch 5.      Train Loss: 3.4582, Val Loss: 4.4970.      Train RMSE: 16.9256, Val RMSE: 14.5846\n",
      "Epoch 10.      Train Loss: 3.4584, Val Loss: 4.4983.      Train RMSE: 16.9259, Val RMSE: 14.5849\n",
      "Epoch 15.      Train Loss: 3.4583, Val Loss: 4.4970.      Train RMSE: 16.9242, Val RMSE: 14.5833\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [1:44:26, 168.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:256, lr: 0.0001, l2_lambda:0.1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 5.4196, Val Loss: 6.3001.      Train RMSE: 58976753898.3173, Val RMSE: 528369.6742\n",
      "Epoch 5.      Train Loss: 4.4249, Val Loss: 5.3398.      Train RMSE: 59610829612.8828, Val RMSE: 645590.5332\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [1:45:12, 131.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:256, lr: 0.0001, l2_lambda:0.1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 5.4196, Val Loss: 6.3001.      Train RMSE: 58976753898.3173, Val RMSE: 528369.6742\n",
      "Epoch 5.      Train Loss: 4.4249, Val Loss: 5.3398.      Train RMSE: 59610829612.8828, Val RMSE: 645590.5332\n",
      "Epoch 10.      Train Loss: 3.6609, Val Loss: 4.5176.      Train RMSE: 58086322336.5214, Val RMSE: 138847.7456\n",
      "Epoch 15.      Train Loss: 3.1168, Val Loss: 3.5191.      Train RMSE: 32438804621.8844, Val RMSE: 2450.8060\n",
      "Epoch 20.      Train Loss: 2.8700, Val Loss: 3.0370.      Train RMSE: 8117306.0656, Val RMSE: 3553.1914\n",
      "Epoch 25.      Train Loss: 2.7622, Val Loss: 2.8785.      Train RMSE: 7224586.4093, Val RMSE: 3925.8454\n",
      "Epoch 30.      Train Loss: 2.7133, Val Loss: 2.8205.      Train RMSE: 7198114.3142, Val RMSE: 3944.2696\n",
      "Epoch 35.      Train Loss: 2.6880, Val Loss: 2.7930.      Train RMSE: 7480333.3870, Val RMSE: 4051.8052\n",
      "Epoch 40.      Train Loss: 2.6760, Val Loss: 2.7830.      Train RMSE: 6808477.3571, Val RMSE: 4273.0750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [1:50:08, 181.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:256, lr: 0.0001, l2_lambda:1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 23.3832, Val Loss: 24.2626.      Train RMSE: 45788382087.3650, Val RMSE: 143024.2497\n",
      "Epoch 5.      Train Loss: 15.4835, Val Loss: 16.4325.      Train RMSE: 22363998249.4613, Val RMSE: 7735.8857\n",
      "Epoch 10.      Train Loss: 9.7844, Val Loss: 10.7664.      Train RMSE: 28311990.5151, Val RMSE: 271.2708\n",
      "Epoch 15.      Train Loss: 6.6937, Val Loss: 7.6733.      Train RMSE: 60.4163, Val RMSE: 30.8675\n",
      "Epoch 20.      Train Loss: 5.0517, Val Loss: 6.0106.      Train RMSE: 16.7747, Val RMSE: 14.4884\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [1:52:47, 174.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:256, lr: 0.0001, l2_lambda:1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 23.3832, Val Loss: 24.2626.      Train RMSE: 45788382087.3650, Val RMSE: 143024.2497\n",
      "Epoch 5.      Train Loss: 15.4835, Val Loss: 16.4325.      Train RMSE: 22363998249.4613, Val RMSE: 7735.8857\n",
      "Epoch 10.      Train Loss: 9.7844, Val Loss: 10.7664.      Train RMSE: 28311990.5151, Val RMSE: 271.2708\n",
      "Epoch 15.      Train Loss: 6.6937, Val Loss: 7.6733.      Train RMSE: 60.4163, Val RMSE: 30.8675\n",
      "Epoch 20.      Train Loss: 5.0517, Val Loss: 6.0106.      Train RMSE: 16.7747, Val RMSE: 14.4884\n",
      "Epoch 25.      Train Loss: 4.1902, Val Loss: 5.1316.      Train RMSE: 16.8357, Val RMSE: 14.5105\n",
      "Epoch 30.      Train Loss: 3.7258, Val Loss: 4.6643.      Train RMSE: 16.8567, Val RMSE: 14.5226\n",
      "Epoch 35.      Train Loss: 3.4615, Val Loss: 4.4002.      Train RMSE: 16.8613, Val RMSE: 14.5255\n",
      "Epoch 40.      Train Loss: 3.3348, Val Loss: 4.2734.      Train RMSE: 16.8613, Val RMSE: 14.5255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [1:57:43, 211.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:256, lr: 0.0001, l2_lambda:3, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 63.2433, Val Loss: 64.1209.      Train RMSE: 33353708808.4493, Val RMSE: 108234.9326\n",
      "Epoch 5.      Train Loss: 39.8615, Val Loss: 40.8348.      Train RMSE: 16453728558.2109, Val RMSE: 3521.3012\n",
      "Epoch 10.      Train Loss: 22.9167, Val Loss: 23.9500.      Train RMSE: 670292.5539, Val RMSE: 242.7901\n",
      "Epoch 15.      Train Loss: 13.6508, Val Loss: 14.6964.      Train RMSE: 31.0500, Val RMSE: 15.6444\n",
      "Epoch 20.      Train Loss: 8.6969, Val Loss: 9.7269.      Train RMSE: 16.9054, Val RMSE: 14.5707\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [2:00:19, 194.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:256, lr: 0.0001, l2_lambda:3, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 63.2433, Val Loss: 64.1209.      Train RMSE: 33353708808.4493, Val RMSE: 108234.9326\n",
      "Epoch 5.      Train Loss: 39.8615, Val Loss: 40.8348.      Train RMSE: 16453728558.2109, Val RMSE: 3521.3012\n",
      "Epoch 10.      Train Loss: 22.9167, Val Loss: 23.9500.      Train RMSE: 670292.5539, Val RMSE: 242.7901\n",
      "Epoch 15.      Train Loss: 13.6508, Val Loss: 14.6964.      Train RMSE: 31.0500, Val RMSE: 15.6444\n",
      "Epoch 20.      Train Loss: 8.6969, Val Loss: 9.7269.      Train RMSE: 16.9054, Val RMSE: 14.5707\n",
      "Epoch 25.      Train Loss: 6.1057, Val Loss: 7.1205.      Train RMSE: 16.9122, Val RMSE: 14.5723\n",
      "Epoch 30.      Train Loss: 4.7066, Val Loss: 5.7231.      Train RMSE: 16.9136, Val RMSE: 14.5732\n",
      "Epoch 35.      Train Loss: 3.9177, Val Loss: 4.9359.      Train RMSE: 16.9137, Val RMSE: 14.5735\n",
      "Epoch 40.      Train Loss: 3.5385, Val Loss: 4.5574.      Train RMSE: 16.9136, Val RMSE: 14.5734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [2:05:34, 230.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:256, lr: 0.0001, l2_lambda:5, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 103.1042, Val Loss: 103.9816.      Train RMSE: 32828610291.1468, Val RMSE: 101601.8545\n",
      "Epoch 5.      Train Loss: 64.1412, Val Loss: 65.1241.      Train RMSE: 16624616371.5950, Val RMSE: 2623.0507\n",
      "Epoch 10.      Train Loss: 35.9465, Val Loss: 36.9926.      Train RMSE: 675865.1874, Val RMSE: 239.3401\n",
      "Epoch 15.      Train Loss: 20.5048, Val Loss: 21.5681.      Train RMSE: 31.1881, Val RMSE: 14.9575\n",
      "Epoch 20.      Train Loss: 12.2417, Val Loss: 13.2914.      Train RMSE: 16.9211, Val RMSE: 14.5838\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [2:08:24, 212.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:256, lr: 0.0001, l2_lambda:5, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 103.1042, Val Loss: 103.9816.      Train RMSE: 32828610291.1468, Val RMSE: 101601.8545\n",
      "Epoch 5.      Train Loss: 64.1412, Val Loss: 65.1241.      Train RMSE: 16624616371.5950, Val RMSE: 2623.0507\n",
      "Epoch 10.      Train Loss: 35.9465, Val Loss: 36.9926.      Train RMSE: 675865.1874, Val RMSE: 239.3401\n",
      "Epoch 15.      Train Loss: 20.5048, Val Loss: 21.5681.      Train RMSE: 31.1881, Val RMSE: 14.9575\n",
      "Epoch 20.      Train Loss: 12.2417, Val Loss: 13.2914.      Train RMSE: 16.9211, Val RMSE: 14.5838\n",
      "Epoch 25.      Train Loss: 7.9246, Val Loss: 8.9599.      Train RMSE: 16.9248, Val RMSE: 14.5841\n",
      "Epoch 30.      Train Loss: 5.5924, Val Loss: 6.6279.      Train RMSE: 16.9256, Val RMSE: 14.5845\n",
      "Epoch 35.      Train Loss: 4.2782, Val Loss: 5.3148.      Train RMSE: 16.9256, Val RMSE: 14.5845\n",
      "Epoch 40.      Train Loss: 3.6465, Val Loss: 4.6835.      Train RMSE: 16.9255, Val RMSE: 14.5845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [2:14:31, 258.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:256, lr: 0.001, l2_lambda:0.1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 3.7157, Val Loss: 4.5804.      Train RMSE: 65774347298.4966, Val RMSE: 222050.2666\n",
      "Epoch 5.      Train Loss: 2.6795, Val Loss: 2.7982.      Train RMSE: 7051434.7142, Val RMSE: 4851.7634\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [2:15:31, 199.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:256, lr: 0.001, l2_lambda:0.1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 3.7157, Val Loss: 4.5804.      Train RMSE: 65774347298.4966, Val RMSE: 222050.2666\n",
      "Epoch 5.      Train Loss: 2.6795, Val Loss: 2.7982.      Train RMSE: 7051434.7142, Val RMSE: 4851.7634\n",
      "Epoch 10.      Train Loss: 2.6694, Val Loss: 2.7919.      Train RMSE: 6096193.2279, Val RMSE: 5513.6855\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [2:17:19, 171.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:256, lr: 0.001, l2_lambda:1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 10.1413, Val Loss: 11.1216.      Train RMSE: 6448276917.7564, Val RMSE: 274.8540\n",
      "Epoch 5.      Train Loss: 3.3859, Val Loss: 4.3240.      Train RMSE: 16.8600, Val RMSE: 14.5248\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [2:18:22, 139.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:256, lr: 0.001, l2_lambda:1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 10.1413, Val Loss: 11.1216.      Train RMSE: 6448276917.7564, Val RMSE: 274.8540\n",
      "Epoch 5.      Train Loss: 3.3859, Val Loss: 4.3240.      Train RMSE: 16.8600, Val RMSE: 14.5248\n",
      "Epoch 10.      Train Loss: 3.2974, Val Loss: 4.2358.      Train RMSE: 16.8624, Val RMSE: 14.5265\n",
      "Epoch 15.      Train Loss: 3.2972, Val Loss: 4.2357.      Train RMSE: 16.8620, Val RMSE: 14.5261\n",
      "Epoch 20.      Train Loss: 3.2974, Val Loss: 4.2372.      Train RMSE: 16.8612, Val RMSE: 14.5254\n",
      "Epoch 25.      Train Loss: 3.2974, Val Loss: 4.2325.      Train RMSE: 16.8622, Val RMSE: 14.5262\n",
      "Epoch 30.      Train Loss: 3.2978, Val Loss: 4.2378.      Train RMSE: 16.8626, Val RMSE: 14.5266\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [2:23:45, 194.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:256, lr: 0.001, l2_lambda:3, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 24.0979, Val Loss: 25.1339.      Train RMSE: 50329317.2794, Val RMSE: 274.2827\n",
      "Epoch 5.      Train Loss: 3.6879, Val Loss: 4.7077.      Train RMSE: 16.9143, Val RMSE: 14.5741\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [2:24:54, 156.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:256, lr: 0.001, l2_lambda:3, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 24.0979, Val Loss: 25.1339.      Train RMSE: 50329317.2794, Val RMSE: 274.2827\n",
      "Epoch 5.      Train Loss: 3.6879, Val Loss: 4.7077.      Train RMSE: 16.9143, Val RMSE: 14.5741\n",
      "Epoch 10.      Train Loss: 3.4258, Val Loss: 4.4452.      Train RMSE: 16.9139, Val RMSE: 14.5738\n",
      "Epoch 15.      Train Loss: 3.4255, Val Loss: 4.4444.      Train RMSE: 16.9130, Val RMSE: 14.5729\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [2:27:37, 158.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:256, lr: 0.001, l2_lambda:5, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 37.7087, Val Loss: 38.7522.      Train RMSE: 9181508.5769, Val RMSE: 266.1151\n",
      "Epoch 5.      Train Loss: 3.8946, Val Loss: 4.9325.      Train RMSE: 16.9263, Val RMSE: 14.5852\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [2:28:42, 130.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 5, batch_size:256, lr: 0.001, l2_lambda:5, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 37.7087, Val Loss: 38.7522.      Train RMSE: 9181508.5769, Val RMSE: 266.1151\n",
      "Epoch 5.      Train Loss: 3.8946, Val Loss: 4.9325.      Train RMSE: 16.9263, Val RMSE: 14.5852\n",
      "Epoch 10.      Train Loss: 3.4585, Val Loss: 4.4961.      Train RMSE: 16.9259, Val RMSE: 14.5849\n",
      "Epoch 15.      Train Loss: 3.4582, Val Loss: 4.4951.      Train RMSE: 16.9248, Val RMSE: 14.5839\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [2:31:26, 140.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0001, l2_lambda:0.1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 5.1783, Val Loss: 5.6128.      Train RMSE: 16179447082.8365, Val RMSE: 11061276.6287\n",
      "Epoch 5.      Train Loss: 3.4947, Val Loss: 3.7138.      Train RMSE: 35183344029.3965, Val RMSE: 6991.3035\n",
      "Epoch 10.      Train Loss: 2.8012, Val Loss: 2.9494.      Train RMSE: 5674243.5515, Val RMSE: 5765.7822\n",
      "Epoch 15.      Train Loss: 2.6450, Val Loss: 2.7589.      Train RMSE: 5408342.6414, Val RMSE: 6166.6153\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [2:34:24, 151.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0001, l2_lambda:0.1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 5.1783, Val Loss: 5.6128.      Train RMSE: 16179447082.8365, Val RMSE: 11061276.6287\n",
      "Epoch 5.      Train Loss: 3.4947, Val Loss: 3.7138.      Train RMSE: 35183344029.3965, Val RMSE: 6991.3035\n",
      "Epoch 10.      Train Loss: 2.8012, Val Loss: 2.9494.      Train RMSE: 5674243.5515, Val RMSE: 5765.7822\n",
      "Epoch 15.      Train Loss: 2.6450, Val Loss: 2.7589.      Train RMSE: 5408342.6414, Val RMSE: 6166.6153\n",
      "Epoch 20.      Train Loss: 2.6241, Val Loss: 2.7308.      Train RMSE: 5504837.5207, Val RMSE: 6495.8904\n",
      "Epoch 25.      Train Loss: 2.6213, Val Loss: 2.7280.      Train RMSE: 5390198.6609, Val RMSE: 6497.0692\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [2:40:09, 209.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0001, l2_lambda:1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 20.0481, Val Loss: 20.6658.      Train RMSE: 4148391950.5897, Val RMSE: 441622.9281\n",
      "Epoch 5.      Train Loss: 8.1352, Val Loss: 9.0788.      Train RMSE: 2974892.1892, Val RMSE: 18.8451\n",
      "Epoch 10.      Train Loss: 4.0296, Val Loss: 4.9787.      Train RMSE: 17.1328, Val RMSE: 14.4345\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [2:42:23, 187.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0001, l2_lambda:1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 20.0481, Val Loss: 20.6658.      Train RMSE: 4148391950.5897, Val RMSE: 441622.9281\n",
      "Epoch 5.      Train Loss: 8.1352, Val Loss: 9.0788.      Train RMSE: 2974892.1892, Val RMSE: 18.8451\n",
      "Epoch 10.      Train Loss: 4.0296, Val Loss: 4.9787.      Train RMSE: 17.1328, Val RMSE: 14.4345\n",
      "Epoch 15.      Train Loss: 3.3294, Val Loss: 4.2714.      Train RMSE: 16.8386, Val RMSE: 14.5115\n",
      "Epoch 20.      Train Loss: 3.2973, Val Loss: 4.2375.      Train RMSE: 16.8621, Val RMSE: 14.5263\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [2:47:54, 230.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0001, l2_lambda:3, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 52.8472, Val Loss: 53.4502.      Train RMSE: 3630951008.1413, Val RMSE: 223394.4822\n",
      "Epoch 5.      Train Loss: 17.9027, Val Loss: 18.8955.      Train RMSE: 12746.0808, Val RMSE: 14.5816\n",
      "Epoch 10.      Train Loss: 5.6381, Val Loss: 6.6556.      Train RMSE: 16.8999, Val RMSE: 14.5717\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [2:50:44, 212.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0001, l2_lambda:3, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 52.8472, Val Loss: 53.4502.      Train RMSE: 3630951008.1413, Val RMSE: 223394.4822\n",
      "Epoch 5.      Train Loss: 17.9027, Val Loss: 18.8955.      Train RMSE: 12746.0808, Val RMSE: 14.5816\n",
      "Epoch 10.      Train Loss: 5.6381, Val Loss: 6.6556.      Train RMSE: 16.8999, Val RMSE: 14.5717\n",
      "Epoch 15.      Train Loss: 3.5145, Val Loss: 4.5368.      Train RMSE: 16.9141, Val RMSE: 14.5745\n",
      "Epoch 20.      Train Loss: 3.4256, Val Loss: 4.4463.      Train RMSE: 16.9142, Val RMSE: 14.5743\n",
      "Epoch 25.      Train Loss: 3.4257, Val Loss: 4.4463.      Train RMSE: 16.9142, Val RMSE: 14.5742\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [3:02:26, 359.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0001, l2_lambda:5, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 85.6468, Val Loss: 86.2480.      Train RMSE: 3490133983.9364, Val RMSE: 194350.0446\n",
      "Epoch 5.      Train Loss: 27.5744, Val Loss: 28.5796.      Train RMSE: 3460.0993, Val RMSE: 14.5883\n",
      "Epoch 10.      Train Loss: 7.1303, Val Loss: 8.1635.      Train RMSE: 16.9203, Val RMSE: 14.5861\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [3:07:27, 341.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0001, l2_lambda:5, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 85.6468, Val Loss: 86.2480.      Train RMSE: 3490133983.9364, Val RMSE: 194350.0446\n",
      "Epoch 5.      Train Loss: 27.5744, Val Loss: 28.5796.      Train RMSE: 3460.0993, Val RMSE: 14.5883\n",
      "Epoch 10.      Train Loss: 7.1303, Val Loss: 8.1635.      Train RMSE: 16.9203, Val RMSE: 14.5861\n",
      "Epoch 15.      Train Loss: 3.6055, Val Loss: 4.6455.      Train RMSE: 16.9261, Val RMSE: 14.5855\n",
      "Epoch 20.      Train Loss: 3.4583, Val Loss: 4.4968.      Train RMSE: 16.9261, Val RMSE: 14.5852\n",
      "Epoch 25.      Train Loss: 3.4583, Val Loss: 4.4969.      Train RMSE: 16.9260, Val RMSE: 14.5852\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [3:18:17, 434.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.001, l2_lambda:0.1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 2.9384, Val Loss: 3.0818.      Train RMSE: 673237417.4846, Val RMSE: 6267.2224\n",
      "Epoch 5.      Train Loss: 2.6215, Val Loss: 2.7390.      Train RMSE: 4862756.8129, Val RMSE: 3749.9716\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [3:21:51, 367.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.001, l2_lambda:0.1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 2.9384, Val Loss: 3.0818.      Train RMSE: 673237417.4846, Val RMSE: 6267.2224\n",
      "Epoch 5.      Train Loss: 2.6215, Val Loss: 2.7390.      Train RMSE: 4862756.8129, Val RMSE: 3749.9716\n",
      "Epoch 10.      Train Loss: 2.6213, Val Loss: 2.7437.      Train RMSE: 6379114.3319, Val RMSE: 6239.8753\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [3:27:51, 365.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.001, l2_lambda:1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 4.6218, Val Loss: 5.5776.      Train RMSE: 34.7660, Val RMSE: 14.3967\n",
      "Epoch 5.      Train Loss: 3.2974, Val Loss: 4.2361.      Train RMSE: 16.8596, Val RMSE: 14.5241\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [3:32:16, 335.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.001, l2_lambda:1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 4.6218, Val Loss: 5.5776.      Train RMSE: 34.7660, Val RMSE: 14.3967\n",
      "Epoch 5.      Train Loss: 3.2974, Val Loss: 4.2361.      Train RMSE: 16.8596, Val RMSE: 14.5241\n",
      "Epoch 10.      Train Loss: 3.2973, Val Loss: 4.2368.      Train RMSE: 16.8608, Val RMSE: 14.5252\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [3:40:25, 381.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.001, l2_lambda:3, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 7.3068, Val Loss: 8.3226.      Train RMSE: 16.8781, Val RMSE: 14.5657\n",
      "Epoch 5.      Train Loss: 3.4258, Val Loss: 4.4467.      Train RMSE: 16.9126, Val RMSE: 14.5727\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [3:44:32, 341.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.001, l2_lambda:3, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 7.3068, Val Loss: 8.3226.      Train RMSE: 16.8781, Val RMSE: 14.5657\n",
      "Epoch 5.      Train Loss: 3.4258, Val Loss: 4.4467.      Train RMSE: 16.9126, Val RMSE: 14.5727\n",
      "Epoch 10.      Train Loss: 3.4257, Val Loss: 4.4458.      Train RMSE: 16.9126, Val RMSE: 14.5727\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [3:52:08, 375.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.001, l2_lambda:5, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 9.8673, Val Loss: 10.8960.      Train RMSE: 16.9114, Val RMSE: 14.5850\n",
      "Epoch 5.      Train Loss: 3.4584, Val Loss: 4.4971.      Train RMSE: 16.9250, Val RMSE: 14.5842\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [3:55:40, 326.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.001, l2_lambda:5, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 9.8673, Val Loss: 10.8960.      Train RMSE: 16.9114, Val RMSE: 14.5850\n",
      "Epoch 5.      Train Loss: 3.4584, Val Loss: 4.4971.      Train RMSE: 16.9250, Val RMSE: 14.5842\n",
      "Epoch 10.      Train Loss: 3.4585, Val Loss: 4.4958.      Train RMSE: 16.9248, Val RMSE: 14.5840\n",
      "Epoch 15.      Train Loss: 3.4582, Val Loss: 4.4986.      Train RMSE: 16.9269, Val RMSE: 14.5860\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [4:00:32, 316.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0001, l2_lambda:0.1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 5.6198, Val Loss: 5.9758.      Train RMSE: 10419667158.2915, Val RMSE: 11991158.0524\n",
      "Epoch 5.      Train Loss: 4.2410, Val Loss: 4.6162.      Train RMSE: 32388244934.3968, Val RMSE: 398766.3485\n",
      "Epoch 10.      Train Loss: 3.4011, Val Loss: 3.5827.      Train RMSE: 33790904073.0632, Val RMSE: 5992.2033\n",
      "Epoch 15.      Train Loss: 2.9698, Val Loss: 3.1486.      Train RMSE: 122782933.4296, Val RMSE: 5745.8181\n",
      "Epoch 20.      Train Loss: 2.7568, Val Loss: 2.9054.      Train RMSE: 4778250.8558, Val RMSE: 5571.7351\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [4:05:19, 307.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0001, l2_lambda:0.1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 5.6198, Val Loss: 5.9758.      Train RMSE: 10419667158.2915, Val RMSE: 11991158.0524\n",
      "Epoch 5.      Train Loss: 4.2410, Val Loss: 4.6162.      Train RMSE: 32388244934.3968, Val RMSE: 398766.3485\n",
      "Epoch 10.      Train Loss: 3.4011, Val Loss: 3.5827.      Train RMSE: 33790904073.0632, Val RMSE: 5992.2033\n",
      "Epoch 15.      Train Loss: 2.9698, Val Loss: 3.1486.      Train RMSE: 122782933.4296, Val RMSE: 5745.8181\n",
      "Epoch 20.      Train Loss: 2.7568, Val Loss: 2.9054.      Train RMSE: 4778250.8558, Val RMSE: 5571.7351\n",
      "Epoch 25.      Train Loss: 2.6628, Val Loss: 2.7856.      Train RMSE: 5034569.7039, Val RMSE: 5584.1571\n",
      "Epoch 30.      Train Loss: 2.6318, Val Loss: 2.7430.      Train RMSE: 5168686.8380, Val RMSE: 5974.5299\n",
      "Epoch 35.      Train Loss: 2.6237, Val Loss: 2.7295.      Train RMSE: 5245659.0635, Val RMSE: 6290.7430\n",
      "Epoch 40.      Train Loss: 2.6211, Val Loss: 2.7269.      Train RMSE: 5357560.3670, Val RMSE: 6432.9642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [4:13:15, 357.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0001, l2_lambda:1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 22.8367, Val Loss: 23.2866.      Train RMSE: 2781117904.7591, Val RMSE: 4159483.3363\n",
      "Epoch 5.      Train Loss: 13.8523, Val Loss: 14.7054.      Train RMSE: 5501906349.8639, Val RMSE: 7832.6187\n",
      "Epoch 10.      Train Loss: 8.1111, Val Loss: 9.0528.      Train RMSE: 2857236.8153, Val RMSE: 15.1686\n",
      "Epoch 15.      Train Loss: 5.3282, Val Loss: 6.2794.      Train RMSE: 101.7601, Val RMSE: 14.3704\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "51it [4:16:32, 309.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0001, l2_lambda:1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 22.8367, Val Loss: 23.2866.      Train RMSE: 2781117904.7591, Val RMSE: 4159483.3363\n",
      "Epoch 5.      Train Loss: 13.8523, Val Loss: 14.7054.      Train RMSE: 5501906349.8639, Val RMSE: 7832.6187\n",
      "Epoch 10.      Train Loss: 8.1111, Val Loss: 9.0528.      Train RMSE: 2857236.8153, Val RMSE: 15.1686\n",
      "Epoch 15.      Train Loss: 5.3282, Val Loss: 6.2794.      Train RMSE: 101.7601, Val RMSE: 14.3704\n",
      "Epoch 20.      Train Loss: 4.0240, Val Loss: 4.9711.      Train RMSE: 17.0540, Val RMSE: 14.4355\n",
      "Epoch 25.      Train Loss: 3.4912, Val Loss: 4.4345.      Train RMSE: 16.7664, Val RMSE: 14.4787\n",
      "Epoch 30.      Train Loss: 3.3280, Val Loss: 4.2682.      Train RMSE: 16.8396, Val RMSE: 14.5118\n",
      "Epoch 35.      Train Loss: 3.2986, Val Loss: 4.2373.      Train RMSE: 16.8605, Val RMSE: 14.5251\n",
      "Epoch 40.      Train Loss: 3.2969, Val Loss: 4.2363.      Train RMSE: 16.8617, Val RMSE: 14.5260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "52it [4:24:59, 369.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0001, l2_lambda:3, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 60.9012, Val Loss: 61.3282.      Train RMSE: 2215695922.9334, Val RMSE: 3756913.6588\n",
      "Epoch 5.      Train Loss: 34.8865, Val Loss: 35.7535.      Train RMSE: 2571885792.9073, Val RMSE: 4740.7225\n",
      "Epoch 10.      Train Loss: 17.8706, Val Loss: 18.8623.      Train RMSE: 12373.3145, Val RMSE: 14.5498\n",
      "Epoch 15.      Train Loss: 9.5453, Val Loss: 10.5528.      Train RMSE: 16.8613, Val RMSE: 14.5632\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53it [4:28:15, 317.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0001, l2_lambda:3, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 60.9012, Val Loss: 61.3282.      Train RMSE: 2215695922.9334, Val RMSE: 3756913.6588\n",
      "Epoch 5.      Train Loss: 34.8865, Val Loss: 35.7535.      Train RMSE: 2571885792.9073, Val RMSE: 4740.7225\n",
      "Epoch 10.      Train Loss: 17.8706, Val Loss: 18.8623.      Train RMSE: 12373.3145, Val RMSE: 14.5498\n",
      "Epoch 15.      Train Loss: 9.5453, Val Loss: 10.5528.      Train RMSE: 16.8613, Val RMSE: 14.5632\n",
      "Epoch 20.      Train Loss: 5.6256, Val Loss: 6.6419.      Train RMSE: 16.8999, Val RMSE: 14.5718\n",
      "Epoch 25.      Train Loss: 4.0144, Val Loss: 5.0355.      Train RMSE: 16.9101, Val RMSE: 14.5731\n",
      "Epoch 30.      Train Loss: 3.5122, Val Loss: 4.5328.      Train RMSE: 16.9134, Val RMSE: 14.5738\n",
      "Epoch 35.      Train Loss: 3.4280, Val Loss: 4.4476.      Train RMSE: 16.9136, Val RMSE: 14.5737\n",
      "Epoch 40.      Train Loss: 3.4252, Val Loss: 4.4455.      Train RMSE: 16.9137, Val RMSE: 14.5738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [4:37:10, 382.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0001, l2_lambda:5, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 98.9646, Val Loss: 99.3876.      Train RMSE: 2132930914.6495, Val RMSE: 3690688.9184\n",
      "Epoch 5.      Train Loss: 55.8694, Val Loss: 56.7444.      Train RMSE: 2533614260.4735, Val RMSE: 4251.7886\n",
      "Epoch 10.      Train Loss: 27.5257, Val Loss: 28.5299.      Train RMSE: 3378.1899, Val RMSE: 14.5726\n",
      "Epoch 15.      Train Loss: 13.6371, Val Loss: 14.6556.      Train RMSE: 16.9032, Val RMSE: 14.5846\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [4:40:09, 321.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0001, l2_lambda:5, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 98.9646, Val Loss: 99.3876.      Train RMSE: 2132930914.6495, Val RMSE: 3690688.9184\n",
      "Epoch 5.      Train Loss: 55.8694, Val Loss: 56.7444.      Train RMSE: 2533614260.4735, Val RMSE: 4251.7886\n",
      "Epoch 10.      Train Loss: 27.5257, Val Loss: 28.5299.      Train RMSE: 3378.1899, Val RMSE: 14.5726\n",
      "Epoch 15.      Train Loss: 13.6371, Val Loss: 14.6556.      Train RMSE: 16.9032, Val RMSE: 14.5846\n",
      "Epoch 20.      Train Loss: 7.1102, Val Loss: 8.1421.      Train RMSE: 16.9202, Val RMSE: 14.5861\n",
      "Epoch 25.      Train Loss: 4.4356, Val Loss: 5.4739.      Train RMSE: 16.9241, Val RMSE: 14.5849\n",
      "Epoch 30.      Train Loss: 3.6022, Val Loss: 4.6406.      Train RMSE: 16.9256, Val RMSE: 14.5850\n",
      "Epoch 35.      Train Loss: 3.4618, Val Loss: 4.4992.      Train RMSE: 16.9255, Val RMSE: 14.5847\n",
      "Epoch 40.      Train Loss: 3.4578, Val Loss: 4.4960.      Train RMSE: 16.9256, Val RMSE: 14.5847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "56it [4:58:30, 555.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:0.1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 3.5072, Val Loss: 3.6584.      Train RMSE: 35299990080.5313, Val RMSE: 6860.4574\n",
      "Epoch 5.      Train Loss: 2.6232, Val Loss: 2.7445.      Train RMSE: 4828405.0805, Val RMSE: 5127.0974\n",
      "Epoch 10.      Train Loss: 2.6214, Val Loss: 2.7317.      Train RMSE: 6453402.0369, Val RMSE: 3763.0015\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [5:06:05, 525.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:0.1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 3.5072, Val Loss: 3.6584.      Train RMSE: 35299990080.5313, Val RMSE: 6860.4574\n",
      "Epoch 5.      Train Loss: 2.6232, Val Loss: 2.7445.      Train RMSE: 4828405.0805, Val RMSE: 5127.0974\n",
      "Epoch 10.      Train Loss: 2.6214, Val Loss: 2.7317.      Train RMSE: 6453402.0369, Val RMSE: 3763.0015\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "58it [5:12:51, 489.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 8.5327, Val Loss: 9.4707.      Train RMSE: 6421883.7228, Val RMSE: 27.3454\n",
      "Epoch 5.      Train Loss: 3.3064, Val Loss: 4.2432.      Train RMSE: 16.8494, Val RMSE: 14.5177\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "59it [5:16:00, 399.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 8.5327, Val Loss: 9.4707.      Train RMSE: 6421883.7228, Val RMSE: 27.3454\n",
      "Epoch 5.      Train Loss: 3.3064, Val Loss: 4.2432.      Train RMSE: 16.8494, Val RMSE: 14.5177\n",
      "Epoch 10.      Train Loss: 3.2973, Val Loss: 4.2358.      Train RMSE: 16.8623, Val RMSE: 14.5266\n",
      "Epoch 15.      Train Loss: 3.2964, Val Loss: 4.2390.      Train RMSE: 16.8643, Val RMSE: 14.5283\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60it [5:21:48, 384.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:3, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 19.0289, Val Loss: 20.0156.      Train RMSE: 36173.8009, Val RMSE: 31.2079\n",
      "Epoch 5.      Train Loss: 3.4417, Val Loss: 4.4604.      Train RMSE: 16.9127, Val RMSE: 14.5729\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "61it [5:25:04, 327.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:3, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 19.0289, Val Loss: 20.0156.      Train RMSE: 36173.8009, Val RMSE: 31.2079\n",
      "Epoch 5.      Train Loss: 3.4417, Val Loss: 4.4604.      Train RMSE: 16.9127, Val RMSE: 14.5729\n",
      "Epoch 10.      Train Loss: 3.4257, Val Loss: 4.4453.      Train RMSE: 16.9135, Val RMSE: 14.5736\n",
      "Epoch 15.      Train Loss: 3.4248, Val Loss: 4.4483.      Train RMSE: 16.9149, Val RMSE: 14.5749\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [5:35:32, 417.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:5, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 29.4278, Val Loss: 30.4283.      Train RMSE: 11562.0375, Val RMSE: 32.7399\n",
      "Epoch 5.      Train Loss: 3.4841, Val Loss: 4.5213.      Train RMSE: 16.9248, Val RMSE: 14.5841\n",
      "Epoch 10.      Train Loss: 3.4584, Val Loss: 4.4958.      Train RMSE: 16.9252, Val RMSE: 14.5844\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "63it [5:39:38, 366.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:5, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 29.4278, Val Loss: 30.4283.      Train RMSE: 11562.0375, Val RMSE: 32.7399\n",
      "Epoch 5.      Train Loss: 3.4841, Val Loss: 4.5213.      Train RMSE: 16.9248, Val RMSE: 14.5841\n",
      "Epoch 10.      Train Loss: 3.4584, Val Loss: 4.4958.      Train RMSE: 16.9252, Val RMSE: 14.5844\n",
      "Epoch 15.      Train Loss: 3.4574, Val Loss: 4.4981.      Train RMSE: 16.9266, Val RMSE: 14.5857\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [5:42:56, 315.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:128, lr: 0.0001, l2_lambda:0.1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 5.0137, Val Loss: 5.7368.      Train RMSE: 66934088352.5270, Val RMSE: 4249.0036\n",
      "Epoch 5.      Train Loss: 3.4476, Val Loss: 3.1026.      Train RMSE: 11605118.4520, Val RMSE: 3418.3151\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65it [5:44:50, 255.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:128, lr: 0.0001, l2_lambda:0.1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 5.0137, Val Loss: 5.7368.      Train RMSE: 66934088352.5270, Val RMSE: 4249.0036\n",
      "Epoch 5.      Train Loss: 3.4476, Val Loss: 3.1026.      Train RMSE: 11605118.4520, Val RMSE: 3418.3151\n",
      "Epoch 10.      Train Loss: 2.9398, Val Loss: 2.5250.      Train RMSE: 101892.1255, Val RMSE: 2258.2007\n",
      "Epoch 15.      Train Loss: 2.7955, Val Loss: 2.3459.      Train RMSE: 14338.0557, Val RMSE: 2097.9010\n",
      "Epoch 20.      Train Loss: 2.7231, Val Loss: 2.2510.      Train RMSE: 1371.1926, Val RMSE: 2149.2714\n",
      "Epoch 25.      Train Loss: 2.6847, Val Loss: 2.2009.      Train RMSE: 1114.6836, Val RMSE: 2064.1250\n",
      "Epoch 30.      Train Loss: 2.6624, Val Loss: 2.1711.      Train RMSE: 1144.0945, Val RMSE: 2167.4091\n",
      "Epoch 35.      Train Loss: 2.6476, Val Loss: 2.1556.      Train RMSE: 1065.3790, Val RMSE: 1817.7352\n",
      "Epoch 40.      Train Loss: 2.6361, Val Loss: 2.1441.      Train RMSE: 1074.3104, Val RMSE: 2078.7126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [5:54:54, 359.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:128, lr: 0.0001, l2_lambda:1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 19.4357, Val Loss: 20.3547.      Train RMSE: 53435625226.0227, Val RMSE: 22808.0267\n",
      "Epoch 5.      Train Loss: 6.9526, Val Loss: 7.9294.      Train RMSE: 4294581614.6832, Val RMSE: 267.4795\n",
      "Epoch 10.      Train Loss: 3.7308, Val Loss: 4.6741.      Train RMSE: 35.7561, Val RMSE: 14.3669\n",
      "Epoch 15.      Train Loss: 3.3037, Val Loss: 4.2504.      Train RMSE: 16.6740, Val RMSE: 14.4350\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "67it [5:58:48, 321.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:128, lr: 0.0001, l2_lambda:1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 19.4357, Val Loss: 20.3547.      Train RMSE: 53435625226.0227, Val RMSE: 22808.0267\n",
      "Epoch 5.      Train Loss: 6.9526, Val Loss: 7.9294.      Train RMSE: 4294581614.6832, Val RMSE: 267.4795\n",
      "Epoch 10.      Train Loss: 3.7308, Val Loss: 4.6741.      Train RMSE: 35.7561, Val RMSE: 14.3669\n",
      "Epoch 15.      Train Loss: 3.3037, Val Loss: 4.2504.      Train RMSE: 16.6740, Val RMSE: 14.4350\n",
      "Epoch 20.      Train Loss: 3.2964, Val Loss: 4.2389.      Train RMSE: 16.7771, Val RMSE: 14.4769\n",
      "Epoch 25.      Train Loss: 3.2965, Val Loss: 4.2390.      Train RMSE: 16.7848, Val RMSE: 14.4801\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "68it [6:05:03, 337.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:128, lr: 0.0001, l2_lambda:3, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 51.0518, Val Loss: 52.0043.      Train RMSE: 49075255313.1890, Val RMSE: 13753.3845\n",
      "Epoch 5.      Train Loss: 14.3006, Val Loss: 15.3844.      Train RMSE: 938793.1183, Val RMSE: 44.0121\n",
      "Epoch 10.      Train Loss: 4.8187, Val Loss: 5.8490.      Train RMSE: 16.8847, Val RMSE: 14.5641\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [6:08:00, 289.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:128, lr: 0.0001, l2_lambda:3, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 51.0518, Val Loss: 52.0043.      Train RMSE: 49075255313.1890, Val RMSE: 13753.3845\n",
      "Epoch 5.      Train Loss: 14.3006, Val Loss: 15.3844.      Train RMSE: 938793.1183, Val RMSE: 44.0121\n",
      "Epoch 10.      Train Loss: 4.8187, Val Loss: 5.8490.      Train RMSE: 16.8847, Val RMSE: 14.5641\n",
      "Epoch 15.      Train Loss: 3.4630, Val Loss: 4.4855.      Train RMSE: 16.9139, Val RMSE: 14.5712\n",
      "Epoch 20.      Train Loss: 3.4257, Val Loss: 4.4466.      Train RMSE: 16.9155, Val RMSE: 14.5720\n",
      "Epoch 25.      Train Loss: 3.4258, Val Loss: 4.4469.      Train RMSE: 16.9150, Val RMSE: 14.5714\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [6:14:07, 312.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:128, lr: 0.0001, l2_lambda:5, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 82.8321, Val Loss: 83.7953.      Train RMSE: 49449036200.4928, Val RMSE: 13602.5827\n",
      "Epoch 5.      Train Loss: 21.6015, Val Loss: 22.7061.      Train RMSE: 192482.8382, Val RMSE: 30.2231\n",
      "Epoch 10.      Train Loss: 5.7813, Val Loss: 6.8284.      Train RMSE: 16.9147, Val RMSE: 14.5802\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [6:16:41, 265.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:128, lr: 0.0001, l2_lambda:5, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 82.8321, Val Loss: 83.7953.      Train RMSE: 49449036200.4928, Val RMSE: 13602.5827\n",
      "Epoch 5.      Train Loss: 21.6015, Val Loss: 22.7061.      Train RMSE: 192482.8382, Val RMSE: 30.2231\n",
      "Epoch 10.      Train Loss: 5.7813, Val Loss: 6.8284.      Train RMSE: 16.9147, Val RMSE: 14.5802\n",
      "Epoch 15.      Train Loss: 3.5197, Val Loss: 4.5598.      Train RMSE: 16.9264, Val RMSE: 14.5824\n",
      "Epoch 20.      Train Loss: 3.4584, Val Loss: 4.4973.      Train RMSE: 16.9273, Val RMSE: 14.5829\n",
      "Epoch 25.      Train Loss: 3.4585, Val Loss: 4.4975.      Train RMSE: 16.9270, Val RMSE: 14.5826\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [6:22:06, 283.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:128, lr: 0.001, l2_lambda:0.1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 2.9282, Val Loss: 2.5218.      Train RMSE: 80885.6607, Val RMSE: 1261.5621\n",
      "Epoch 5.      Train Loss: 2.5854, Val Loss: 2.1224.      Train RMSE: 415226.9691, Val RMSE: 21078.8470\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73it [6:23:36, 225.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:128, lr: 0.001, l2_lambda:0.1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 2.9282, Val Loss: 2.5218.      Train RMSE: 80885.6607, Val RMSE: 1261.5621\n",
      "Epoch 5.      Train Loss: 2.5854, Val Loss: 2.1224.      Train RMSE: 415226.9691, Val RMSE: 21078.8470\n",
      "Epoch 10.      Train Loss: 2.5743, Val Loss: 2.1281.      Train RMSE: 655271.9441, Val RMSE: 8783.7869\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [6:26:02, 201.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:128, lr: 0.001, l2_lambda:1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 4.1188, Val Loss: 5.0711.      Train RMSE: 404.7309, Val RMSE: 14.5443\n",
      "Epoch 5.      Train Loss: 3.2966, Val Loss: 4.2411.      Train RMSE: 16.7906, Val RMSE: 14.4832\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "75it [6:27:34, 168.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:128, lr: 0.001, l2_lambda:1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 4.1188, Val Loss: 5.0711.      Train RMSE: 404.7309, Val RMSE: 14.5443\n",
      "Epoch 5.      Train Loss: 3.2966, Val Loss: 4.2411.      Train RMSE: 16.7906, Val RMSE: 14.4832\n",
      "Epoch 10.      Train Loss: 3.2969, Val Loss: 4.2396.      Train RMSE: 16.7931, Val RMSE: 14.4833\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "76it [6:30:29, 170.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:128, lr: 0.001, l2_lambda:3, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 5.9966, Val Loss: 7.0380.      Train RMSE: 16.8716, Val RMSE: 14.5595\n",
      "Epoch 5.      Train Loss: 3.4259, Val Loss: 4.4491.      Train RMSE: 16.9150, Val RMSE: 14.5715\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77it [6:31:49, 143.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:128, lr: 0.001, l2_lambda:3, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 5.9966, Val Loss: 7.0380.      Train RMSE: 16.8716, Val RMSE: 14.5595\n",
      "Epoch 5.      Train Loss: 3.4259, Val Loss: 4.4491.      Train RMSE: 16.9150, Val RMSE: 14.5715\n",
      "Epoch 10.      Train Loss: 3.4263, Val Loss: 4.4457.      Train RMSE: 16.9136, Val RMSE: 14.5702\n",
      "Epoch 15.      Train Loss: 3.4259, Val Loss: 4.4478.      Train RMSE: 16.9149, Val RMSE: 14.5714\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78it [6:35:23, 164.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:128, lr: 0.001, l2_lambda:5, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 7.7368, Val Loss: 8.7948.      Train RMSE: 16.9071, Val RMSE: 14.5798\n",
      "Epoch 5.      Train Loss: 3.4586, Val Loss: 4.4997.      Train RMSE: 16.9263, Val RMSE: 14.5819\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "79it [6:37:59, 161.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:128, lr: 0.001, l2_lambda:5, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 7.7368, Val Loss: 8.7948.      Train RMSE: 16.9071, Val RMSE: 14.5798\n",
      "Epoch 5.      Train Loss: 3.4586, Val Loss: 4.4997.      Train RMSE: 16.9263, Val RMSE: 14.5819\n",
      "Epoch 10.      Train Loss: 3.4591, Val Loss: 4.4964.      Train RMSE: 16.9253, Val RMSE: 14.5810\n",
      "Epoch 15.      Train Loss: 3.4585, Val Loss: 4.4977.      Train RMSE: 16.9270, Val RMSE: 14.5826\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [6:46:20, 263.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:256, lr: 0.0001, l2_lambda:0.1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 5.3270, Val Loss: 6.0908.      Train RMSE: 74493499163.6881, Val RMSE: 38608.7658\n",
      "Epoch 5.      Train Loss: 4.0745, Val Loss: 3.9466.      Train RMSE: 45556272590.0099, Val RMSE: 2356.8253\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "81it [6:49:23, 239.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:256, lr: 0.0001, l2_lambda:0.1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 5.3270, Val Loss: 6.0908.      Train RMSE: 74493499163.6881, Val RMSE: 38608.7658\n",
      "Epoch 5.      Train Loss: 4.0745, Val Loss: 3.9466.      Train RMSE: 45556272590.0099, Val RMSE: 2356.8253\n",
      "Epoch 10.      Train Loss: 3.3833, Val Loss: 3.0252.      Train RMSE: 1536079.2726, Val RMSE: 2844.5684\n",
      "Epoch 15.      Train Loss: 3.0530, Val Loss: 2.6532.      Train RMSE: 57775.0850, Val RMSE: 2119.3707\n",
      "Epoch 20.      Train Loss: 2.8888, Val Loss: 2.4601.      Train RMSE: 103360.3616, Val RMSE: 1776.7667\n",
      "Epoch 25.      Train Loss: 2.8048, Val Loss: 2.3553.      Train RMSE: 55903.8786, Val RMSE: 1671.6754\n",
      "Epoch 30.      Train Loss: 2.7545, Val Loss: 2.2860.      Train RMSE: 9451.3186, Val RMSE: 1738.7555\n",
      "Epoch 35.      Train Loss: 2.7199, Val Loss: 2.2429.      Train RMSE: 1596.6268, Val RMSE: 1893.7435\n",
      "Epoch 40.      Train Loss: 2.6942, Val Loss: 2.2090.      Train RMSE: 1133.3112, Val RMSE: 2066.1228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "82it [7:02:51, 410.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:256, lr: 0.0001, l2_lambda:1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 22.6507, Val Loss: 23.5169.      Train RMSE: 71641627645.7561, Val RMSE: 33075.3225\n",
      "Epoch 5.      Train Loss: 12.5474, Val Loss: 13.5321.      Train RMSE: 32240194799.0147, Val RMSE: 3994.8748\n",
      "Epoch 10.      Train Loss: 6.9038, Val Loss: 7.8809.      Train RMSE: 616108282.5112, Val RMSE: 164.6670\n",
      "Epoch 15.      Train Loss: 4.6267, Val Loss: 5.5797.      Train RMSE: 351543.7101, Val RMSE: 14.9378\n",
      "Epoch 20.      Train Loss: 3.7185, Val Loss: 4.6599.      Train RMSE: 39.7814, Val RMSE: 14.3625\n",
      "Epoch 25.      Train Loss: 3.3875, Val Loss: 4.3312.      Train RMSE: 17.4010, Val RMSE: 14.3845\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [7:09:56, 414.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:256, lr: 0.0001, l2_lambda:1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 22.6507, Val Loss: 23.5169.      Train RMSE: 71641627645.7561, Val RMSE: 33075.3225\n",
      "Epoch 5.      Train Loss: 12.5474, Val Loss: 13.5321.      Train RMSE: 32240194799.0147, Val RMSE: 3994.8748\n",
      "Epoch 10.      Train Loss: 6.9038, Val Loss: 7.8809.      Train RMSE: 616108282.5112, Val RMSE: 164.6670\n",
      "Epoch 15.      Train Loss: 4.6267, Val Loss: 5.5797.      Train RMSE: 351543.7101, Val RMSE: 14.9378\n",
      "Epoch 20.      Train Loss: 3.7185, Val Loss: 4.6599.      Train RMSE: 39.7814, Val RMSE: 14.3625\n",
      "Epoch 25.      Train Loss: 3.3875, Val Loss: 4.3312.      Train RMSE: 17.4010, Val RMSE: 14.3845\n",
      "Epoch 30.      Train Loss: 3.3040, Val Loss: 4.2484.      Train RMSE: 16.6774, Val RMSE: 14.4361\n",
      "Epoch 35.      Train Loss: 3.2973, Val Loss: 4.2396.      Train RMSE: 16.7588, Val RMSE: 14.4685\n",
      "Epoch 40.      Train Loss: 3.2963, Val Loss: 4.2386.      Train RMSE: 16.7788, Val RMSE: 14.4775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [7:19:07, 455.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:256, lr: 0.0001, l2_lambda:3, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 60.9484, Val Loss: 61.8272.      Train RMSE: 71620726615.4911, Val RMSE: 32823.4082\n",
      "Epoch 5.      Train Loss: 30.7513, Val Loss: 31.8118.      Train RMSE: 34143174154.3224, Val RMSE: 90628.4067\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "85it [7:20:53, 350.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:256, lr: 0.0001, l2_lambda:3, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 60.9484, Val Loss: 61.8272.      Train RMSE: 71620726615.4911, Val RMSE: 32823.4082\n",
      "Epoch 5.      Train Loss: 30.7513, Val Loss: 31.8118.      Train RMSE: 34143174154.3224, Val RMSE: 90628.4067\n",
      "Epoch 10.      Train Loss: 14.2729, Val Loss: 15.3544.      Train RMSE: 970253.3065, Val RMSE: 44.4870\n",
      "Epoch 15.      Train Loss: 7.5687, Val Loss: 8.6195.      Train RMSE: 888.3667, Val RMSE: 14.5512\n",
      "Epoch 20.      Train Loss: 4.8094, Val Loss: 5.8389.      Train RMSE: 16.8854, Val RMSE: 14.5644\n",
      "Epoch 25.      Train Loss: 3.7634, Val Loss: 4.7863.      Train RMSE: 16.9063, Val RMSE: 14.5686\n",
      "Epoch 30.      Train Loss: 3.4629, Val Loss: 4.4833.      Train RMSE: 16.9142, Val RMSE: 14.5714\n",
      "Epoch 35.      Train Loss: 3.4273, Val Loss: 4.4473.      Train RMSE: 16.9150, Val RMSE: 14.5715\n",
      "Epoch 40.      Train Loss: 3.4256, Val Loss: 4.4465.      Train RMSE: 16.9152, Val RMSE: 14.5717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [7:30:32, 419.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:256, lr: 0.0001, l2_lambda:5, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 99.2956, Val Loss: 100.1763.      Train RMSE: 71716395467.4682, Val RMSE: 35734.8225\n",
      "Epoch 5.      Train Loss: 49.0273, Val Loss: 50.1130.      Train RMSE: 35174142557.3451, Val RMSE: 81669.1848\n",
      "Epoch 10.      Train Loss: 21.5645, Val Loss: 22.6669.      Train RMSE: 190998.2829, Val RMSE: 30.1764\n",
      "Epoch 15.      Train Loss: 10.3801, Val Loss: 11.4482.      Train RMSE: 167.5958, Val RMSE: 14.5777\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "87it [7:34:57, 372.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:256, lr: 0.0001, l2_lambda:5, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 99.2956, Val Loss: 100.1763.      Train RMSE: 71716395467.4682, Val RMSE: 35734.8225\n",
      "Epoch 5.      Train Loss: 49.0273, Val Loss: 50.1130.      Train RMSE: 35174142557.3451, Val RMSE: 81669.1848\n",
      "Epoch 10.      Train Loss: 21.5645, Val Loss: 22.6669.      Train RMSE: 190998.2829, Val RMSE: 30.1764\n",
      "Epoch 15.      Train Loss: 10.3801, Val Loss: 11.4482.      Train RMSE: 167.5958, Val RMSE: 14.5777\n",
      "Epoch 20.      Train Loss: 5.7676, Val Loss: 6.8141.      Train RMSE: 16.9152, Val RMSE: 14.5805\n",
      "Epoch 25.      Train Loss: 4.0195, Val Loss: 5.0595.      Train RMSE: 16.9228, Val RMSE: 14.5813\n",
      "Epoch 30.      Train Loss: 3.5191, Val Loss: 4.5572.      Train RMSE: 16.9266, Val RMSE: 14.5826\n",
      "Epoch 35.      Train Loss: 3.4603, Val Loss: 4.4981.      Train RMSE: 16.9269, Val RMSE: 14.5825\n",
      "Epoch 40.      Train Loss: 3.4582, Val Loss: 4.4970.      Train RMSE: 16.9271, Val RMSE: 14.5827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88it [7:44:03, 424.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:256, lr: 0.001, l2_lambda:0.1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 3.3916, Val Loss: 3.0388.      Train RMSE: 7771036.3520, Val RMSE: 2274.8308\n",
      "Epoch 5.      Train Loss: 2.6623, Val Loss: 2.1933.      Train RMSE: 1597.9900, Val RMSE: 3021.2112\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [7:45:16, 319.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:256, lr: 0.001, l2_lambda:0.1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 3.3916, Val Loss: 3.0388.      Train RMSE: 7771036.3520, Val RMSE: 2274.8308\n",
      "Epoch 5.      Train Loss: 2.6623, Val Loss: 2.1933.      Train RMSE: 1597.9900, Val RMSE: 3021.2112\n",
      "Epoch 10.      Train Loss: 2.5967, Val Loss: 2.1129.      Train RMSE: 930.7926, Val RMSE: 1906.9597\n",
      "Epoch 15.      Train Loss: 2.5941, Val Loss: 2.1165.      Train RMSE: 1563.3826, Val RMSE: 3193.7630\n",
      "Epoch 20.      Train Loss: 2.5908, Val Loss: 2.1004.      Train RMSE: 1261.6273, Val RMSE: 2534.6807\n",
      "Epoch 25.      Train Loss: 2.5833, Val Loss: 2.1123.      Train RMSE: 8830.4677, Val RMSE: 1080.6973\n",
      "Epoch 30.      Train Loss: 2.5752, Val Loss: 2.0865.      Train RMSE: 578771.7276, Val RMSE: 24825.5030\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [7:50:29, 317.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:256, lr: 0.001, l2_lambda:1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 7.3009, Val Loss: 8.2787.      Train RMSE: 30097361079.1842, Val RMSE: 839.9543\n",
      "Epoch 5.      Train Loss: 3.2988, Val Loss: 4.2440.      Train RMSE: 16.7116, Val RMSE: 14.4508\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [7:51:47, 245.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:256, lr: 0.001, l2_lambda:1, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 7.3009, Val Loss: 8.2787.      Train RMSE: 30097361079.1842, Val RMSE: 839.9543\n",
      "Epoch 5.      Train Loss: 3.2988, Val Loss: 4.2440.      Train RMSE: 16.7116, Val RMSE: 14.4508\n",
      "Epoch 10.      Train Loss: 3.2976, Val Loss: 4.2401.      Train RMSE: 16.7867, Val RMSE: 14.4807\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [7:53:57, 210.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:256, lr: 0.001, l2_lambda:3, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 15.3891, Val Loss: 16.4723.      Train RMSE: 2731989.9727, Val RMSE: 74.0456\n",
      "Epoch 5.      Train Loss: 3.4334, Val Loss: 4.4554.      Train RMSE: 16.9155, Val RMSE: 14.5721\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "93it [7:55:03, 167.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:256, lr: 0.001, l2_lambda:3, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 15.3891, Val Loss: 16.4723.      Train RMSE: 2731989.9727, Val RMSE: 74.0456\n",
      "Epoch 5.      Train Loss: 3.4334, Val Loss: 4.4554.      Train RMSE: 16.9155, Val RMSE: 14.5721\n",
      "Epoch 10.      Train Loss: 3.4271, Val Loss: 4.4457.      Train RMSE: 16.9148, Val RMSE: 14.5713\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "94it [7:57:10, 155.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:256, lr: 0.001, l2_lambda:5, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 23.4133, Val Loss: 24.5184.      Train RMSE: 714313.3095, Val RMSE: 51.1019\n",
      "Epoch 5.      Train Loss: 3.4705, Val Loss: 4.5103.      Train RMSE: 16.9274, Val RMSE: 14.5830\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "95it [7:58:21, 130.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:256, lr: 0.001, l2_lambda:5, likelihood\n",
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 23.4133, Val Loss: 24.5184.      Train RMSE: 714313.3095, Val RMSE: 51.1019\n",
      "Epoch 5.      Train Loss: 3.4705, Val Loss: 4.5103.      Train RMSE: 16.9274, Val RMSE: 14.5830\n",
      "Epoch 10.      Train Loss: 3.4599, Val Loss: 4.4958.      Train RMSE: 16.9266, Val RMSE: 14.5822\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96it [8:00:29, 300.30s/it]\n"
     ]
    }
   ],
   "source": [
    "h_list = [5, 10, 25]#[4, 6, 8, 10]\n",
    "batch_size_list = [128, 256]\n",
    "lr_list = [1e-4, 1e-3]\n",
    "l2_lambda_list = [0.1, 1, 3, 5]\n",
    "stop_criteria_list = ['RMSE', 'likelihood']\n",
    "\n",
    "d1=source1_tensor.shape[1]  # number of features of the 1st source\n",
    "d2=source2_tensor.shape[1]  # number of features of the 2nd source\n",
    "\n",
    "# Make sure the folder exists\n",
    "output_dir = 'validation_results_final_attempt_normalized_direct'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for h, batch_size, lr, l2_lambda, stop_cr in tqdm.tqdm(itertools.product(h_list, batch_size_list, lr_list, l2_lambda_list, stop_criteria_list)):\n",
    "    # if [h, batch_size, lr, l2_lambda] in existing_results:\n",
    "    #     continue\n",
    "\n",
    "    # create dataset\n",
    "    train_loader, val_loader, test_loader = create_datasets(source1_array, source2_array, target_direct_array, weight_array, batch_size, h)\n",
    "\n",
    "    print(f\"\\n📊 h: {h}, batch_size:{batch_size}, lr: {lr}, l2_lambda:{l2_lambda}, {stop_cr}\")\n",
    "    \n",
    "    ensemble, losses = train_tme_ensemble(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        d1=d1,  # number of features of the 1st source\n",
    "        d2=d2,  # number of features of the 2nd source\n",
    "        h=h,  # lag length\n",
    "        num_models=1,#20,\n",
    "        device=device,\n",
    "        lr=lr,\n",
    "        weight_decay=0.1,\n",
    "        l2_lambda=l2_lambda,\n",
    "        max_epochs=40,\n",
    "        patience=5,\n",
    "        adam=True,\n",
    "        direct_target=True,\n",
    "        stop_criteria=stop_cr\n",
    "    )\n",
    "\n",
    "    y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, val_loader, device='cpu', direct_target=True)\n",
    "\n",
    "    results_to_save = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'val_losses': losses\n",
    "    }\n",
    "\n",
    "    # Create a filename based on hyperparameters\n",
    "    filename = f\"h{h}_batch{batch_size}_lr{lr:.0e}_lambda{l2_lambda}_{stop_cr}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Save the dictionary to a JSON file\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results_to_save, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6380663c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'h': 100, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 68979.37607658409, 'mae': 59339.983965488624, 'val_losses': [0.26479744779892633]}, {'h': 100, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 68979.37607658409, 'mae': 59339.983965488624, 'val_losses': [0.26479744779892633]}, {'h': 100, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 15.069406876841056, 'mae': 5.043644012542797, 'val_losses': [7.617420178944947]}, {'h': 100, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 15.069406876841056, 'mae': 5.043644012542797, 'val_losses': [7.617420178944947]}, {'h': 100, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 54897.372032017556, 'mae': 32563.52545414799, 'val_losses': [19.377565383911133]}, {'h': 100, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 54897.372032017556, 'mae': 32563.52545414799, 'val_losses': [19.377565383911133]}, {'h': 100, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 68056548271988.234, 'mae': 62998276469511.89, 'val_losses': [7.836399936773738]}, {'h': 100, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 68056548271988.234, 'mae': 62998276469511.89, 'val_losses': [7.836399936773738]}, {'h': 100, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 12171518422225.041, 'mae': 2210299555252.9517, 'val_losses': [12.745102624424169]}, {'h': 100, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 12171518422225.041, 'mae': 2210299555252.9517, 'val_losses': [12.745102624424169]}, {'h': 100, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 3089832644.857638, 'mae': 2860182247.1295567, 'val_losses': [0.6772387618290596]}, {'h': 100, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 3089832644.857638, 'mae': 2860182247.1295567, 'val_losses': [0.6772387618290596]}, {'h': 100, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 67310.86018505298, 'mae': 46268.41250250406, 'val_losses': [7.6765339227973435]}, {'h': 100, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 67310.86018505298, 'mae': 46268.41250250406, 'val_losses': [7.6765339227973435]}, {'h': 100, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 40155.73999565415, 'mae': 21051.379042949913, 'val_losses': [2.9053135387233047]}, {'h': 100, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 40155.73999565415, 'mae': 21051.379042949913, 'val_losses': [2.9053135387233047]}, {'h': 100, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 15.069406876841056, 'mae': 5.043644012542797, 'val_losses': [18.64002033921539]}, {'h': 100, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 15.069406876841056, 'mae': 5.043644012542797, 'val_losses': [18.64002033921539]}, {'h': 100, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 15.069406876841056, 'mae': 5.043644012542797, 'val_losses': [7.836691892049352]}, {'h': 100, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 15.069406876841056, 'mae': 5.043644012542797, 'val_losses': [7.836691892049352]}, {'h': 100, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 99123.06266389378, 'mae': 66658.7586020746, 'val_losses': [8.092309262908874]}, {'h': 100, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 99123.06266389378, 'mae': 66658.7586020746, 'val_losses': [8.092309262908874]}, {'h': 100, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 80395.05931500279, 'mae': 57145.44888221134, 'val_losses': [2.51166455667527]}, {'h': 100, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 80395.05931500279, 'mae': 57145.44888221134, 'val_losses': [2.51166455667527]}, {'h': 100, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 14774103260233.932, 'mae': 3129089929818.645, 'val_losses': [11.5506896190956]}, {'h': 100, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 14774103260233.932, 'mae': 3129089929818.645, 'val_losses': [11.5506896190956]}, {'h': 100, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 14381965694791.578, 'mae': 3003361700746.9844, 'val_losses': [29.38122055178783]}, {'h': 100, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 14381965694791.578, 'mae': 3003361700746.9844, 'val_losses': [29.38122055178783]}, {'h': 100, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 24897.789117041513, 'mae': 12205.2122721225, 'val_losses': [2.64041359111911]}, {'h': 100, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 24897.789117041513, 'mae': 12205.2122721225, 'val_losses': [2.64041359111911]}, {'h': 100, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 9455814357287.006, 'mae': 1317582936061.2969, 'val_losses': [98.2235306286421]}, {'h': 100, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 9455814357287.006, 'mae': 1317582936061.2969, 'val_losses': [98.2235306286421]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 15.072409210499876, 'mae': 5.049950293037167, 'val_losses': [7.526100934773195]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 15.072409210499876, 'mae': 5.049950293037167, 'val_losses': [7.526100934773195]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 89826.04724898943, 'mae': 64080.27113042802, 'val_losses': [1.746459339485794]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 89826.04724898943, 'mae': 64080.27113042802, 'val_losses': [1.746459339485794]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 10857.77149335717, 'mae': 2522.0454414450123, 'val_losses': [3.1398896584745315]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 10857.77149335717, 'mae': 2522.0454414450123, 'val_losses': [3.1398896584745315]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 5094401881.233604, 'mae': 4715922935.838725, 'val_losses': [3.5218634155930064]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 5094401881.233604, 'mae': 4715922935.838725, 'val_losses': [3.5218634155930064]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 68048390911635.96, 'mae': 62983977975033.734, 'val_losses': [20.05964635630123]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 68048390911635.96, 'mae': 62983977975033.734, 'val_losses': [20.05964635630123]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 68050358874801.05, 'mae': 62988122140317.22, 'val_losses': [34.81329702158443]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 68050358874801.05, 'mae': 62988122140317.22, 'val_losses': [34.81329702158443]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 3540.1624450264994, 'mae': 547.3590819533518, 'val_losses': [22.02240828217053]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 3540.1624450264994, 'mae': 547.3590819533518, 'val_losses': [22.02240828217053]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 1136.008250665491, 'mae': 90.25081792256853, 'val_losses': [31.782062170935458]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 1136.008250665491, 'mae': 90.25081792256853, 'val_losses': [31.782062170935458]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 15.072409210499876, 'mae': 5.049950293037167, 'val_losses': [7.662225492665025]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 15.072409210499876, 'mae': 5.049950293037167, 'val_losses': [7.662225492665025]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 15.072409210499876, 'mae': 5.049950293037167, 'val_losses': [8.797287497364106]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 15.072409210499876, 'mae': 5.049950293037167, 'val_losses': [8.797287497364106]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 15.072409210361094, 'mae': 5.049950292689948, 'val_losses': [11.304258964100821]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 15.072409210361094, 'mae': 5.049950292689948, 'val_losses': [11.304258964100821]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 15.16535042892225, 'mae': 8.80577879452656, 'val_losses': [13.819583478521128]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 15.16535042892225, 'mae': 8.80577879452656, 'val_losses': [13.819583478521128]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 758937714032.2826, 'mae': 11394333381.96216, 'val_losses': [20.323001830304257]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 758937714032.2826, 'mae': 11394333381.96216, 'val_losses': [20.323001830304257]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 758937714032.2826, 'mae': 11394333381.96216, 'val_losses': [37.44372933809875]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 758937714032.2826, 'mae': 11394333381.96216, 'val_losses': [37.44372933809875]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 758937714032.2826, 'mae': 11394333381.96216, 'val_losses': [75.48981475830078]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 758937714032.2826, 'mae': 11394333381.96216, 'val_losses': [75.48981475830078]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 758937714032.2826, 'mae': 11394333381.96216, 'val_losses': [113.53590393066406]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 758937714032.2826, 'mae': 11394333381.96216, 'val_losses': [113.53590393066406]}, {'h': 25, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 15.069525064534918, 'mae': 5.04602204856172, 'val_losses': [7.575232119100993]}, {'h': 25, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 15.069525064534918, 'mae': 5.04602204856172, 'val_losses': [7.575232119100993]}, {'h': 25, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 88368.84310521049, 'mae': 70935.22564315132, 'val_losses': [11.355183577928386]}, {'h': 25, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 88368.84310521049, 'mae': 70935.22564315132, 'val_losses': [11.355183577928386]}, {'h': 25, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 97098.5846173218, 'mae': 62588.42390645988, 'val_losses': [7.688464757848958]}, {'h': 25, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 97098.5846173218, 'mae': 62588.42390645988, 'val_losses': [7.688464757848958]}, {'h': 25, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 15.338348445624137, 'mae': 9.197971578836926, 'val_losses': [7.614977875938181]}, {'h': 25, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 15.338348445624137, 'mae': 9.197971578836926, 'val_losses': [7.614977875938181]}, {'h': 25, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 93489.7449154685, 'mae': 77987.97182342631, 'val_losses': [0.37197359443688]}, {'h': 25, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 93489.7449154685, 'mae': 77987.97182342631, 'val_losses': [0.37197359443688]}, {'h': 25, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 85923.25868007803, 'mae': 67189.61689563826, 'val_losses': [1.1456504828128657]}, {'h': 25, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 85923.25868007803, 'mae': 67189.61689563826, 'val_losses': [1.1456504828128657]}, {'h': 25, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 26328.242675608035, 'mae': 24369.41130055231, 'val_losses': [0.2419198281818726]}, {'h': 25, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 26328.242675608035, 'mae': 24369.41130055231, 'val_losses': [0.2419198281818726]}, {'h': 25, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 33054.4169676163, 'mae': 30595.331828682138, 'val_losses': [3.231472291907326]}, {'h': 25, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 33054.4169676163, 'mae': 30595.331828682138, 'val_losses': [3.231472291907326]}, {'h': 25, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 15.069525064534918, 'mae': 5.04602204856172, 'val_losses': [7.533015938078771]}, {'h': 25, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 15.069525064534918, 'mae': 5.04602204856172, 'val_losses': [7.533015938078771]}, {'h': 25, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 88032.39550327612, 'mae': 69770.04927753503, 'val_losses': [1.2102843161489143]}, {'h': 25, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 88032.39550327612, 'mae': 69770.04927753503, 'val_losses': [1.2102843161489143]}, {'h': 25, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 419646328.1902674, 'mae': 388065288.7227999, 'val_losses': [7.9431502511266805]}, {'h': 25, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 419646328.1902674, 'mae': 388065288.7227999, 'val_losses': [7.9431502511266805]}, {'h': 25, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 4564.864210811716, 'mae': 872.8616410207617, 'val_losses': [3.6614142949463893]}, {'h': 25, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 4564.864210811716, 'mae': 872.8616410207617, 'val_losses': [3.6614142949463893]}, {'h': 25, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 597009876632.4692, 'mae': 5106322212.47607, 'val_losses': [0.5314532745568479]}, {'h': 25, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 597009876632.4692, 'mae': 5106322212.47607, 'val_losses': [0.5314532745568479]}, {'h': 25, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 584721410149.7483, 'mae': 4682632998.67578, 'val_losses': [1.2584866993739956]}, {'h': 25, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 584721410149.7483, 'mae': 4682632998.67578, 'val_losses': [1.2584866993739956]}, {'h': 25, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 38431.838738376384, 'mae': 19282.120436883262, 'val_losses': [2.819055058917061]}, {'h': 25, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 38431.838738376384, 'mae': 19282.120436883262, 'val_losses': [2.819055058917061]}, {'h': 25, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 1852.741043456241, 'mae': 338.24235596346784, 'val_losses': [5.339966039188573]}, {'h': 25, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 1852.741043456241, 'mae': 338.24235596346784, 'val_losses': [5.339966039188573]}, {'h': 5, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 86678.66909030119, 'mae': 72642.51067465774, 'val_losses': [0.993814944243822]}, {'h': 5, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 86678.66909030119, 'mae': 72642.51067465774, 'val_losses': [0.993814944243822]}, {'h': 5, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 15.071926471322984, 'mae': 5.04965797915121, 'val_losses': [7.842506098942678]}, {'h': 5, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 15.071926471322984, 'mae': 5.04965797915121, 'val_losses': [7.842506098942678]}, {'h': 5, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 68060071383567.984, 'mae': 63004013314866.445, 'val_losses': [4.830797998631587]}, {'h': 5, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 68060071383567.984, 'mae': 63004013314866.445, 'val_losses': [4.830797998631587]}, {'h': 5, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 7231.791079983321, 'mae': 863.3475186987366, 'val_losses': [16.718727533934548]}, {'h': 5, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 7231.791079983321, 'mae': 863.3475186987366, 'val_losses': [16.718727533934548]}, {'h': 5, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 58872.36702063993, 'mae': 42122.46839966859, 'val_losses': [4.134205108783284]}, {'h': 5, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 58872.36702063993, 'mae': 42122.46839966859, 'val_losses': [4.134205108783284]}, {'h': 5, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 13651.30827012896, 'mae': 4452.090146278641, 'val_losses': [1.9620317435655437]}, {'h': 5, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 13651.30827012896, 'mae': 4452.090146278641, 'val_losses': [1.9620317435655437]}, {'h': 5, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 6245.628689812526, 'mae': 1029.1128747001583, 'val_losses': [5.362102897440801]}, {'h': 5, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 6245.628689812526, 'mae': 1029.1128747001583, 'val_losses': [5.362102897440801]}, {'h': 5, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 3629.9620911130555, 'mae': 3361.4128152725934, 'val_losses': [11.100628071143978]}, {'h': 5, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 3629.9620911130555, 'mae': 3361.4128152725934, 'val_losses': [11.100628071143978]}, {'h': 5, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 83874.46837787918, 'mae': 71722.74193745751, 'val_losses': [1.1854483167656134]}, {'h': 5, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 83874.46837787918, 'mae': 71722.74193745751, 'val_losses': [1.1854483167656134]}, {'h': 5, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 15.071926474808654, 'mae': 5.049657995124636, 'val_losses': [9.117346386440465]}, {'h': 5, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 15.071926474808654, 'mae': 5.049657995124636, 'val_losses': [9.117346386440465]}, {'h': 5, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 8063.226015465303, 'mae': 1328.5292541435501, 'val_losses': [9.408976949629237]}, {'h': 5, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 8063.226015465303, 'mae': 1328.5292541435501, 'val_losses': [9.408976949629237]}, {'h': 5, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 7739.192814576912, 'mae': 920.0775076568631, 'val_losses': [10.619846594138224]}, {'h': 5, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 7739.192814576912, 'mae': 920.0775076568631, 'val_losses': [10.619846594138224]}, {'h': 5, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 68055615716025.3, 'mae': 62993281705812.36, 'val_losses': [20.41301320810787]}, {'h': 5, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 68055615716025.3, 'mae': 62993281705812.36, 'val_losses': [20.41301320810787]}, {'h': 5, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 68055615716025.3, 'mae': 62993281705812.36, 'val_losses': [38.35010691158107]}, {'h': 5, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 68055615716025.3, 'mae': 62993281705812.36, 'val_losses': [38.35010691158107]}, {'h': 5, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 68055615716025.3, 'mae': 62993281705812.36, 'val_losses': [78.21040119108606]}, {'h': 5, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 68055615716025.3, 'mae': 62993281705812.36, 'val_losses': [78.21040119108606]}, {'h': 5, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 68055615716025.3, 'mae': 62993281705812.36, 'val_losses': [118.07069171842981]}, {'h': 5, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 68055615716025.3, 'mae': 62993281705812.36, 'val_losses': [118.07069171842981]}]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing your saved results\n",
    "input_dir = 'validation_results_final_attempt'\n",
    "\n",
    "# Prepare a list to collect all results\n",
    "all_results = []\n",
    "existing_results = []\n",
    "\n",
    "# Loop over all files in the directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        \n",
    "        # Open and load the JSON file\n",
    "        with open(filepath, 'r') as f:\n",
    "            result = json.load(f)\n",
    "        \n",
    "        # Parse hyperparameters from the filename\n",
    "        name_parts = filename.replace('.json', '').split('_')\n",
    "        h = int(name_parts[0][1:])  # strip the 'h'\n",
    "        batch_size = int(name_parts[1][5:])  # strip 'batch'\n",
    "        lr = float(name_parts[2][2:].replace('e', 'e'))  # scientific notation stays\n",
    "        l2_lambda = float(name_parts[3][6:])  # strip 'lambda'\n",
    "        stop_cr = name_parts[4]  # strip 'criteria'\n",
    "\n",
    "        existing_results.append([h,batch_size,lr,l2_lambda,stop_cr])\n",
    "        \n",
    "        # Combine hyperparameters and results\n",
    "        entry = {\n",
    "            'h': h,\n",
    "            'batch_size': batch_size,\n",
    "            'lr': lr,\n",
    "            'l2_lambda': l2_lambda,\n",
    "            'stop_cr':stop_cr,\n",
    "            **result  # unpack the RMSE, MAE, etc.\n",
    "        }\n",
    "        all_results.append(entry)\n",
    "\n",
    "# Now `all_results` is a list of dictionaries\n",
    "print(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3faad820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>l2_lambda</th>\n",
       "      <th>stop_cr</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>val_losses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>15.069407</td>\n",
       "      <td>5.043644</td>\n",
       "      <td>[7.617420178944947]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>15.069407</td>\n",
       "      <td>5.043644</td>\n",
       "      <td>[7.617420178944947]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>15.069407</td>\n",
       "      <td>5.043644</td>\n",
       "      <td>[18.64002033921539]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>15.069407</td>\n",
       "      <td>5.043644</td>\n",
       "      <td>[18.64002033921539]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>15.069407</td>\n",
       "      <td>5.043644</td>\n",
       "      <td>[7.836691892049352]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>100</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>15.069407</td>\n",
       "      <td>5.043644</td>\n",
       "      <td>[7.836691892049352]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>25</td>\n",
       "      <td>128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>15.069525</td>\n",
       "      <td>5.046022</td>\n",
       "      <td>[7.575232119100993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>25</td>\n",
       "      <td>128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>15.069525</td>\n",
       "      <td>5.046022</td>\n",
       "      <td>[7.575232119100993]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>25</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>15.069525</td>\n",
       "      <td>5.046022</td>\n",
       "      <td>[7.533015938078771]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>25</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>15.069525</td>\n",
       "      <td>5.046022</td>\n",
       "      <td>[7.533015938078771]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>15.071926</td>\n",
       "      <td>5.049658</td>\n",
       "      <td>[7.842506098942678]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>15.071926</td>\n",
       "      <td>5.049658</td>\n",
       "      <td>[7.842506098942678]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>15.071926</td>\n",
       "      <td>5.049658</td>\n",
       "      <td>[9.117346386440465]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>15.071926</td>\n",
       "      <td>5.049658</td>\n",
       "      <td>[9.117346386440465]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.0</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>15.072409</td>\n",
       "      <td>5.049950</td>\n",
       "      <td>[11.304258964100821]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>3.0</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>15.072409</td>\n",
       "      <td>5.049950</td>\n",
       "      <td>[11.304258964100821]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>15.072409</td>\n",
       "      <td>5.049950</td>\n",
       "      <td>[8.797287497364106]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>15.072409</td>\n",
       "      <td>5.049950</td>\n",
       "      <td>[8.797287497364106]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>15.072409</td>\n",
       "      <td>5.049950</td>\n",
       "      <td>[7.662225492665025]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.1</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>15.072409</td>\n",
       "      <td>5.049950</td>\n",
       "      <td>[7.662225492665025]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       h  batch_size     lr  l2_lambda     stop_cr       rmse       mae  \\\n",
       "3    100         128  0.001        1.0        RMSE  15.069407  5.043644   \n",
       "2    100         128  0.001        1.0  likelihood  15.069407  5.043644   \n",
       "17   100         256  0.001        0.1        RMSE  15.069407  5.043644   \n",
       "16   100         256  0.001        0.1  likelihood  15.069407  5.043644   \n",
       "19   100         256  0.001        1.0        RMSE  15.069407  5.043644   \n",
       "18   100         256  0.001        1.0  likelihood  15.069407  5.043644   \n",
       "65    25         128  0.001        0.1        RMSE  15.069525  5.046022   \n",
       "64    25         128  0.001        0.1  likelihood  15.069525  5.046022   \n",
       "80    25         256  0.001        0.1  likelihood  15.069525  5.046022   \n",
       "81    25         256  0.001        0.1        RMSE  15.069525  5.046022   \n",
       "99     5         128  0.001        1.0        RMSE  15.071926  5.049658   \n",
       "98     5         128  0.001        1.0  likelihood  15.071926  5.049658   \n",
       "114    5         256  0.001        1.0  likelihood  15.071926  5.049658   \n",
       "115    5         256  0.001        1.0        RMSE  15.071926  5.049658   \n",
       "53    10         256  0.001        3.0        RMSE  15.072409  5.049950   \n",
       "52    10         256  0.001        3.0  likelihood  15.072409  5.049950   \n",
       "51    10         256  0.001        1.0        RMSE  15.072409  5.049950   \n",
       "50    10         256  0.001        1.0  likelihood  15.072409  5.049950   \n",
       "48    10         256  0.001        0.1  likelihood  15.072409  5.049950   \n",
       "49    10         256  0.001        0.1        RMSE  15.072409  5.049950   \n",
       "\n",
       "               val_losses  \n",
       "3     [7.617420178944947]  \n",
       "2     [7.617420178944947]  \n",
       "17    [18.64002033921539]  \n",
       "16    [18.64002033921539]  \n",
       "19    [7.836691892049352]  \n",
       "18    [7.836691892049352]  \n",
       "65    [7.575232119100993]  \n",
       "64    [7.575232119100993]  \n",
       "80    [7.533015938078771]  \n",
       "81    [7.533015938078771]  \n",
       "99    [7.842506098942678]  \n",
       "98    [7.842506098942678]  \n",
       "114   [9.117346386440465]  \n",
       "115   [9.117346386440465]  \n",
       "53   [11.304258964100821]  \n",
       "52   [11.304258964100821]  \n",
       "51    [8.797287497364106]  \n",
       "50    [8.797287497364106]  \n",
       "48    [7.662225492665025]  \n",
       "49    [7.662225492665025]  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_results).sort_values(\"rmse\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "25309f04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 6228.705229856462, 'mae': 4057.033469512603, 'val_losses': [2.7177600600436085]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 6157.428270675481, 'mae': 4073.441802385299, 'val_losses': [2.720077675507694]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 14.525172299878667, 'mae': 4.854197910140237, 'val_losses': [4.236113470108783]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 14.524382497531139, 'mae': 4.854869315897113, 'val_losses': [5.577551650219276]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 14.575473923032664, 'mae': 4.818254072418233, 'val_losses': [4.44591977752623]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 14.572619262439595, 'mae': 4.8200079892921215, 'val_losses': [8.32257206713567]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 14.586259129535254, 'mae': 4.811870480613147, 'val_losses': [4.495032580172429]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 14.585066184903685, 'mae': 4.8125562221022715, 'val_losses': [4.7247890585758645]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 6484.961044111233, 'mae': 4320.611801463361, 'val_losses': [2.726440468406091]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 6166.615226192292, 'mae': 3962.4221549268386, 'val_losses': [2.949371301736988]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 14.526335354124134, 'mae': 4.8532354133546205, 'val_losses': [4.2372599038921415]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 14.45612137037627, 'mae': 4.781401812690522, 'val_losses': [7.720754791478642]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 14.574236906940142, 'mae': 4.819011056304426, 'val_losses': [4.44628306295051]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 14.573051479224329, 'mae': 4.811851339747914, 'val_losses': [14.839963576832755]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 14.585179922667262, 'mae': 4.812490617696665, 'val_losses': [4.4967949214528815]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 14.585931762503737, 'mae': 4.809383121819099, 'val_losses': [21.80469935057593]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 4874.071947147343, 'mae': 3316.322300124153, 'val_losses': [2.710247133598953]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 5006.597160928745, 'mae': 3333.0490841077367, 'val_losses': [2.7113638874448713]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 14.527665406505145, 'mae': 4.8521458601502365, 'val_losses': [4.235389736832166]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 14.527876992095985, 'mae': 4.851636720591015, 'val_losses': [5.5608681538065925]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 14.574609653642375, 'mae': 4.818782503677424, 'val_losses': [4.443622636013344]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 14.575482330812564, 'mae': 4.818248524094189, 'val_losses': [8.300939927335646]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 14.586293915236613, 'mae': 4.811850542797229, 'val_losses': [4.494587624659304]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 14.584368940291194, 'mae': 4.812959636467264, 'val_losses': [4.521345330066368]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 6432.964219451606, 'mae': 4261.974542397454, 'val_losses': [2.7256689670144536]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 5501.579475586483, 'mae': 3382.830428680327, 'val_losses': [2.941806399431385]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 14.525986708122135, 'mae': 4.85352298810131, 'val_losses': [4.236153727672139]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 14.38473183279145, 'mae': 4.748922781817904, 'val_losses': [8.327800578758366]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 14.57379810309285, 'mae': 4.819280398255307, 'val_losses': [4.445226165114856]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 14.565803332304757, 'mae': 4.802866757339597, 'val_losses': [16.692371556016266]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 14.584740282595709, 'mae': 4.812744569148644, 'val_losses': [4.495750907991753]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 14.585463368352544, 'mae': 4.808060808527884, 'val_losses': [24.90410942327781]}, {'h': 25, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 10374.097347841533, 'mae': 430.2110538345559, 'val_losses': [2.0769510515889182]}, {'h': 25, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 14904.32217315563, 'mae': 605.8175979278528, 'val_losses': [2.2409466999720355]}, {'h': 25, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 14.4650695019372, 'mae': 4.7882054695233975, 'val_losses': [4.236893728131154]}, {'h': 25, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 14.492096546502975, 'mae': 4.807007771349164, 'val_losses': [4.263716308796992]}, {'h': 25, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 14.571800551864264, 'mae': 4.8155410144043485, 'val_losses': [4.445216790574496]}, {'h': 25, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 14.57095056684082, 'mae': 4.816065266906797, 'val_losses': [7.037950527472574]}, {'h': 25, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 14.582584066951226, 'mae': 4.809093021742472, 'val_losses': [4.4962329317311776]}, {'h': 25, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 14.582058390836558, 'mae': 4.809398006547919, 'val_losses': [8.794847504037325]}, {'h': 25, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 2078.7125569658524, 'mae': 1027.4981412134136, 'val_losses': [2.1441121144128625]}, {'h': 25, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 2929.952768856239, 'mae': 1081.9276916853046, 'val_losses': [4.633889595016104]}, {'h': 25, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 14.48013018029614, 'mae': 4.79790993941391, 'val_losses': [4.238893856767748]}, {'h': 25, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 14.448909917474444, 'mae': 4.769400968451381, 'val_losses': [4.491244148035518]}, {'h': 25, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 14.571444952292575, 'mae': 4.815759902949029, 'val_losses': [4.446572977988446]}, {'h': 25, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 14.567814491530045, 'mae': 4.804773272107817, 'val_losses': [9.557971907443688]}, {'h': 25, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 14.582562819933349, 'mae': 4.809105322394051, 'val_losses': [4.497327611094615]}, {'h': 25, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 14.581032652517495, 'mae': 4.804001079789478, 'val_losses': [13.003638759988252]}, {'h': 25, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 1415.160108166369, 'mae': 510.54574718179543, 'val_losses': [2.073997501467095]}, {'h': 25, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 3620.938821218824, 'mae': 1877.2335398414493, 'val_losses': [2.5126408323889873]}, {'h': 25, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 14.472256103029244, 'mae': 4.794486756915385, 'val_losses': [4.23736521064258]}, {'h': 25, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 14.476185517096653, 'mae': 4.793326011093763, 'val_losses': [4.398288554832583]}, {'h': 25, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 14.570711109125254, 'mae': 4.816213500720986, 'val_losses': [4.444991698030566]}, {'h': 25, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 14.572317822459055, 'mae': 4.815222209690556, 'val_losses': [7.0268953276462245]}, {'h': 25, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 14.58188162072255, 'mae': 4.809500772196075, 'val_losses': [4.4956519369219174]}, {'h': 25, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 14.582658293076273, 'mae': 4.809049725321354, 'val_losses': [8.777711508704014]}, {'h': 25, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 2066.122799335037, 'mae': 1003.3653869619706, 'val_losses': [2.2090306997787756]}, {'h': 25, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 3015.817273059075, 'mae': 889.0485228314712, 'val_losses': [4.384350233390683]}, {'h': 25, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 14.47750587283002, 'mae': 4.794963031873038, 'val_losses': [4.238338783139088]}, {'h': 25, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 14.384539403772527, 'mae': 4.7210582666033645, 'val_losses': [4.659853231711466]}, {'h': 25, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 14.571676627192714, 'mae': 4.8156172837495985, 'val_losses': [4.445979915681432]}, {'h': 25, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 7521.89305244821, 'mae': 65.08794852278928, 'val_losses': [43.93827876106637]}, {'h': 25, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 14.582676555753498, 'mae': 4.80903948922477, 'val_losses': [4.496560878440982]}, {'h': 25, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 14.580114674893911, 'mae': 4.7942841557878415, 'val_losses': [14.801800196288061]}, {'h': 5, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 5613.599125774626, 'mae': 3431.4658412017493, 'val_losses': [2.7671545918603413]}, {'h': 5, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 6053.788966828211, 'mae': 3923.3525269824163, 'val_losses': [2.7984880983340936]}, {'h': 5, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 14.525106797679905, 'mae': 4.853655459386882, 'val_losses': [4.2358082845562794]}, {'h': 5, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 14.52365668145943, 'mae': 4.854882286827656, 'val_losses': [6.644109593063105]}, {'h': 5, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 14.573625565435169, 'mae': 4.818874917655421, 'val_losses': [4.445815785986478]}, {'h': 5, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 14.571722826788802, 'mae': 4.8200500517710125, 'val_losses': [11.557613794920874]}, {'h': 5, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 14.583721742353008, 'mae': 4.812836592921657, 'val_losses': [4.495883871297368]}, {'h': 5, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 14.582720505957498, 'mae': 4.813419355346479, 'val_losses': [16.321759294291013]}, {'h': 5, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 4235.15248699478, 'mae': 2578.350447038551, 'val_losses': [2.7824908746559114]}, {'h': 5, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 3624.904614170633, 'mae': 2082.695742763973, 'val_losses': [3.4394935100782114]}, {'h': 5, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 14.525791074021399, 'mae': 4.853089488187896, 'val_losses': [4.237045102432126]}, {'h': 5, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 14.512135102488534, 'mae': 4.837811480045719, 'val_losses': [7.274908546541558]}, {'h': 5, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 14.57378912104991, 'mae': 4.818774544061739, 'val_losses': [4.445717219446526]}, {'h': 5, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 14.5721371128834, 'mae': 4.818811566832305, 'val_losses': [13.463940995638488]}, {'h': 5, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 14.58483032957929, 'mae': 4.81219498985088, 'val_losses': [4.496218972518796]}, {'h': 5, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 14.58380459891897, 'mae': 4.81218880676386, 'val_losses': [19.51018513226118]}, {'h': 5, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 5551.574717580649, 'mae': 3387.964143684034, 'val_losses': [2.7674933112547047]}, {'h': 5, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 5055.297842380252, 'mae': 3020.6614392998845, 'val_losses': [3.103829837236248]}, {'h': 5, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 14.52660018733426, 'mae': 4.852424265772733, 'val_losses': [4.2324679562302885]}, {'h': 5, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 14.525072031447214, 'mae': 4.853669470085981, 'val_losses': [6.6284719217019]}, {'h': 5, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 14.573179490504165, 'mae': 4.819149165078496, 'val_losses': [4.44347282315864]}, {'h': 5, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 14.574171081627577, 'mae': 4.818541801439845, 'val_losses': [11.567236603283492]}, {'h': 5, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 14.58408862819879, 'mae': 4.81262374853257, 'val_losses': [4.494054387827388]}, {'h': 5, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 14.585313244540318, 'mae': 4.811918148724039, 'val_losses': [16.275580671967052]}, {'h': 5, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'likelihood', 'rmse': 4273.075019819375, 'mae': 2633.7172535364216, 'val_losses': [2.7830494871882143]}, {'h': 5, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 0.1, 'stop_cr': 'RMSE', 'rmse': 660434.5281976676, 'mae': 9429.965346544852, 'val_losses': [6.300127474988093]}, {'h': 5, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'likelihood', 'rmse': 14.525481019917668, 'mae': 4.853360595219443, 'val_losses': [4.273355355028246]}, {'h': 5, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 1.0, 'stop_cr': 'RMSE', 'rmse': 14.495109591988962, 'mae': 4.821017802306405, 'val_losses': [7.252524399366535]}, {'h': 5, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'likelihood', 'rmse': 14.573440246322173, 'mae': 4.819024550823999, 'val_losses': [4.557407171999822]}, {'h': 5, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 3.0, 'stop_cr': 'RMSE', 'rmse': 14.571769670050658, 'mae': 4.8165872394812, 'val_losses': [13.43692391817687]}, {'h': 5, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'likelihood', 'rmse': 14.584478428254775, 'mae': 4.812420758704011, 'val_losses': [4.683479617853633]}, {'h': 5, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 5.0, 'stop_cr': 'RMSE', 'rmse': 14.584257944441589, 'mae': 4.811292278768922, 'val_losses': [19.469285464677654]}]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing your saved results\n",
    "input_dir = 'validation_results_final_attempt_normalized_direct'\n",
    "\n",
    "# Prepare a list to collect all results\n",
    "all_results = []\n",
    "existing_results = []\n",
    "\n",
    "# Loop over all files in the directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        \n",
    "        # Open and load the JSON file\n",
    "        with open(filepath, 'r') as f:\n",
    "            result = json.load(f)\n",
    "        \n",
    "        # Parse hyperparameters from the filename\n",
    "        name_parts = filename.replace('.json', '').split('_')\n",
    "        h = int(name_parts[0][1:])  # strip the 'h'\n",
    "        batch_size = int(name_parts[1][5:])  # strip 'batch'\n",
    "        lr = float(name_parts[2][2:].replace('e', 'e'))  # scientific notation stays\n",
    "        l2_lambda = float(name_parts[3][6:])  # strip 'lambda'\n",
    "        stop_cr = name_parts[4]  # strip 'criteria'\n",
    "\n",
    "        existing_results.append([h,batch_size,lr,l2_lambda,stop_cr])\n",
    "        \n",
    "        # Combine hyperparameters and results\n",
    "        entry = {\n",
    "            'h': h,\n",
    "            'batch_size': batch_size,\n",
    "            'lr': lr,\n",
    "            'l2_lambda': l2_lambda,\n",
    "            'stop_cr':stop_cr,\n",
    "            **result  # unpack the RMSE, MAE, etc.\n",
    "        }\n",
    "        all_results.append(entry)\n",
    "\n",
    "# Now `all_results` is a list of dictionaries\n",
    "print(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "94c8c7a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>l2_lambda</th>\n",
       "      <th>stop_cr</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>val_losses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>25</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>14.384539</td>\n",
       "      <td>4.721058</td>\n",
       "      <td>[4.659853231711466]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>14.384732</td>\n",
       "      <td>4.748923</td>\n",
       "      <td>[8.327800578758366]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>25</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>14.448910</td>\n",
       "      <td>4.769401</td>\n",
       "      <td>[4.491244148035518]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>14.456121</td>\n",
       "      <td>4.781402</td>\n",
       "      <td>[7.720754791478642]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>25</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>14.465070</td>\n",
       "      <td>4.788205</td>\n",
       "      <td>[4.236893728131154]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>25</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>14.472256</td>\n",
       "      <td>4.794487</td>\n",
       "      <td>[4.23736521064258]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>25</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>14.476186</td>\n",
       "      <td>4.793326</td>\n",
       "      <td>[4.398288554832583]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>25</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>14.477506</td>\n",
       "      <td>4.794963</td>\n",
       "      <td>[4.238338783139088]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>25</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>14.480130</td>\n",
       "      <td>4.797910</td>\n",
       "      <td>[4.238893856767748]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>25</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>14.492097</td>\n",
       "      <td>4.807008</td>\n",
       "      <td>[4.263716308796992]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>14.495110</td>\n",
       "      <td>4.821018</td>\n",
       "      <td>[7.252524399366535]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>14.512135</td>\n",
       "      <td>4.837811</td>\n",
       "      <td>[7.274908546541558]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>14.523657</td>\n",
       "      <td>4.854882</td>\n",
       "      <td>[6.644109593063105]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>14.524382</td>\n",
       "      <td>4.854869</td>\n",
       "      <td>[5.577551650219276]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>RMSE</td>\n",
       "      <td>14.525072</td>\n",
       "      <td>4.853669</td>\n",
       "      <td>[6.6284719217019]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>14.525107</td>\n",
       "      <td>4.853655</td>\n",
       "      <td>[4.2358082845562794]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>14.525172</td>\n",
       "      <td>4.854198</td>\n",
       "      <td>[4.236113470108783]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>5</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>14.525481</td>\n",
       "      <td>4.853361</td>\n",
       "      <td>[4.273355355028246]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>5</td>\n",
       "      <td>128</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>14.525791</td>\n",
       "      <td>4.853089</td>\n",
       "      <td>[4.237045102432126]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>14.525987</td>\n",
       "      <td>4.853523</td>\n",
       "      <td>[4.236153727672139]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     h  batch_size      lr  l2_lambda     stop_cr       rmse       mae  \\\n",
       "59  25         256  0.0001        1.0        RMSE  14.384539  4.721058   \n",
       "27  10         256  0.0001        1.0        RMSE  14.384732  4.748923   \n",
       "43  25         128  0.0001        1.0        RMSE  14.448910  4.769401   \n",
       "11  10         128  0.0001        1.0        RMSE  14.456121  4.781402   \n",
       "34  25         128  0.0010        1.0  likelihood  14.465070  4.788205   \n",
       "50  25         256  0.0010        1.0  likelihood  14.472256  4.794487   \n",
       "51  25         256  0.0010        1.0        RMSE  14.476186  4.793326   \n",
       "58  25         256  0.0001        1.0  likelihood  14.477506  4.794963   \n",
       "42  25         128  0.0001        1.0  likelihood  14.480130  4.797910   \n",
       "35  25         128  0.0010        1.0        RMSE  14.492097  4.807008   \n",
       "91   5         256  0.0001        1.0        RMSE  14.495110  4.821018   \n",
       "75   5         128  0.0001        1.0        RMSE  14.512135  4.837811   \n",
       "67   5         128  0.0010        1.0        RMSE  14.523657  4.854882   \n",
       "3   10         128  0.0010        1.0        RMSE  14.524382  4.854869   \n",
       "83   5         256  0.0010        1.0        RMSE  14.525072  4.853669   \n",
       "66   5         128  0.0010        1.0  likelihood  14.525107  4.853655   \n",
       "2   10         128  0.0010        1.0  likelihood  14.525172  4.854198   \n",
       "90   5         256  0.0001        1.0  likelihood  14.525481  4.853361   \n",
       "74   5         128  0.0001        1.0  likelihood  14.525791  4.853089   \n",
       "26  10         256  0.0001        1.0  likelihood  14.525987  4.853523   \n",
       "\n",
       "              val_losses  \n",
       "59   [4.659853231711466]  \n",
       "27   [8.327800578758366]  \n",
       "43   [4.491244148035518]  \n",
       "11   [7.720754791478642]  \n",
       "34   [4.236893728131154]  \n",
       "50    [4.23736521064258]  \n",
       "51   [4.398288554832583]  \n",
       "58   [4.238338783139088]  \n",
       "42   [4.238893856767748]  \n",
       "35   [4.263716308796992]  \n",
       "91   [7.252524399366535]  \n",
       "75   [7.274908546541558]  \n",
       "67   [6.644109593063105]  \n",
       "3    [5.577551650219276]  \n",
       "83     [6.6284719217019]  \n",
       "66  [4.2358082845562794]  \n",
       "2    [4.236113470108783]  \n",
       "90   [4.273355355028246]  \n",
       "74   [4.237045102432126]  \n",
       "26   [4.236153727672139]  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_results).sort_values(\"rmse\").head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69f1470",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fff78adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 100, batch_size:256, lr: 0.001, l2_lambda:1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/2\n",
      "Epoch 1.      Train Loss: 4.3489, Val Loss: 5.0285.      Train RMSE: 21907429.8663, Val RMSE: 246.1650\n",
      "Epoch 5.      Train Loss: 1.9335, Val Loss: 1.6357.      Train RMSE: 3997.7097, Val RMSE: 10040.2470\n",
      "Epoch 10.      Train Loss: 1.9281, Val Loss: 1.6358.      Train RMSE: 3649.8656, Val RMSE: 9563.5353\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/2\n",
      "Epoch 1.      Train Loss: 2.9476, Val Loss: 2.6765.      Train RMSE: 1584.7315, Val RMSE: 3730.1980\n",
      "Epoch 5.      Train Loss: 1.9287, Val Loss: 1.6363.      Train RMSE: 3574.3518, Val RMSE: 9467.1618\n",
      "Epoch 10.      Train Loss: 1.9307, Val Loss: 1.6251.      Train RMSE: 4028.7133, Val RMSE: 10159.8720\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "h, batch_size, lr, l2_lambda, stop_cr = 100, 256, 1e-3, 1, 'RMSE'\n",
    "\n",
    "d1=source1_tensor.shape[1]  # number of features of the 1st source\n",
    "d2=source2_tensor.shape[1]  # number of features of the 2nd source\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = create_datasets(source1_array, source2_array, target_array, weight_array, batch_size, h)\n",
    "\n",
    "print(f\"\\n📊 h: {h}, batch_size:{batch_size}, lr: {lr}, l2_lambda:{l2_lambda}, {stop_cr}\")\n",
    "\n",
    "ensemble, losses = train_tme_ensemble(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    d1=d1,  # number of features of the 1st source\n",
    "    d2=d2,  # number of features of the 2nd source\n",
    "    h=h,  # lag length\n",
    "    num_models=2,#20,\n",
    "    device=device,\n",
    "    lr=lr,\n",
    "    weight_decay=0.1,\n",
    "    l2_lambda=l2_lambda,\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    adam=True,\n",
    "    direct_target=False,\n",
    "    stop_criteria=stop_cr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b64d4967",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, test_loader, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4ddf4eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10686.299290363746 3382.2510533179084\n"
     ]
    }
   ],
   "source": [
    "print(rmse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "faeb3632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 25, batch_size:128, lr: 0.0001, l2_lambda:1, RMSE\n",
      "\n",
      "🌱 Training ensemble model 1/10\n",
      "Model 1 initialized with seed: 1581798263\n",
      "Epoch 1.      Train Loss: 23.0315, Val Loss: 23.6696.      Train RMSE: 18890253067.2180, Val RMSE: 61936.8535\n",
      "Epoch 5.      Train Loss: 8.9735, Val Loss: 9.8773.      Train RMSE: 223184317.7682, Val RMSE: 87.5109\n",
      "Epoch 10.      Train Loss: 4.5166, Val Loss: 5.4789.      Train RMSE: 161.1182, Val RMSE: 14.4605\n",
      "Epoch 15.      Train Loss: 3.6072, Val Loss: 4.5265.      Train RMSE: 54.4358, Val RMSE: 14.4867\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/10\n",
      "Model 2 initialized with seed: 805311477\n",
      "Epoch 1.      Train Loss: 18.6276, Val Loss: 17.3453.      Train RMSE: 898420420.0992, Val RMSE: 41058.7450\n",
      "Epoch 5.      Train Loss: 7.4518, Val Loss: 8.1189.      Train RMSE: 33786.8708, Val RMSE: 14.5328\n",
      "Epoch 10.      Train Loss: 4.2188, Val Loss: 5.1075.      Train RMSE: 51.9776, Val RMSE: 14.1949\n",
      "Epoch 15.      Train Loss: 3.4235, Val Loss: 4.3641.      Train RMSE: 18.4530, Val RMSE: 14.3213\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/10\n",
      "Model 3 initialized with seed: 499550012\n",
      "Epoch 1.      Train Loss: 21.2489, Val Loss: 22.2362.      Train RMSE: 32482016034.2947, Val RMSE: 17312528.8510\n",
      "Epoch 5.      Train Loss: 6.9399, Val Loss: 7.8952.      Train RMSE: 21581421.7036, Val RMSE: 228.4569\n",
      "Epoch 10.      Train Loss: 3.6068, Val Loss: 4.5514.      Train RMSE: 45.6166, Val RMSE: 14.3528\n",
      "Epoch 15.      Train Loss: 3.3077, Val Loss: 4.2538.      Train RMSE: 16.6529, Val RMSE: 14.4217\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 4/10\n",
      "Model 4 initialized with seed: 2061950113\n",
      "Epoch 1.      Train Loss: 19.5392, Val Loss: 20.9883.      Train RMSE: 45691317824.7598, Val RMSE: 388274.2810\n",
      "Epoch 5.      Train Loss: 7.8841, Val Loss: 8.7471.      Train RMSE: 495016292.5763, Val RMSE: 2920.6263\n",
      "Epoch 10.      Train Loss: 4.2965, Val Loss: 5.1702.      Train RMSE: 6977.5761, Val RMSE: 17.2672\n",
      "Epoch 15.      Train Loss: 3.4433, Val Loss: 4.3695.      Train RMSE: 19.5323, Val RMSE: 14.3060\n",
      "Epoch 20.      Train Loss: 3.2996, Val Loss: 4.2434.      Train RMSE: 16.7559, Val RMSE: 14.4669\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 5/10\n",
      "Model 5 initialized with seed: 1124417233\n",
      "Epoch 1.      Train Loss: 21.1176, Val Loss: 21.2264.      Train RMSE: 30853301364.3763, Val RMSE: 120812.9180\n",
      "Epoch 5.      Train Loss: 7.4676, Val Loss: 8.3759.      Train RMSE: 1245003.1448, Val RMSE: 1042.4794\n",
      "Epoch 10.      Train Loss: 3.8131, Val Loss: 4.7535.      Train RMSE: 19.0049, Val RMSE: 14.3073\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 6/10\n",
      "Model 6 initialized with seed: 1279896647\n",
      "Epoch 1.      Train Loss: 18.8360, Val Loss: 19.6145.      Train RMSE: 57420878997.0336, Val RMSE: 7788933200.8249\n",
      "Epoch 5.      Train Loss: 6.7365, Val Loss: 7.5889.      Train RMSE: 2549599.0953, Val RMSE: 940.5544\n",
      "Epoch 10.      Train Loss: 3.9647, Val Loss: 4.9084.      Train RMSE: 59.8085, Val RMSE: 14.2099\n",
      "Epoch 15.      Train Loss: 3.4410, Val Loss: 4.3859.      Train RMSE: 16.7086, Val RMSE: 14.4457\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 7/10\n",
      "Model 7 initialized with seed: 1523395765\n",
      "Epoch 1.      Train Loss: 18.4827, Val Loss: 19.2690.      Train RMSE: 113564934208.1345, Val RMSE: 67373982365.0085\n",
      "Epoch 5.      Train Loss: 5.9964, Val Loss: 6.8535.      Train RMSE: 208447249.1863, Val RMSE: 445.1123\n",
      "Epoch 10.      Train Loss: 3.4555, Val Loss: 4.4022.      Train RMSE: 215.3880, Val RMSE: 14.1114\n",
      "Epoch 15.      Train Loss: 3.2980, Val Loss: 4.2431.      Train RMSE: 16.7053, Val RMSE: 14.4451\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 8/10\n",
      "Model 8 initialized with seed: 1960940757\n",
      "Epoch 1.      Train Loss: 17.8158, Val Loss: 18.4285.      Train RMSE: 63884347324.4361, Val RMSE: 6428.7066\n",
      "Epoch 5.      Train Loss: 5.6842, Val Loss: 6.4707.      Train RMSE: 239.1917, Val RMSE: 183.1378\n",
      "Epoch 10.      Train Loss: 3.3827, Val Loss: 4.3411.      Train RMSE: 16.8542, Val RMSE: 14.5166\n",
      "Epoch 15.      Train Loss: 3.2976, Val Loss: 4.2379.      Train RMSE: 16.8628, Val RMSE: 14.5236\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 9/10\n",
      "Model 9 initialized with seed: 1771317417\n",
      "Epoch 1.      Train Loss: 16.0533, Val Loss: 17.0019.      Train RMSE: 57688949405.7468, Val RMSE: 246571.9753\n",
      "Epoch 5.      Train Loss: 5.3274, Val Loss: 6.1880.      Train RMSE: 2060472.4848, Val RMSE: 14.5190\n",
      "Epoch 10.      Train Loss: 3.5014, Val Loss: 4.4112.      Train RMSE: 17.3517, Val RMSE: 14.3289\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 10/10\n",
      "Model 10 initialized with seed: 708439349\n",
      "Epoch 1.      Train Loss: 18.9808, Val Loss: 19.9433.      Train RMSE: 98077974640.5708, Val RMSE: 11336424332.9414\n",
      "Epoch 5.      Train Loss: 6.9202, Val Loss: 7.9164.      Train RMSE: 19636589103.9228, Val RMSE: 145.7219\n",
      "Epoch 10.      Train Loss: 3.6755, Val Loss: 4.6159.      Train RMSE: 585.7237, Val RMSE: 14.2390\n",
      "Epoch 15.      Train Loss: 3.3086, Val Loss: 4.2536.      Train RMSE: 16.7859, Val RMSE: 14.3688\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "h, batch_size, lr, l2_lambda, stop_cr = 25, 128, 1e-4, 1, 'RMSE'\n",
    "\n",
    "d1=source1_tensor.shape[1]  # number of features of the 1st source\n",
    "d2=source2_tensor.shape[1]  # number of features of the 2nd source\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "train_loader, val_loader, test_loader = create_datasets(source1_array, source2_array, target_direct_array, weight_array, batch_size, h)\n",
    "\n",
    "print(f\"\\n📊 h: {h}, batch_size:{batch_size}, lr: {lr}, l2_lambda:{l2_lambda}, {stop_cr}\")\n",
    "\n",
    "ensemble, val_losses, all_train_losses, all_val_losses = train_tme_ensemble(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    d1=d1,  # number of features of the 1st source\n",
    "    d2=d2,  # number of features of the 2nd source\n",
    "    h=h,  # lag length\n",
    "    num_models=10,#20,\n",
    "    device=device,\n",
    "    lr=lr,\n",
    "    weight_decay=0.1,\n",
    "    l2_lambda=l2_lambda,\n",
    "    max_epochs=60,\n",
    "    patience=7,\n",
    "    adam=True,\n",
    "    direct_target=True,\n",
    "    stop_criteria=stop_cr,\n",
    "    output_everything=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2de3eacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = evaluate_tme_ensemble(ensemble, test_loader, device='cpu', direct_target=True, output_everything=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "397678c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31187, 22])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict['y_preds'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e24087a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31187])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "11.130561490457083 3.7821060498975974 tensor(20561.4803, dtype=torch.float64) tensor(8.5047, dtype=torch.float64) tensor(20552.9755, dtype=torch.float64) tensor(37.3111, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "67652524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.13677988340564 3.5934002361890722 tensor(10.0984, dtype=torch.float64) tensor(0.0380, dtype=torch.float64) tensor(10.0604, dtype=torch.float64) tensor(3.1516, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "final_preds = output_dict['y_preds'].mean(dim=1)\n",
    "final_epistemic = output_dict['epistemic_var'].mean(dim=1) - torch.square(final_preds)\n",
    "final_aleatoric = output_dict['aleatoric_var'].mean(dim=1)\n",
    "final_var = final_epistemic + final_aleatoric\n",
    "\n",
    "final_rmse = np.sqrt(mean_squared_error(output_dict['y_trues'], final_preds))\n",
    "final_mae = mean_absolute_error(output_dict['y_trues'], final_preds)\n",
    "print(final_rmse, final_mae, final_var.mean(), final_epistemic.mean(), final_aleatoric.mean(), torch.sqrt(final_var).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3cd3c7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31187, 110])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict['y_preds'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ead573f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.165281954315725 3.659165400568652 tensor(11.1550, dtype=torch.float64) tensor(0.0090, dtype=torch.float64) tensor(11.1460, dtype=torch.float64) tensor(3.3399, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "model1_preds = output_dict['y_preds'][:,3]\n",
    "model1_epistemic = output_dict['epistemic_var'][:,3] - torch.square(model1_preds)\n",
    "model1_aleatoric = output_dict['aleatoric_var'][:,3]\n",
    "model1_var = model1_epistemic + model1_aleatoric\n",
    "\n",
    "model1_rmse = np.sqrt(mean_squared_error(output_dict['y_trues'], model1_preds))\n",
    "model1_mae = mean_absolute_error(output_dict['y_trues'], model1_preds)\n",
    "print(model1_rmse, model1_mae, model1_var.mean(), model1_epistemic.mean(), model1_aleatoric.mean(), torch.sqrt(model1_var).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "ad59da4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.152251599724869 3.622072699398024 tensor(10.4408, dtype=torch.float64) tensor(0.0156, dtype=torch.float64) tensor(10.4252, dtype=torch.float64) tensor(3.2273, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "index_best_model = [i*11 + 3 for i in range(3)]\n",
    "best_model_preds = output_dict['y_preds'][:,index_best_model].mean(dim=1)\n",
    "best_model_epistemic = output_dict['epistemic_var'][:,index_best_model].mean(dim=1) - torch.square(best_model_preds)\n",
    "best_model_aleatoric = output_dict['aleatoric_var'][:,index_best_model].mean(dim=1)\n",
    "best_model_var = best_model_epistemic + best_model_aleatoric\n",
    "\n",
    "best_model_rmse = np.sqrt(mean_squared_error(output_dict['y_trues'], best_model_preds))\n",
    "best_model_mae = mean_absolute_error(output_dict['y_trues'], best_model_preds)\n",
    "print(best_model_rmse, best_model_mae, best_model_var.mean(), best_model_epistemic.mean(), best_model_aleatoric.mean(), torch.sqrt(best_model_var).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "480f8c90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Density'>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(np.array(output_dict['aleatoric_var'][:,index_best_model].mean(dim=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8221a7bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.129820020785179 3.5838455456637273\n"
     ]
    }
   ],
   "source": [
    "print(rmse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "685939fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nomal_mean = 2*torch.log(final_preds) - 0.5*torch.log(final_var + torch.square(final_preds))\n",
    "# normal_var = -2*torch.log(final_preds) + torch.log(final_var + torch.square(final_preds))\n",
    "# normal_025 = nomal_mean - 2*torch.sqrt(normal_var)\n",
    "# normal_975 = nomal_mean + 2*torch.sqrt(normal_var)\n",
    "\n",
    "normal_var = torch.log(final_var/torch.square(final_preds) + 1)\n",
    "nomal_mean = torch.log(final_preds) - 0.5*normal_var\n",
    "normal_025 = nomal_mean - 2*torch.sqrt(normal_var)\n",
    "normal_975 = nomal_mean + 2*torch.sqrt(normal_var)\n",
    "\n",
    "# normal_var = torch.square(torch.log(torch.sqrt(final_var)/final_preds + 1))\n",
    "# nomal_mean = torch.log(torch.square(final_preds)/final_var + torch.square(final_preds))\n",
    "# normal_025 = nomal_mean - 2*torch.sqrt(normal_var)\n",
    "# normal_975 = nomal_mean + 2*torch.sqrt(normal_var)\n",
    "\n",
    "normal_var = torch.log(final_var/torch.square(final_preds) + 1)\n",
    "nomal_mean = torch.log( torch.square(final_preds) / torch.sqrt(final_var + torch.square(final_preds)) )\n",
    "normal_025 = nomal_mean - 2*torch.sqrt(normal_var)\n",
    "normal_975 = nomal_mean + 2*torch.sqrt(normal_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "21ad88fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.2293, 2.2285, 2.2277,  ..., 2.2470, 2.2489, 2.2493],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_975"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "45d82ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9229807291499663)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.where(output_dict['y_trues'] < np.array(torch.exp(normal_975)),1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "89473515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.7152659762080354)"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.where(output_dict['y_trues'] > np.array(torch.exp(normal_025)),1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e5f90cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.63163003e-02, 1.00010005e-03, 4.08722013e-02, ...,\n",
       "       1.52988994e+00, 9.35145140e-01, 9.83765185e-01])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dict['y_trues']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b824ce91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "time = np.arange(len(final_preds))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(time, output_dict['y_trues'], label='True', color='blue', linewidth=0.4, alpha=0.5)\n",
    "plt.plot(time, final_preds, label='Predicted Mean', color='red')\n",
    "plt.fill_between(time, torch.exp(normal_025), torch.exp(normal_975), color='blue', alpha=0.3, label='±2σ CI')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('title')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8b6604",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 3.5))\n",
    "plt.fill_between(range(len(predictions.mean(1))), torch.maximum(torch.tensor(0),predictions.mean(1)-1.96*torch.sqrt(IW.mean(0) - torch.square(predictions.mean(1)))), predictions.mean(1)+1.96*torch.sqrt(IW.mean(0) - torch.square(predictions.mean(1))),\n",
    "                    color=\"#4C72B0\", alpha=0.15, label=\"95 % band\")\n",
    "plt.plot(test_df['total_volume'].iloc[h:].reset_index()['total_volume'], label=\"true\", color=\"#4C72B0\", alpha=.8, linewidth=1.0)\n",
    "plt.plot(predictions.mean(1), label=\"pred\", color=\"#BD561A\", alpha=.9, linewidth=1.2)\n",
    "plt.title(\"Test – total volume (±1.96 σ)\")\n",
    "plt.xlabel(\"sample\");\n",
    "plt.ylabel(\"volume\")\n",
    "plt.legend();\n",
    "plt.tight_layout();\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_fin_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
