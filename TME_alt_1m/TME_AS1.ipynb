{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc18b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy import cumsum, log, polyfit, sqrt, std, subtract\n",
    "from numpy.random import randn\n",
    "import pandas as pd\n",
    "# from pandas_datareader import data as web\n",
    "import seaborn as sns\n",
    "from pylab import rcParams \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from arch import arch_model\n",
    "from numpy.linalg import LinAlgError\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as tsa\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, q_stat, adfuller\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import probplot, moment\n",
    "from arch import arch_model\n",
    "from arch.univariate import ConstantMean, GARCH, Normal\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from itertools import product\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f12cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_txn_data, preprocess_txn_data, compute_lob_features, create_lob_dataset, merge_txn_and_lob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baf1a446",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"darkgrid\", color_codes=True)\n",
    "rcParams['figure.figsize'] = 8,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac6d5cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49ac3768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trx Data loaded successfully.\n",
      "preprocessed lob Data loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>buy_volume</th>\n",
       "      <th>sell_volume</th>\n",
       "      <th>buy_txn</th>\n",
       "      <th>sell_txn</th>\n",
       "      <th>volume_imbalance</th>\n",
       "      <th>txn_imbalance</th>\n",
       "      <th>total_volume</th>\n",
       "      <th>mean_volume</th>\n",
       "      <th>deseasoned_total_volume</th>\n",
       "      <th>log_deseasoned_total_volume</th>\n",
       "      <th>ask_volume</th>\n",
       "      <th>bid_volume</th>\n",
       "      <th>ask_slope_1</th>\n",
       "      <th>ask_slope_5</th>\n",
       "      <th>ask_slope_10</th>\n",
       "      <th>bid_slope_1</th>\n",
       "      <th>bid_slope_5</th>\n",
       "      <th>bid_slope_10</th>\n",
       "      <th>spread</th>\n",
       "      <th>lob_volume_imbalance</th>\n",
       "      <th>slope_imbalance_1</th>\n",
       "      <th>slope_imbalance_5</th>\n",
       "      <th>slope_imbalance_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5819</th>\n",
       "      <td>2018-06-04 22:00:05+00:00</td>\n",
       "      <td>0.059804</td>\n",
       "      <td>0.730357</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.670553</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.790162</td>\n",
       "      <td>4.380444</td>\n",
       "      <td>0.180384</td>\n",
       "      <td>-1.712667</td>\n",
       "      <td>2695.804973</td>\n",
       "      <td>586356.113693</td>\n",
       "      <td>1761.630667</td>\n",
       "      <td>2695.804973</td>\n",
       "      <td>2695.804973</td>\n",
       "      <td>3.972121</td>\n",
       "      <td>53.502450</td>\n",
       "      <td>160.246934</td>\n",
       "      <td>6.19</td>\n",
       "      <td>583660.308720</td>\n",
       "      <td>1757.658546</td>\n",
       "      <td>2642.302523</td>\n",
       "      <td>2535.558040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5820</th>\n",
       "      <td>2018-06-04 22:01:05+00:00</td>\n",
       "      <td>0.089359</td>\n",
       "      <td>0.849477</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.760118</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.938836</td>\n",
       "      <td>3.692009</td>\n",
       "      <td>0.254289</td>\n",
       "      <td>-1.369285</td>\n",
       "      <td>2699.165417</td>\n",
       "      <td>586350.938081</td>\n",
       "      <td>1765.312385</td>\n",
       "      <td>2699.165417</td>\n",
       "      <td>2699.165417</td>\n",
       "      <td>4.017044</td>\n",
       "      <td>52.408273</td>\n",
       "      <td>155.071322</td>\n",
       "      <td>4.97</td>\n",
       "      <td>583651.772664</td>\n",
       "      <td>1761.295341</td>\n",
       "      <td>2646.757144</td>\n",
       "      <td>2544.094095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5821</th>\n",
       "      <td>2018-06-04 22:02:05+00:00</td>\n",
       "      <td>0.313458</td>\n",
       "      <td>0.508952</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.195494</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.822410</td>\n",
       "      <td>3.324900</td>\n",
       "      <td>0.247349</td>\n",
       "      <td>-1.396955</td>\n",
       "      <td>2657.946212</td>\n",
       "      <td>586317.596946</td>\n",
       "      <td>1723.843180</td>\n",
       "      <td>2657.946212</td>\n",
       "      <td>2657.946212</td>\n",
       "      <td>3.831055</td>\n",
       "      <td>46.578294</td>\n",
       "      <td>158.194750</td>\n",
       "      <td>4.90</td>\n",
       "      <td>583659.650734</td>\n",
       "      <td>1720.012125</td>\n",
       "      <td>2611.367918</td>\n",
       "      <td>2499.751462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5822</th>\n",
       "      <td>2018-06-04 22:03:05+00:00</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.199219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.198227</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.200211</td>\n",
       "      <td>4.128645</td>\n",
       "      <td>0.048493</td>\n",
       "      <td>-3.026331</td>\n",
       "      <td>2650.599402</td>\n",
       "      <td>586308.612876</td>\n",
       "      <td>1718.061157</td>\n",
       "      <td>2650.599402</td>\n",
       "      <td>2650.599402</td>\n",
       "      <td>3.631836</td>\n",
       "      <td>51.036074</td>\n",
       "      <td>160.641345</td>\n",
       "      <td>4.32</td>\n",
       "      <td>583658.013474</td>\n",
       "      <td>1714.429321</td>\n",
       "      <td>2599.563327</td>\n",
       "      <td>2489.958056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823</th>\n",
       "      <td>2018-06-04 22:04:05+00:00</td>\n",
       "      <td>0.172042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172042</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.172042</td>\n",
       "      <td>6.271124</td>\n",
       "      <td>0.027434</td>\n",
       "      <td>-3.595966</td>\n",
       "      <td>2650.082079</td>\n",
       "      <td>586314.173248</td>\n",
       "      <td>1715.979046</td>\n",
       "      <td>2650.082079</td>\n",
       "      <td>2650.082079</td>\n",
       "      <td>3.704804</td>\n",
       "      <td>51.092926</td>\n",
       "      <td>160.489197</td>\n",
       "      <td>4.32</td>\n",
       "      <td>583664.091169</td>\n",
       "      <td>1712.274243</td>\n",
       "      <td>2598.989153</td>\n",
       "      <td>2489.592882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      datetime  buy_volume  sell_volume  buy_txn  sell_txn  \\\n",
       "5819 2018-06-04 22:00:05+00:00    0.059804     0.730357      5.0      10.0   \n",
       "5820 2018-06-04 22:01:05+00:00    0.089359     0.849477      3.0       4.0   \n",
       "5821 2018-06-04 22:02:05+00:00    0.313458     0.508952      2.0       4.0   \n",
       "5822 2018-06-04 22:03:05+00:00    0.000992     0.199219      1.0       4.0   \n",
       "5823 2018-06-04 22:04:05+00:00    0.172042     0.000000      7.0       0.0   \n",
       "\n",
       "      volume_imbalance  txn_imbalance  total_volume  mean_volume  \\\n",
       "5819          0.670553            5.0      0.790162     4.380444   \n",
       "5820          0.760118            1.0      0.938836     3.692009   \n",
       "5821          0.195494            2.0      0.822410     3.324900   \n",
       "5822          0.198227            3.0      0.200211     4.128645   \n",
       "5823          0.172042            7.0      0.172042     6.271124   \n",
       "\n",
       "      deseasoned_total_volume  log_deseasoned_total_volume   ask_volume  \\\n",
       "5819                 0.180384                    -1.712667  2695.804973   \n",
       "5820                 0.254289                    -1.369285  2699.165417   \n",
       "5821                 0.247349                    -1.396955  2657.946212   \n",
       "5822                 0.048493                    -3.026331  2650.599402   \n",
       "5823                 0.027434                    -3.595966  2650.082079   \n",
       "\n",
       "         bid_volume  ask_slope_1  ask_slope_5  ask_slope_10  bid_slope_1  \\\n",
       "5819  586356.113693  1761.630667  2695.804973   2695.804973     3.972121   \n",
       "5820  586350.938081  1765.312385  2699.165417   2699.165417     4.017044   \n",
       "5821  586317.596946  1723.843180  2657.946212   2657.946212     3.831055   \n",
       "5822  586308.612876  1718.061157  2650.599402   2650.599402     3.631836   \n",
       "5823  586314.173248  1715.979046  2650.082079   2650.082079     3.704804   \n",
       "\n",
       "      bid_slope_5  bid_slope_10  spread  lob_volume_imbalance  \\\n",
       "5819    53.502450    160.246934    6.19         583660.308720   \n",
       "5820    52.408273    155.071322    4.97         583651.772664   \n",
       "5821    46.578294    158.194750    4.90         583659.650734   \n",
       "5822    51.036074    160.641345    4.32         583658.013474   \n",
       "5823    51.092926    160.489197    4.32         583664.091169   \n",
       "\n",
       "      slope_imbalance_1  slope_imbalance_5  slope_imbalance_10  \n",
       "5819        1757.658546        2642.302523         2535.558040  \n",
       "5820        1761.295341        2646.757144         2544.094095  \n",
       "5821        1720.012125        2611.367918         2499.751462  \n",
       "5822        1714.429321        2599.563327         2489.958056  \n",
       "5823        1712.274243        2598.989153         2489.592882  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trx_df = read_txn_data(use_load=False)\n",
    "trx_df = preprocess_txn_data(trx_df, freq='1min')\n",
    "trx_df['log_deseasoned_total_volume'] = np.log(trx_df['deseasoned_total_volume'] + 1e-07)\n",
    "\n",
    "lob_df = create_lob_dataset(use_load=False)\n",
    "\n",
    "df_merged = merge_txn_and_lob(trx_df, lob_df)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924251d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45d90738",
   "metadata": {},
   "source": [
    "## TME implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d437b82f",
   "metadata": {},
   "source": [
    "### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d00bdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 10  # window length\n",
    "batch_size = 128\n",
    "\n",
    "# -----------------------------\n",
    "df = df_merged.sort_values('datetime').reset_index(drop=True)\n",
    "# STEP 1: Create time-of-day feature\n",
    "df['time_of_day'] = df['datetime'].dt.strftime('%H:%M')\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 2: Split indices (AFTER creating lags!)\n",
    "n_total = len(df)\n",
    "n_train = int(0.7 * n_total)\n",
    "n_val = int(0.1 * n_total)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 3: Create deseasonalizing map using per-time volume means from train only\n",
    "train_deseason_df = df.iloc[:n_train]\n",
    "mean_volume_by_time = train_deseason_df.groupby('time_of_day')['total_volume'].mean()\n",
    "df['mean_volume'] = df['time_of_day'].map(mean_volume_by_time)\n",
    "\n",
    "df['deseasoned_total_volume'] = df['total_volume'] / df['mean_volume']\n",
    "df['log_deseasoned_total_volume'] = np.log(df['deseasoned_total_volume'] + 1e-7)\n",
    "df['target'] = df['deseasoned_total_volume']\n",
    "\n",
    "del train_deseason_df\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 4: Define the source-specific features\n",
    "source1_cols = ['buy_volume', 'sell_volume', 'buy_txn', 'sell_txn', 'volume_imbalance', 'txn_imbalance']\n",
    "source2_cols = ['ask_volume', 'bid_volume', 'ask_slope_1', 'ask_slope_5', 'ask_slope_10', 'bid_slope_1', 'bid_slope_5', 'bid_slope_10', 'spread',\n",
    "       'lob_volume_imbalance', 'slope_imbalance_1', 'slope_imbalance_5', 'slope_imbalance_10']\n",
    "# target_col = 'log_deseasoned_total_volume'\n",
    "target_col = 'target'\n",
    "target_direct_col = 'total_volume'\n",
    "weight_col = 'mean_volume'\n",
    "datetime_col = 'datetime'\n",
    "\n",
    "# # Normalize source1 and source2 features using training data only\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler1 = StandardScaler()\n",
    "# scaler2 = StandardScaler()\n",
    "\n",
    "# # Fit only on training portion\n",
    "# source1_train_raw = df[source1_cols].iloc[:n_train]\n",
    "# source2_train_raw = df[source2_cols].iloc[:n_train]\n",
    "\n",
    "# scaler1.fit(source1_train_raw)\n",
    "# scaler2.fit(source2_train_raw)\n",
    "\n",
    "# # Apply normalization to the whole dataset\n",
    "# df[source1_cols] = scaler1.transform(df[source1_cols])\n",
    "# df[source2_cols] = scaler2.transform(df[source2_cols])\n",
    "\n",
    "# --- Create rolling windows efficiently ---\n",
    "source1_array = df[source1_cols].values  # shape (N, F1)\n",
    "source2_array = df[source2_cols].values  # shape (N, F2)\n",
    "target_array = df[target_col].values + 1e-7  # shape (N,)\n",
    "target_direct_array = df[target_direct_col].values + 1e-7  # shape (N,)\n",
    "weight_array = df[weight_col].values  # shape (N,)\n",
    "timestamps_array = df[datetime_col].values\n",
    "\n",
    "\n",
    "# Create sliding windows ([:-1] in windows and [h:] in targets make sure the targets are matched with corresponding features)\n",
    "source1_windows = sliding_window_view(source1_array, window_shape=(h,), axis=0)[:-1]  # shape (N - h, F1, h)\n",
    "source2_windows = sliding_window_view(source2_array, window_shape=(h,), axis=0)[:-1]  # shape (N - h, F2, h)\n",
    "y = target_array[h:]\n",
    "w = weight_array[h:]\n",
    "timestamps = timestamps_array[h:]\n",
    "\n",
    "# Convert to tensors\n",
    "source1_tensor = torch.tensor(source1_windows, dtype=torch.float32)\n",
    "source2_tensor = torch.tensor(source2_windows, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "w_tensor = torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "# --- Time-based split (preserving time order) ---\n",
    "n_total = len(y_tensor)\n",
    "n_train = int(n_total * 0.7)\n",
    "n_val = int(n_total * 0.1)\n",
    "\n",
    "source1_train, source1_val, source1_test = source1_tensor[:n_train], source1_tensor[n_train:n_train + n_val], source1_tensor[n_train + n_val:]\n",
    "source2_train, source2_val, source2_test = source2_tensor[:n_train], source2_tensor[n_train:n_train + n_val], source2_tensor[n_train + n_val:]\n",
    "y_train, y_val, y_test = y_tensor[:n_train], y_tensor[n_train:n_train + n_val], y_tensor[n_train + n_val:]\n",
    "w_train, w_val, w_test = w_tensor[:n_train], w_tensor[n_train:n_train + n_val], w_tensor[n_train + n_val:]\n",
    "\n",
    "# (Optional) timestamps split for tracking\n",
    "timestamps_train = timestamps[:n_train]\n",
    "timestamps_val = timestamps[n_train:n_train + n_val]\n",
    "timestamps_test = timestamps[n_train + n_val:]\n",
    "\n",
    "# Dataset ready for PyTorch training\n",
    "train_dataset = torch.utils.data.TensorDataset(source1_train, source2_train, y_train, w_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(source1_val, source2_val, y_val, w_val)\n",
    "test_dataset = torch.utils.data.TensorDataset(source1_test, source2_test, y_test, w_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "## function for dataset creation for hyperparams search\n",
    "def create_datasets(source1_array, source2_array, target_array, weight_array, batch_size, h):\n",
    "       # Create sliding windows ([:-1] in windows and [h:] in targets make sure the targets are matched with corresponding features)\n",
    "       source1_windows = sliding_window_view(source1_array, window_shape=(h,), axis=0)[:-1]  # shape (N - h, F1, h)\n",
    "       source2_windows = sliding_window_view(source2_array, window_shape=(h,), axis=0)[:-1]  # shape (N - h, F2, h)\n",
    "       y = target_array[h:]\n",
    "       w = weight_array[h:]\n",
    "\n",
    "       # Convert to tensors\n",
    "       source1_tensor = torch.tensor(source1_windows, dtype=torch.float32)\n",
    "       source2_tensor = torch.tensor(source2_windows, dtype=torch.float32)\n",
    "       y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "       w_tensor = torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "       # --- Time-based split (preserving time order) ---\n",
    "       n_total = len(y_tensor)\n",
    "       n_train = int(n_total * 0.7)\n",
    "       n_val = int(n_total * 0.1)\n",
    "\n",
    "       source1_train, source1_val, source1_test = source1_tensor[:n_train], source1_tensor[n_train:n_train + n_val], source1_tensor[n_train + n_val:]\n",
    "       source2_train, source2_val, source2_test = source2_tensor[:n_train], source2_tensor[n_train:n_train + n_val], source2_tensor[n_train + n_val:]\n",
    "       y_train, y_val, y_test = y_tensor[:n_train], y_tensor[n_train:n_train + n_val], y_tensor[n_train + n_val:]\n",
    "       w_train, w_val, w_test = w_tensor[:n_train], w_tensor[n_train:n_train + n_val], w_tensor[n_train + n_val:]\n",
    "\n",
    "       # Dataset ready for PyTorch training\n",
    "       train_dataset = torch.utils.data.TensorDataset(source1_train, source2_train, y_train, w_train)\n",
    "       val_dataset = torch.utils.data.TensorDataset(source1_val, source2_val, y_val, w_val)\n",
    "       test_dataset = torch.utils.data.TensorDataset(source1_test, source2_test, y_test, w_test)\n",
    "\n",
    "       train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "       val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "       test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "       return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2e430",
   "metadata": {},
   "source": [
    "### TME components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf9371f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearRegressor(nn.Module):\n",
    "    def __init__(self, d, h, latent_variable):\n",
    "        \"\"\"\n",
    "        d: number of features in the source data\n",
    "        h: number of lags in the source data\n",
    "        latent_variable (bool): if True the class is devoted for modeling latent variable z, if False => y|s_i\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_variable = latent_variable\n",
    "\n",
    "        # Mean parameters\n",
    "        self.L_mu = nn.Parameter(torch.empty(d))\n",
    "        self.R_mu = nn.Parameter(torch.empty(h))\n",
    "        self.b_mu = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        # Xavier init for 1D weight tensors\n",
    "        # nn.init.xavier_uniform_(self.L_mu.unsqueeze(0))\n",
    "        # nn.init.xavier_uniform_(self.R_mu.unsqueeze(0))\n",
    "\n",
    "        nn.init.xavier_normal_(self.L_mu.unsqueeze(0))\n",
    "        nn.init.xavier_normal_(self.R_mu.unsqueeze(0))\n",
    "\n",
    "        if not self.latent_variable:\n",
    "            self.L_sigma = nn.Parameter(torch.empty(d))\n",
    "            self.R_sigma = nn.Parameter(torch.empty(h))\n",
    "            self.b_sigma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "            # nn.init.xavier_uniform_(self.L_sigma.unsqueeze(0))\n",
    "            # nn.init.xavier_uniform_(self.R_sigma.unsqueeze(0))\n",
    "\n",
    "            nn.init.xavier_normal_(self.L_sigma.unsqueeze(0))\n",
    "            nn.init.xavier_normal_(self.R_sigma.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):  # x: (B, d, h)\n",
    "        mu = torch.einsum('bdh,d,h->b', x, self.L_mu, self.R_mu) + self.b_mu  # [B]\n",
    "        if self.latent_variable:\n",
    "            return mu\n",
    "        log_var = torch.einsum('bdh,d,h->b', x, self.L_sigma, self.R_sigma) + self.b_sigma\n",
    "        log_var = torch.clamp(log_var, min=-10, max=10)\n",
    "        var = torch.exp(log_var)  # Ensure positivity\n",
    "        return mu, var\n",
    "    \n",
    "\n",
    "class TME(nn.Module):\n",
    "    def __init__(self, d1, d2, h):\n",
    "        super().__init__()\n",
    "        self.target1 = BilinearRegressor(d1, h, latent_variable=False)\n",
    "        self.target2 = BilinearRegressor(d2, h, latent_variable=False)\n",
    "        self.latent1 = BilinearRegressor(d1, h, latent_variable=True)\n",
    "        self.latent2 = BilinearRegressor(d2, h, latent_variable=True)\n",
    "\n",
    "    def forward(self, x1, x2, return_all=False):\n",
    "        # x1: (B, d1, h), x2: (B, d2, h)\n",
    "        mu1, var1 = self.target1(x1)  # [B], [B]\n",
    "        mu2, var2 = self.target2(x2)\n",
    "\n",
    "        logit1 = self.latent1(x1)\n",
    "        logit2 = self.latent2(x2)\n",
    "\n",
    "        logits = torch.stack([logit1, logit2], dim=1)  # [B, num_sources]\n",
    "        probs = F.softmax(logits, dim=1)     # [B, num_sources]\n",
    "\n",
    "        if True:#not return_all:\n",
    "            # Clamp to avoid numerical instability\n",
    "            mu1 = torch.clamp(mu1, -10, 10)\n",
    "            mu2 = torch.clamp(mu2, -10, 10)\n",
    "            var1 = torch.clamp(var1, min=1e-5, max=10)\n",
    "            var2 = torch.clamp(var2, min=1e-5, max=10)\n",
    "\n",
    "        # Mixture of expected values under log-normal\n",
    "        exp1 = torch.exp(mu1 + 0.5 * var1)\n",
    "        exp2 = torch.exp(mu2 + 0.5 * var2)\n",
    "        final_pred = probs[:, 0] * exp1 + probs[:, 1] * exp2  # [B]\n",
    "\n",
    "        if return_all:\n",
    "            return final_pred, mu1, var1, mu2, var2, probs\n",
    "        return final_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c959838",
   "metadata": {},
   "source": [
    "### Training routine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7da649f",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc6e25e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tme_loss(y, mu1, var1, mu2, var2, probs, model, l2_lambda=0.1):\n",
    "    \"\"\"\n",
    "    Implements:\n",
    "        -ln ∑_s [ lognormal(y_t | μ_s, σ_s^2) * P(z_t = s | x) ] + λ * ||θ||^2\n",
    "    \"\"\"\n",
    "\n",
    "    eps = 1e-8  # for numerical stability\n",
    "    log_y = torch.log(y + eps)\n",
    "\n",
    "    # Log-normal density terms (not in log-space)\n",
    "    def lognormal_pdf(y, log_y, mu, var):\n",
    "        coef = 1.0 / (y * torch.sqrt(2 * torch.pi * var + eps))\n",
    "        exponent = torch.exp(- (log_y - mu) ** 2 / (2 * var + eps))\n",
    "        return coef * exponent\n",
    "\n",
    "    p1 = lognormal_pdf(y, log_y, mu1, var1)\n",
    "    p2 = lognormal_pdf(y, log_y, mu2, var2)\n",
    "\n",
    "    # Combine with selector probabilities\n",
    "    # print(probs[:,1])\n",
    "    weighted_sum = probs[:,0] * p1 + probs[:,1] * p2\n",
    "\n",
    "    # Negative log-likelihood (mean over batch)\n",
    "    nll = -torch.log(weighted_sum + eps).mean() #maybe mean or sum\n",
    "\n",
    "    # L2 Regularization (Gaussian prior on θ)\n",
    "    l2_penalty = sum((p**2).sum() for p in model.parameters())\n",
    "    reg = l2_lambda * l2_penalty\n",
    "\n",
    "    return nll + reg\n",
    "\n",
    "\n",
    "def train_tme_model(model, train_loader, val_loader, lr=5e-4, weight_decay=0.1, l2_lambda=0.1,\n",
    "                    max_epochs=100, patience=10, device='cpu', adam=False, direct_target=False):\n",
    "    model.to(device)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=lr)#, weight_decay=0.1)#, momentum=0.9)\n",
    "    if adam:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_rmse = float('inf')\n",
    "    best_state_dict = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for x1, x2, y, w in train_loader:\n",
    "            x1, x2, y, w = x1.to(device), x2.to(device), y.to(device), w.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            final_pred, mu1, var1, mu2, var2, probs = model(x1, x2, return_all=True)\n",
    "\n",
    "            loss = tme_loss(y, mu1, var1, mu2, var2, probs, model, l2_lambda=l2_lambda)\n",
    "            loss.backward()\n",
    "\n",
    "            # total_norm = 0\n",
    "            # for p in model.parameters():\n",
    "            #     if p.grad is not None:\n",
    "            #         param_norm = p.grad.data.norm(2)\n",
    "            #         total_norm += param_norm.item() ** 2\n",
    "            # total_norm = total_norm ** 0.5\n",
    "            # print(f\"Gradient norm: {total_norm:.4f}\")\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        y_preds_val = []\n",
    "        y_true_val = []\n",
    "        w_val = []\n",
    "\n",
    "        train_losses = []\n",
    "        y_preds_train = []\n",
    "        y_true_train = []\n",
    "        w_train = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x1, x2, y, w in val_loader:\n",
    "                x1, x2, y, w = x1.to(device), x2.to(device), y.to(device), w.to(device)\n",
    "\n",
    "                final_pred, mu1, var1, mu2, var2, probs = model(x1, x2, return_all=True)\n",
    "\n",
    "                val_loss = tme_loss(y, mu1, var1, mu2, var2, probs, model, l2_lambda=l2_lambda)\n",
    "                val_losses.append(val_loss.item())\n",
    "                y_preds_val.append(final_pred.detach().cpu())\n",
    "                y_true_val.append(y.detach().cpu())\n",
    "                w_val.append(w.detach().cpu())\n",
    "\n",
    "            for x1, x2, y, w in train_loader:\n",
    "                x1, x2, y, w = x1.to(device), x2.to(device), y.to(device), w.to(device)\n",
    "\n",
    "                final_pred, mu1, var1, mu2, var2, probs = model(x1, x2, return_all=True)\n",
    "\n",
    "                train_loss = tme_loss(y, mu1, var1, mu2, var2, probs, model, l2_lambda=l2_lambda)\n",
    "                train_losses.append(train_loss.item())\n",
    "                y_preds_train.append(final_pred.detach().cpu())\n",
    "                y_true_train.append(y.detach().cpu())\n",
    "                w_train.append(w.detach().cpu())\n",
    "            \n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        y_preds_val = torch.cat(y_preds_val).numpy()\n",
    "        y_true_val = torch.cat(y_true_val).numpy()\n",
    "        w_val = torch.cat(w_val).numpy()\n",
    "        if direct_target:\n",
    "            rmse_val = np.sqrt(mean_squared_error(y_true_val, y_preds_val))\n",
    "        else:\n",
    "            rmse_val = np.sqrt(mean_squared_error(y_true_val*w_val, y_preds_val*w_val))\n",
    "\n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "        y_preds_train = torch.cat(y_preds_train).numpy()\n",
    "        y_true_train = torch.cat(y_true_train).numpy()\n",
    "        w_train = torch.cat(w_train).numpy()\n",
    "        if direct_target:\n",
    "            rmse_train = np.sqrt(mean_squared_error(y_true_train, y_preds_train))\n",
    "        else:    \n",
    "            rmse_train = np.sqrt(mean_squared_error(y_true_train*w_train, y_preds_train*w_train))\n",
    "\n",
    "        if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}.      Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}.      Train RMSE: {rmse_train:.4f}, Val RMSE: {rmse_val:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if rmse_val < best_val_rmse - 1e-4:#avg_val_loss < best_val_loss - 1e-4\n",
    "            # best_val_loss = avg_val_loss\n",
    "            best_val_rmse = rmse_val\n",
    "            best_state_dict = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Restore best model\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    return model, best_val_loss\n",
    "\n",
    "\n",
    "def train_tme_ensemble(train_loader, val_loader, d1, d2, h, num_models=20, device='cpu', adam=False, direct_target=False, **train_kwargs):\n",
    "    ensemble = []\n",
    "    val_losses = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        print(f\"\\n🌱 Training ensemble model {i + 1}/{num_models}\")\n",
    "\n",
    "        # Set seed for reproducibility\n",
    "        torch.manual_seed(i)\n",
    "        model = TME(d1, d2, h)  # Initialize new model\n",
    "\n",
    "        # Train the model using your function\n",
    "        trained_model, best_val_loss = train_tme_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            device=device,\n",
    "            adam=adam,\n",
    "            direct_target=direct_target,\n",
    "            **train_kwargs\n",
    "        )\n",
    "\n",
    "        # Save the model and its validation loss\n",
    "        ensemble.append(trained_model)\n",
    "        val_losses.append(best_val_loss)\n",
    "\n",
    "    return ensemble, val_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4758171f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🌱 Training ensemble model 1/1\n",
      "Epoch 1.      Train Loss: 3.1812, Val Loss: 4.4807.      Train RMSE: 50.8647, Val RMSE: 53.8183\n",
      "Epoch 5.      Train Loss: 2.5648, Val Loss: 4.0476.      Train RMSE: 32.3022, Val RMSE: 15.2410\n",
      "Epoch 10.      Train Loss: 2.8046, Val Loss: 4.2756.      Train RMSE: 16.7381, Val RMSE: 15.3284\n",
      "Epoch 15.      Train Loss: 4.1822, Val Loss: 2.8934.      Train RMSE: 17.4150, Val RMSE: 15.0587\n",
      "Epoch 20.      Train Loss: 2.2755, Val Loss: 3.6473.      Train RMSE: 101.5434, Val RMSE: 66.7435\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "ensemble, losses = train_tme_ensemble(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    d1=source1_tensor.shape[1],  # number of features of the 1st source\n",
    "    d2=source2_tensor.shape[1],  # number of features of the 2nd source\n",
    "    h=source1_tensor.shape[2],  # lag length\n",
    "    num_models=1,#20,\n",
    "    device=device,\n",
    "    lr=1e-3,\n",
    "    weight_decay=0.1,\n",
    "    l2_lambda=5,\n",
    "    max_epochs=60,\n",
    "    patience=10,\n",
    "    adam=True\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c36afb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tme_ensemble(ensemble, test_loader, all_preds=False, device='cpu', direct_target=False):\n",
    "    all_preds = []\n",
    "    all_preds_median = []\n",
    "    y_trues = []\n",
    "    w_trues = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, y, w in test_loader:\n",
    "            x1, x2, y, w = x1.to(device).to(torch.float64), x2.to(device).to(torch.float64), y.to(device).to(torch.float64), w.to(device).to(torch.float64)\n",
    "            batch_preds = []\n",
    "\n",
    "            for model in ensemble:\n",
    "                model.eval()\n",
    "                model.to(device).to(torch.float64)\n",
    "                pred = model(x1, x2)\n",
    "                # print(pred)\n",
    "                # return\n",
    "                batch_preds.append(pred.cpu())\n",
    "\n",
    "            # Average predictions from all models\n",
    "            avg_pred = torch.stack(batch_preds).mean(dim=0)\n",
    "            median_pred = torch.stack(batch_preds).median(dim=0).values\n",
    "            # avg_pred = torch.stack(batch_preds).median(dim=0)\n",
    "            all_preds.append(avg_pred)\n",
    "            all_preds_median.append(median_pred)\n",
    "            y_trues.append(y.cpu())\n",
    "            w_trues.append(w.cpu())\n",
    "\n",
    "    y_preds = torch.cat(all_preds).numpy()\n",
    "    y_preds_median = torch.cat(all_preds_median).numpy()\n",
    "    y_trues = torch.cat(y_trues).numpy()\n",
    "    w_trues = torch.cat(w_trues).numpy()\n",
    "\n",
    "    if direct_target:\n",
    "        rmse = np.sqrt(mean_squared_error(y_trues, y_preds))\n",
    "        mae = mean_absolute_error(y_trues, y_preds)\n",
    "    else:\n",
    "        rmse = np.sqrt(mean_squared_error(y_trues*w_trues, y_preds*w_trues))\n",
    "        mae = mean_absolute_error(y_trues*w_trues, y_preds*w_trues)\n",
    "\n",
    "    # print(f\"📊 Ensemble Test RMSE: {rmse:.4f}\")\n",
    "    # print(f\"📊 Ensemble Test MAE: {mae:.4f}\")\n",
    "    return y_preds, y_preds_median, rmse, mae\n",
    "\n",
    "\n",
    "# y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, test_loader, device='cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d81108cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.595749548023331 3.5011974121583433\n"
     ]
    }
   ],
   "source": [
    "print(rmse, mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33b128e",
   "metadata": {},
   "source": [
    "Some things to consider:  \n",
    "    \n",
    "    - Model training takes a long time (full 20 model ensemble took me more than 3 hours)  \n",
    "    - For hyperparams tuning we may use the smaller ensemble  \n",
    "    - Adam vs SGD?  \n",
    "    - Now I am clamping the values of the predicted variances and means when training and predicting (to avoid numerical blow up)  \n",
    "    - The RMSE and MAE I got are reasonable (they are not extremely different). This is just an indication that probably the calculations are doing what they are supposed to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1338aca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "229df85a",
   "metadata": {},
   "source": [
    "## Hyperparams search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee69d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 128, 0.001, 0.1], [10, 128, 0.001, 1.0], [10, 128, 0.001, 3.0], [10, 128, 0.001, 5.0], [10, 128, 0.0001, 0.1], [10, 128, 0.0001, 1.0], [10, 128, 0.0001, 3.0], [10, 128, 0.0001, 5.0], [10, 128, 0.0005, 0.1], [10, 128, 0.0005, 1.0], [10, 128, 0.0005, 3.0], [10, 128, 0.0005, 5.0], [10, 128, 5e-05, 0.1], [10, 128, 5e-05, 1.0], [10, 128, 5e-05, 3.0], [10, 128, 5e-05, 5.0], [10, 256, 5e-05, 0.1], [10, 64, 0.001, 0.1], [10, 64, 0.001, 1.0], [10, 64, 0.001, 3.0], [10, 64, 0.001, 5.0], [10, 64, 0.0001, 0.1], [10, 64, 0.0001, 1.0], [10, 64, 0.0001, 3.0], [10, 64, 0.0001, 5.0], [10, 64, 0.0005, 0.1], [10, 64, 0.0005, 1.0], [10, 64, 0.0005, 3.0], [10, 64, 0.0005, 5.0], [10, 64, 5e-05, 0.1], [10, 64, 5e-05, 1.0], [10, 64, 5e-05, 3.0], [10, 64, 5e-05, 5.0]]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing your saved results\n",
    "input_dir = 'validation_results'\n",
    "# Make sure the folder exists\n",
    "os.makedirs(input_dir, exist_ok=True)\n",
    "\n",
    "# Prepare a list to collect existing files (i.e. corresponding parameters have been already validated)\n",
    "existing_results = []\n",
    "\n",
    "# Loop over all files in the directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        \n",
    "        # Open and load the JSON file\n",
    "        with open(filepath, 'r') as f:\n",
    "            result = json.load(f)\n",
    "        \n",
    "        # Parse hyperparameters from the filename\n",
    "        name_parts = filename.replace('.json', '').split('_')\n",
    "        h = int(name_parts[0][1:])  # strip the 'h'\n",
    "        batch_size = int(name_parts[1][5:])  # strip 'batch'\n",
    "        lr = float(name_parts[2][2:].replace('e', 'e'))  # scientific notation stays\n",
    "        l2_lambda = float(name_parts[3][6:])  # strip 'lambda'\n",
    "\n",
    "        existing_results.append([h,batch_size,lr,l2_lambda])\n",
    "\n",
    "print(existing_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "aef73302",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 5e-05, l2_lambda:0.1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 20.3230, Val Loss: 20.3233.      Train RMSE: 26090.2323, Val RMSE: 22021.9251\n",
      "Epoch 5.      Train Loss: 19.4542, Val Loss: 19.4532.      Train RMSE: 27903.2110, Val RMSE: 43075.3996\n",
      "Epoch 10.      Train Loss: 7.9277, Val Loss: 8.5330.      Train RMSE: 3268816.9958, Val RMSE: 3268697.9837\n",
      "Epoch 15.      Train Loss: 3.1595, Val Loss: 4.2051.      Train RMSE: 22458.1866, Val RMSE: 1479.2523\n",
      "Epoch 20.      Train Loss: 2.6098, Val Loss: 2.7804.      Train RMSE: 37695.2258, Val RMSE: 2092.4844\n",
      "Epoch 25.      Train Loss: 2.3678, Val Loss: 1.5851.      Train RMSE: 43902.9533, Val RMSE: 3438.0080\n",
      "Epoch 30.      Train Loss: 2.2986, Val Loss: 1.5700.      Train RMSE: 45554.0259, Val RMSE: 4301.2317\n",
      "Epoch 35.      Train Loss: 2.1702, Val Loss: 1.4761.      Train RMSE: 100005.8162, Val RMSE: 100143.4057\n",
      "Epoch 40.      Train Loss: 2.1561, Val Loss: 1.4559.      Train RMSE: 99832.3895, Val RMSE: 95883.0648\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 20.4595, Val Loss: 20.4585.      Train RMSE: 24141.1940, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 19.5813, Val Loss: 19.5803.      Train RMSE: 27903.1135, Val RMSE: 50405.7774\n",
      "Epoch 10.      Train Loss: 8.0549, Val Loss: 8.6601.      Train RMSE: 3268906.8134, Val RMSE: 3268907.6153\n",
      "Epoch 15.      Train Loss: 7.7759, Val Loss: 8.3809.      Train RMSE: 3268906.8134, Val RMSE: 3268905.0492\n",
      "Epoch 20.      Train Loss: 7.6361, Val Loss: 8.2418.      Train RMSE: 3268981.3922, Val RMSE: 3268907.6153\n",
      "Epoch 25.      Train Loss: 2.3560, Val Loss: 1.6100.      Train RMSE: 64685.3267, Val RMSE: 25472.7613\n",
      "Epoch 30.      Train Loss: 2.2550, Val Loss: 1.5476.      Train RMSE: 107600.9790, Val RMSE: 112110.0397\n",
      "Epoch 35.      Train Loss: 2.2061, Val Loss: 1.5221.      Train RMSE: 115175.7797, Val RMSE: 124312.1171\n",
      "Epoch 40.      Train Loss: 2.1772, Val Loss: 1.4890.      Train RMSE: 109736.7206, Val RMSE: 112261.0626\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 11.2850, Val Loss: 10.8361.      Train RMSE: 559454.1321, Val RMSE: 395106.4349\n",
      "Epoch 5.      Train Loss: 4.0164, Val Loss: 4.0227.      Train RMSE: 478209.3019, Val RMSE: 401388.4077\n",
      "Epoch 10.      Train Loss: 2.9615, Val Loss: 2.7580.      Train RMSE: 220846.8542, Val RMSE: 193352.6115\n",
      "Epoch 15.      Train Loss: 2.6220, Val Loss: 2.3992.      Train RMSE: 72643.5030, Val RMSE: 72061.4280\n",
      "Epoch 20.      Train Loss: 2.5217, Val Loss: 2.2900.      Train RMSE: 50160.9773, Val RMSE: 17438.4210\n",
      "Epoch 25.      Train Loss: 2.4925, Val Loss: 2.2504.      Train RMSE: 53686.2280, Val RMSE: 12838.9763\n",
      "Epoch 30.      Train Loss: 2.3363, Val Loss: 1.5535.      Train RMSE: 55544.4396, Val RMSE: 11473.2147\n",
      "Epoch 35.      Train Loss: 2.1787, Val Loss: 1.4945.      Train RMSE: 86757.1168, Val RMSE: 52918.8021\n",
      "Epoch 40.      Train Loss: 2.1575, Val Loss: 1.4697.      Train RMSE: 95382.6711, Val RMSE: 83365.7273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [36:52, 2212.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 5e-05, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 37.4464, Val Loss: 37.4467.      Train RMSE: 26090.2360, Val RMSE: 22021.9251\n",
      "Epoch 5.      Train Loss: 28.7593, Val Loss: 28.7582.      Train RMSE: 27903.2110, Val RMSE: 43075.3996\n",
      "Epoch 10.      Train Loss: 11.9729, Val Loss: 12.5778.      Train RMSE: 3268951.5609, Val RMSE: 3268907.6153\n",
      "Epoch 15.      Train Loss: 4.4121, Val Loss: 3.8662.      Train RMSE: 21785.4263, Val RMSE: 67.1153\n",
      "Epoch 20.      Train Loss: 3.5799, Val Loss: 2.9585.      Train RMSE: 16806.3826, Val RMSE: 579.3373\n",
      "Epoch 25.      Train Loss: 2.8680, Val Loss: 2.1672.      Train RMSE: 19774.1714, Val RMSE: 546.3184\n",
      "Epoch 30.      Train Loss: 2.4599, Val Loss: 1.8516.      Train RMSE: 26869.9267, Val RMSE: 641.6730\n",
      "Epoch 35.      Train Loss: 2.3177, Val Loss: 1.6741.      Train RMSE: 31066.2013, Val RMSE: 688.1283\n",
      "Epoch 40.      Train Loss: 2.2840, Val Loss: 1.6567.      Train RMSE: 37454.2366, Val RMSE: 1189.9204\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 38.8124, Val Loss: 38.8113.      Train RMSE: 24141.1940, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 30.0310, Val Loss: 30.0301.      Train RMSE: 27903.1135, Val RMSE: 50405.7774\n",
      "Epoch 10.      Train Loss: 13.2948, Val Loss: 13.8984.      Train RMSE: 3268667.5082, Val RMSE: 3268907.6153\n",
      "Epoch 15.      Train Loss: 10.5018, Val Loss: 11.1058.      Train RMSE: 3268764.7080, Val RMSE: 3268905.3699\n",
      "Epoch 20.      Train Loss: 4.6020, Val Loss: 5.3512.      Train RMSE: 28505.2517, Val RMSE: 556.5927\n",
      "Epoch 25.      Train Loss: 3.4374, Val Loss: 2.7728.      Train RMSE: 13473.5189, Val RMSE: 211.0485\n",
      "Epoch 30.      Train Loss: 3.0372, Val Loss: 2.3756.      Train RMSE: 23509.7433, Val RMSE: 350.8992\n",
      "Epoch 35.      Train Loss: 2.7626, Val Loss: 2.1127.      Train RMSE: 26785.0800, Val RMSE: 468.0570\n",
      "Epoch 40.      Train Loss: 2.5802, Val Loss: 1.9280.      Train RMSE: 27592.8872, Val RMSE: 491.9657\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 24.8349, Val Loss: 24.3673.      Train RMSE: 559314.8906, Val RMSE: 392364.9332\n",
      "Epoch 5.      Train Loss: 10.1303, Val Loss: 10.2729.      Train RMSE: 381977.9080, Val RMSE: 302480.1011\n",
      "Epoch 10.      Train Loss: 5.0340, Val Loss: 5.2550.      Train RMSE: 100147.2196, Val RMSE: 99283.9542\n",
      "Epoch 15.      Train Loss: 3.3543, Val Loss: 3.6939.      Train RMSE: 5309.9981, Val RMSE: 253.4348\n",
      "Epoch 20.      Train Loss: 2.6658, Val Loss: 2.2631.      Train RMSE: 143.0662, Val RMSE: 143.6661\n",
      "Epoch 25.      Train Loss: 2.3150, Val Loss: 1.7063.      Train RMSE: 43705.4797, Val RMSE: 1637.4321\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [1:25:31, 2627.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 5e-05, l2_lambda:3\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 75.3535, Val Loss: 75.3538.      Train RMSE: 26090.2360, Val RMSE: 22021.9251\n",
      "Epoch 5.      Train Loss: 47.9568, Val Loss: 47.9558.      Train RMSE: 27903.2110, Val RMSE: 43075.3996\n",
      "Epoch 10.      Train Loss: 18.9682, Val Loss: 17.4793.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 15.      Train Loss: 8.1663, Val Loss: 7.4971.      Train RMSE: 18588.3224, Val RMSE: 62.5602\n",
      "Epoch 20.      Train Loss: 5.7918, Val Loss: 5.1531.      Train RMSE: 17447.8973, Val RMSE: 637.5747\n",
      "Epoch 25.      Train Loss: 3.8199, Val Loss: 3.2093.      Train RMSE: 20664.2919, Val RMSE: 427.4115\n",
      "Epoch 30.      Train Loss: 2.8159, Val Loss: 2.2201.      Train RMSE: 19626.0496, Val RMSE: 262.1400\n",
      "Epoch 35.      Train Loss: 2.4558, Val Loss: 1.8258.      Train RMSE: 8819.3782, Val RMSE: 136.6826\n",
      "Epoch 40.      Train Loss: 2.3823, Val Loss: 1.7386.      Train RMSE: 874.1507, Val RMSE: 73.8644\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 79.4548, Val Loss: 79.4537.      Train RMSE: 24141.1940, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 51.8807, Val Loss: 51.8799.      Train RMSE: 31214.8354, Val RMSE: 50405.7774\n",
      "Epoch 10.      Train Loss: 23.2946, Val Loss: 23.8980.      Train RMSE: 3268648.7416, Val RMSE: 3268907.6153\n",
      "Epoch 15.      Train Loss: 13.7769, Val Loss: 13.9647.      Train RMSE: 806946.9637, Val RMSE: 986320.0279\n",
      "Epoch 20.      Train Loss: 10.5346, Val Loss: 11.2702.      Train RMSE: 411083.3341, Val RMSE: 489051.3674\n",
      "Epoch 25.      Train Loss: 6.4031, Val Loss: 5.7264.      Train RMSE: 71.7431, Val RMSE: 63.9393\n",
      "Epoch 30.      Train Loss: 5.1569, Val Loss: 4.5112.      Train RMSE: 63.8474, Val RMSE: 51.9534\n",
      "Epoch 35.      Train Loss: 4.1992, Val Loss: 3.5395.      Train RMSE: 59.8796, Val RMSE: 44.0960\n",
      "Epoch 40.      Train Loss: 3.5276, Val Loss: 2.8796.      Train RMSE: 56.9219, Val RMSE: 41.5962\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 54.4528, Val Loss: 53.9485.      Train RMSE: 555468.7976, Val RMSE: 386674.5049\n",
      "Epoch 5.      Train Loss: 23.1031, Val Loss: 23.3998.      Train RMSE: 344219.3816, Val RMSE: 256250.5654\n",
      "Epoch 10.      Train Loss: 8.2249, Val Loss: 8.8305.      Train RMSE: 48242.4358, Val RMSE: 43958.3832\n",
      "Epoch 15.      Train Loss: 4.2307, Val Loss: 5.1004.      Train RMSE: 68.4659, Val RMSE: 44.6248\n",
      "Epoch 20.      Train Loss: 2.7762, Val Loss: 2.3748.      Train RMSE: 9895.2926, Val RMSE: 145.9228\n",
      "Epoch 25.      Train Loss: 2.4279, Val Loss: 2.0320.      Train RMSE: 753.0101, Val RMSE: 20.7711\n",
      "Epoch 30.      Train Loss: 2.3360, Val Loss: 1.9706.      Train RMSE: 319.7443, Val RMSE: 27.6948\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [2:45:12, 3611.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 5e-05, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 113.3089, Val Loss: 113.3092.      Train RMSE: 26090.2360, Val RMSE: 22021.9251\n",
      "Epoch 5.      Train Loss: 67.6479, Val Loss: 67.6469.      Train RMSE: 27903.2110, Val RMSE: 43075.3996\n",
      "Epoch 10.      Train Loss: 26.7123, Val Loss: 25.2234.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 15.      Train Loss: 11.6663, Val Loss: 11.0472.      Train RMSE: 18758.1696, Val RMSE: 22.9881\n",
      "Epoch 20.      Train Loss: 7.3381, Val Loss: 6.6828.      Train RMSE: 22479.6932, Val RMSE: 733.9451\n",
      "Epoch 25.      Train Loss: 4.3311, Val Loss: 3.7114.      Train RMSE: 22610.5144, Val RMSE: 374.1846\n",
      "Epoch 30.      Train Loss: 3.4163, Val Loss: 4.4211.      Train RMSE: 110.4258, Val RMSE: 43.1094\n",
      "Epoch 35.      Train Loss: 2.4876, Val Loss: 1.8724.      Train RMSE: 295.7268, Val RMSE: 50.5667\n",
      "Epoch 40.      Train Loss: 2.3693, Val Loss: 1.9622.      Train RMSE: 10516.1175, Val RMSE: 21.0255\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 120.1444, Val Loss: 120.1433.      Train RMSE: 24141.1940, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 74.1878, Val Loss: 74.1870.      Train RMSE: 31214.8354, Val RMSE: 50405.7774\n",
      "Epoch 10.      Train Loss: 33.8414, Val Loss: 34.4448.      Train RMSE: 3268648.2604, Val RMSE: 3268907.6153\n",
      "Epoch 15.      Train Loss: 19.8722, Val Loss: 20.1272.      Train RMSE: 1329124.7825, Val RMSE: 1898826.3963\n",
      "Epoch 20.      Train Loss: 14.6643, Val Loss: 15.3891.      Train RMSE: 360577.3291, Val RMSE: 435191.6799\n",
      "Epoch 25.      Train Loss: 8.3470, Val Loss: 7.7088.      Train RMSE: 56.3718, Val RMSE: 53.4446\n",
      "Epoch 30.      Train Loss: 6.5111, Val Loss: 5.8443.      Train RMSE: 40.9101, Val RMSE: 35.9305\n",
      "Epoch 35.      Train Loss: 5.0622, Val Loss: 4.3988.      Train RMSE: 35.1124, Val RMSE: 29.5143\n",
      "Epoch 40.      Train Loss: 4.0745, Val Loss: 3.4082.      Train RMSE: 32.2860, Val RMSE: 27.1867\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 83.8147, Val Loss: 83.2459.      Train RMSE: 546402.9301, Val RMSE: 373987.1310\n",
      "Epoch 5.      Train Loss: 36.2161, Val Loss: 36.5932.      Train RMSE: 343353.6357, Val RMSE: 249736.6026\n",
      "Epoch 10.      Train Loss: 11.6524, Val Loss: 12.4370.      Train RMSE: 43068.7844, Val RMSE: 29483.2129\n",
      "Epoch 15.      Train Loss: 4.8729, Val Loss: 5.8266.      Train RMSE: 42.5516, Val RMSE: 27.2202\n",
      "Epoch 20.      Train Loss: 2.8790, Val Loss: 2.4780.      Train RMSE: 191.5925, Val RMSE: 146.9928\n",
      "Epoch 25.      Train Loss: 2.4657, Val Loss: 2.1099.      Train RMSE: 146.9034, Val RMSE: 70.5998\n",
      "Epoch 30.      Train Loss: 2.3671, Val Loss: 2.0359.      Train RMSE: 90.9128, Val RMSE: 88.8902\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [3:27:34, 3189.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0001, l2_lambda:0.1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 20.0597, Val Loss: 20.0593.      Train RMSE: 26090.2201, Val RMSE: 34208.5893\n",
      "Epoch 5.      Train Loss: 7.9602, Val Loss: 8.5658.      Train RMSE: 3268742.2529, Val RMSE: 3268593.2431\n",
      "Epoch 10.      Train Loss: 2.6966, Val Loss: 3.0983.      Train RMSE: 28714.0239, Val RMSE: 1022.0173\n",
      "Epoch 15.      Train Loss: 2.3443, Val Loss: 1.5500.      Train RMSE: 44670.9327, Val RMSE: 3587.4922\n",
      "Epoch 20.      Train Loss: 2.1648, Val Loss: 1.4708.      Train RMSE: 99124.3169, Val RMSE: 98233.6832\n",
      "Epoch 25.      Train Loss: 2.1586, Val Loss: 1.4402.      Train RMSE: 88904.5986, Val RMSE: 51431.6634\n",
      "Epoch 30.      Train Loss: 2.1428, Val Loss: 1.4524.      Train RMSE: 82133.9924, Val RMSE: 34399.0716\n",
      "Epoch 35.      Train Loss: 2.1442, Val Loss: 1.4364.      Train RMSE: 84985.5720, Val RMSE: 45141.5877\n",
      "Epoch 40.      Train Loss: 2.1429, Val Loss: 1.4384.      Train RMSE: 85106.9223, Val RMSE: 50102.8651\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 20.1913, Val Loss: 20.1902.      Train RMSE: 24141.1940, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 8.0847, Val Loss: 8.6899.      Train RMSE: 3268891.8974, Val RMSE: 3268907.6153\n",
      "Epoch 10.      Train Loss: 7.6537, Val Loss: 8.2594.      Train RMSE: 3268981.7130, Val RMSE: 3268907.6153\n",
      "Epoch 15.      Train Loss: 2.6112, Val Loss: 2.7369.      Train RMSE: 57477.8950, Val RMSE: 13803.3608\n",
      "Epoch 20.      Train Loss: 2.3201, Val Loss: 1.5438.      Train RMSE: 56409.4630, Val RMSE: 11854.8029\n",
      "Epoch 25.      Train Loss: 2.1634, Val Loss: 1.4511.      Train RMSE: 110450.4464, Val RMSE: 117602.5426\n",
      "Epoch 30.      Train Loss: 2.1494, Val Loss: 1.4381.      Train RMSE: 87348.5267, Val RMSE: 38605.8085\n",
      "Epoch 35.      Train Loss: 2.1433, Val Loss: 1.4526.      Train RMSE: 87950.2725, Val RMSE: 42341.6293\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 7.8270, Val Loss: 7.9384.      Train RMSE: 577291.6079, Val RMSE: 442038.3587\n",
      "Epoch 5.      Train Loss: 3.0189, Val Loss: 2.8376.      Train RMSE: 252303.4800, Val RMSE: 214569.5521\n",
      "Epoch 10.      Train Loss: 2.5498, Val Loss: 2.3303.      Train RMSE: 59232.2161, Val RMSE: 46167.4088\n",
      "Epoch 15.      Train Loss: 2.4912, Val Loss: 2.2600.      Train RMSE: 52570.2176, Val RMSE: 11184.5311\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [3:57:51, 2694.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0001, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 34.8134, Val Loss: 34.8130.      Train RMSE: 26090.2225, Val RMSE: 34208.5893\n",
      "Epoch 5.      Train Loss: 12.2928, Val Loss: 12.8979.      Train RMSE: 3268891.8974, Val RMSE: 3268802.8814\n",
      "Epoch 10.      Train Loss: 3.5740, Val Loss: 4.4330.      Train RMSE: 10049.5763, Val RMSE: 145.0134\n",
      "Epoch 15.      Train Loss: 2.4996, Val Loss: 1.9465.      Train RMSE: 26217.2047, Val RMSE: 548.5112\n",
      "Epoch 20.      Train Loss: 2.2894, Val Loss: 1.6604.      Train RMSE: 31327.4699, Val RMSE: 721.7778\n",
      "Epoch 25.      Train Loss: 2.2829, Val Loss: 1.6360.      Train RMSE: 32949.9907, Val RMSE: 855.6383\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 36.1298, Val Loss: 36.1287.      Train RMSE: 24141.1940, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 13.5955, Val Loss: 14.2008.      Train RMSE: 3268906.8134, Val RMSE: 3268907.6153\n",
      "Epoch 10.      Train Loss: 4.7686, Val Loss: 5.5206.      Train RMSE: 28947.6847, Val RMSE: 542.5842\n",
      "Epoch 15.      Train Loss: 3.0941, Val Loss: 2.4560.      Train RMSE: 1979.3092, Val RMSE: 128.7470\n",
      "Epoch 20.      Train Loss: 2.6491, Val Loss: 2.0273.      Train RMSE: 25633.4881, Val RMSE: 398.1487\n",
      "Epoch 25.      Train Loss: 2.4077, Val Loss: 1.7918.      Train RMSE: 30533.1998, Val RMSE: 641.7400\n",
      "Epoch 30.      Train Loss: 2.2984, Val Loss: 1.6747.      Train RMSE: 33504.1089, Val RMSE: 885.0403\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 19.4388, Val Loss: 19.5162.      Train RMSE: 556854.1987, Val RMSE: 422551.0324\n",
      "Epoch 5.      Train Loss: 5.0968, Val Loss: 5.3171.      Train RMSE: 112114.8120, Val RMSE: 109105.5521\n",
      "Epoch 10.      Train Loss: 2.9481, Val Loss: 3.3292.      Train RMSE: 796.1534, Val RMSE: 150.0463\n",
      "Epoch 15.      Train Loss: 2.3181, Val Loss: 1.7260.      Train RMSE: 9559.3422, Val RMSE: 162.0097\n",
      "Epoch 20.      Train Loss: 2.2846, Val Loss: 1.6668.      Train RMSE: 39206.8125, Val RMSE: 1313.1257\n",
      "Epoch 25.      Train Loss: 2.2963, Val Loss: 1.6098.      Train RMSE: 36528.9829, Val RMSE: 1224.5600\n",
      "Epoch 30.      Train Loss: 2.2896, Val Loss: 1.6943.      Train RMSE: 39554.5404, Val RMSE: 1381.7094\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [4:27:28, 2382.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0001, l2_lambda:3\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 67.0944, Val Loss: 67.0940.      Train RMSE: 26090.2213, Val RMSE: 34208.5893\n",
      "Epoch 5.      Train Loss: 19.0201, Val Loss: 19.6249.      Train RMSE: 3268951.7213, Val RMSE: 3268907.6153\n",
      "Epoch 10.      Train Loss: 5.7651, Val Loss: 5.1090.      Train RMSE: 22507.3357, Val RMSE: 1095.9491\n",
      "Epoch 15.      Train Loss: 2.9742, Val Loss: 2.4107.      Train RMSE: 1794.8229, Val RMSE: 90.6431\n",
      "Epoch 20.      Train Loss: 2.4096, Val Loss: 1.8190.      Train RMSE: 2794.7941, Val RMSE: 102.6285\n",
      "Epoch 25.      Train Loss: 2.3895, Val Loss: 1.7251.      Train RMSE: 651.8258, Val RMSE: 65.1787\n",
      "Epoch 30.      Train Loss: 2.3975, Val Loss: 1.7203.      Train RMSE: 385.1858, Val RMSE: 55.1889\n",
      "Epoch 35.      Train Loss: 2.3828, Val Loss: 1.7323.      Train RMSE: 621.7877, Val RMSE: 66.4217\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 71.0599, Val Loss: 71.0588.      Train RMSE: 24141.1940, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 23.2855, Val Loss: 23.8899.      Train RMSE: 3268758.9339, Val RMSE: 3268907.6153\n",
      "Epoch 10.      Train Loss: 9.0691, Val Loss: 9.7639.      Train RMSE: 127235.4889, Val RMSE: 177615.7456\n",
      "Epoch 15.      Train Loss: 4.7871, Val Loss: 4.2103.      Train RMSE: 47.7501, Val RMSE: 43.1897\n",
      "Epoch 20.      Train Loss: 3.4801, Val Loss: 2.9110.      Train RMSE: 63.8224, Val RMSE: 45.5962\n",
      "Epoch 25.      Train Loss: 2.7509, Val Loss: 2.1741.      Train RMSE: 97.9587, Val RMSE: 47.2410\n",
      "Epoch 30.      Train Loss: 2.4229, Val Loss: 1.7905.      Train RMSE: 308.4827, Val RMSE: 56.2208\n",
      "Epoch 35.      Train Loss: 2.3845, Val Loss: 1.7294.      Train RMSE: 529.3340, Val RMSE: 62.7820\n",
      "Epoch 40.      Train Loss: 2.3856, Val Loss: 1.7289.      Train RMSE: 1090.4429, Val RMSE: 76.5835\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 44.4197, Val Loss: 44.4011.      Train RMSE: 577374.2469, Val RMSE: 425198.4102\n",
      "Epoch 5.      Train Loss: 7.9118, Val Loss: 8.5316.      Train RMSE: 49759.1202, Val RMSE: 42689.2474\n",
      "Epoch 10.      Train Loss: 2.8774, Val Loss: 2.4611.      Train RMSE: 2569.9502, Val RMSE: 126.9727\n",
      "Epoch 15.      Train Loss: 2.4114, Val Loss: 1.8055.      Train RMSE: 238.1994, Val RMSE: 52.0137\n",
      "Epoch 20.      Train Loss: 2.3814, Val Loss: 1.7681.      Train RMSE: 743.9803, Val RMSE: 71.6012\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [4:59:46, 2237.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0001, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 99.5438, Val Loss: 99.5434.      Train RMSE: 26090.2213, Val RMSE: 34208.5893\n",
      "Epoch 5.      Train Loss: 26.7187, Val Loss: 27.3235.      Train RMSE: 3268951.7213, Val RMSE: 3268907.6153\n",
      "Epoch 10.      Train Loss: 6.4684, Val Loss: 5.7901.      Train RMSE: 21082.1631, Val RMSE: 518.4248\n",
      "Epoch 15.      Train Loss: 3.1755, Val Loss: 2.5732.      Train RMSE: 14723.0258, Val RMSE: 191.5680\n",
      "Epoch 20.      Train Loss: 2.4612, Val Loss: 1.8810.      Train RMSE: 66.6935, Val RMSE: 32.6069\n",
      "Epoch 25.      Train Loss: 2.4032, Val Loss: 1.9664.      Train RMSE: 21.8992, Val RMSE: 22.8434\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 106.1529, Val Loss: 106.1519.      Train RMSE: 24141.1940, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 33.8267, Val Loss: 34.4312.      Train RMSE: 3268769.5198, Val RMSE: 3268907.6153\n",
      "Epoch 10.      Train Loss: 11.1050, Val Loss: 11.8804.      Train RMSE: 63199.4714, Val RMSE: 97556.3076\n",
      "Epoch 15.      Train Loss: 5.8836, Val Loss: 5.3145.      Train RMSE: 40.6926, Val RMSE: 35.8844\n",
      "Epoch 20.      Train Loss: 3.9274, Val Loss: 3.2112.      Train RMSE: 29.9939, Val RMSE: 24.9382\n",
      "Epoch 25.      Train Loss: 2.8721, Val Loss: 2.2775.      Train RMSE: 29.5470, Val RMSE: 22.1022\n",
      "Epoch 30.      Train Loss: 2.4723, Val Loss: 2.1790.      Train RMSE: 22.9271, Val RMSE: 20.6993\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 68.9011, Val Loss: 68.8136.      Train RMSE: 572120.7969, Val RMSE: 414902.8535\n",
      "Epoch 5.      Train Loss: 11.2270, Val Loss: 12.0256.      Train RMSE: 45582.0245, Val RMSE: 34245.9808\n",
      "Epoch 10.      Train Loss: 3.4092, Val Loss: 4.3854.      Train RMSE: 34.0102, Val RMSE: 22.0082\n",
      "Epoch 15.      Train Loss: 2.3863, Val Loss: 2.0535.      Train RMSE: 71.4992, Val RMSE: 71.9107\n",
      "Epoch 20.      Train Loss: 2.3621, Val Loss: 2.0320.      Train RMSE: 88.4318, Val RMSE: 86.7914\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [5:27:26, 2053.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0005, l2_lambda:0.1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 7.9921, Val Loss: 8.5970.      Train RMSE: 3268742.2529, Val RMSE: 3268697.9837\n",
      "Epoch 5.      Train Loss: 2.4916, Val Loss: 2.3034.      Train RMSE: 52713.1285, Val RMSE: 7405.5568\n",
      "Epoch 10.      Train Loss: 2.4819, Val Loss: 2.2442.      Train RMSE: 64684.3373, Val RMSE: 22702.5010\n",
      "Epoch 15.      Train Loss: 18.4384, Val Loss: 18.4185.      Train RMSE: 17.4302, Val RMSE: 15.0717\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 19.0588, Val Loss: 19.0657.      Train RMSE: 94884.2900, Val RMSE: 50449.0022\n",
      "Epoch 5.      Train Loss: 2.4968, Val Loss: 2.2594.      Train RMSE: 54626.8142, Val RMSE: 11167.2496\n",
      "Epoch 10.      Train Loss: 2.4818, Val Loss: 2.2397.      Train RMSE: 53241.3530, Val RMSE: 10699.7413\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 3.0737, Val Loss: 2.9015.      Train RMSE: 276555.4267, Val RMSE: 232794.4643\n",
      "Epoch 5.      Train Loss: 7.3796, Val Loss: 5.8889.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 10.      Train Loss: 2.4818, Val Loss: 2.2237.      Train RMSE: 53692.2049, Val RMSE: 10790.5381\n",
      "Epoch 15.      Train Loss: 7.3518, Val Loss: 5.8611.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [5:42:05, 1686.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0005, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 12.6594, Val Loss: 13.2644.      Train RMSE: 3268742.2529, Val RMSE: 3268697.9837\n",
      "Epoch 5.      Train Loss: 2.8297, Val Loss: 3.6512.      Train RMSE: 89.7619, Val RMSE: 61.7254\n",
      "Epoch 10.      Train Loss: 2.8019, Val Loss: 3.2421.      Train RMSE: 449.1404, Val RMSE: 132.0141\n",
      "Epoch 15.      Train Loss: 3.5050, Val Loss: 4.5740.      Train RMSE: 16.9418, Val RMSE: 14.5999\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 24.8824, Val Loss: 24.8893.      Train RMSE: 94884.3007, Val RMSE: 50449.0174\n",
      "Epoch 5.      Train Loss: 7.5907, Val Loss: 8.1959.      Train RMSE: 3269011.7044, Val RMSE: 3269012.5063\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 5.1148, Val Loss: 5.3030.      Train RMSE: 126201.3994, Val RMSE: 127366.2511\n",
      "Epoch 5.      Train Loss: 2.4270, Val Loss: 2.0709.      Train RMSE: 37.8255, Val RMSE: 25.7286\n",
      "Epoch 10.      Train Loss: 2.8014, Val Loss: 3.1686.      Train RMSE: 213.7924, Val RMSE: 127.0414\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [5:52:47, 1363.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0005, l2_lambda:3\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 18.7746, Val Loss: 17.2867.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 5.      Train Loss: 2.3943, Val Loss: 1.7685.      Train RMSE: 46.7717, Val RMSE: 28.6892\n",
      "Epoch 10.      Train Loss: 2.9491, Val Loss: 3.8602.      Train RMSE: 47.0645, Val RMSE: 30.9748\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 23.2210, Val Loss: 23.8261.      Train RMSE: 3268891.8974, Val RMSE: 3268907.6153\n",
      "Epoch 5.      Train Loss: 3.0263, Val Loss: 3.9298.      Train RMSE: 47.3855, Val RMSE: 34.4612\n",
      "Epoch 10.      Train Loss: 2.4148, Val Loss: 1.8757.      Train RMSE: 30.6925, Val RMSE: 19.8919\n",
      "Epoch 15.      Train Loss: 2.9485, Val Loss: 3.8965.      Train RMSE: 39.1634, Val RMSE: 25.3606\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 7.4505, Val Loss: 8.0912.      Train RMSE: 52593.4457, Val RMSE: 43858.6559\n",
      "Epoch 5.      Train Loss: 2.3911, Val Loss: 1.7818.      Train RMSE: 30.2691, Val RMSE: 19.5512\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [6:05:12, 1174.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.0005, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 26.3883, Val Loss: 24.9004.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 5.      Train Loss: 2.5254, Val Loss: 2.0483.      Train RMSE: 39.6204, Val RMSE: 25.7379\n",
      "Epoch 10.      Train Loss: 2.4492, Val Loss: 1.8228.      Train RMSE: 24.8438, Val RMSE: 16.9739\n",
      "Epoch 15.      Train Loss: 2.4804, Val Loss: 1.9549.      Train RMSE: 25.8838, Val RMSE: 17.6929\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 33.7198, Val Loss: 34.3249.      Train RMSE: 3268891.8974, Val RMSE: 3268907.6153\n",
      "Epoch 5.      Train Loss: 3.1562, Val Loss: 2.5029.      Train RMSE: 29.8565, Val RMSE: 23.4263\n",
      "Epoch 10.      Train Loss: 2.5283, Val Loss: 2.0927.      Train RMSE: 24.7007, Val RMSE: 17.3693\n",
      "Epoch 15.      Train Loss: 2.4500, Val Loss: 1.8388.      Train RMSE: 32.4772, Val RMSE: 21.9345\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 10.5672, Val Loss: 11.3807.      Train RMSE: 46386.5884, Val RMSE: 34510.7074\n",
      "Epoch 5.      Train Loss: 2.4406, Val Loss: 2.0338.      Train RMSE: 137.4737, Val RMSE: 139.2923\n",
      "Epoch 10.      Train Loss: 2.5035, Val Loss: 2.1934.      Train RMSE: 5101.3790, Val RMSE: 394.4688\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [6:19:44, 1082.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.001, l2_lambda:0.1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 7.5986, Val Loss: 8.2035.      Train RMSE: 3268966.7973, Val RMSE: 3269012.5063\n",
      "Epoch 5.      Train Loss: 2.4818, Val Loss: 2.2336.      Train RMSE: 50471.6646, Val RMSE: 7450.1976\n",
      "Epoch 10.      Train Loss: 7.4746, Val Loss: 8.0798.      Train RMSE: 3269011.7044, Val RMSE: 3269012.5063\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 7.7056, Val Loss: 8.3108.      Train RMSE: 3268996.6285, Val RMSE: 3269012.5063\n",
      "Epoch 5.      Train Loss: 2.4819, Val Loss: 2.2567.      Train RMSE: 49164.4750, Val RMSE: 6030.3585\n",
      "Epoch 10.      Train Loss: 2.4821, Val Loss: 2.2313.      Train RMSE: 55843.7019, Val RMSE: 13635.9935\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 7.6509, Val Loss: 8.2564.      Train RMSE: 3268951.7213, Val RMSE: 3268907.6153\n",
      "Epoch 5.      Train Loss: 7.4787, Val Loss: 8.0840.      Train RMSE: 3269011.8648, Val RMSE: 3269012.5063\n",
      "Epoch 10.      Train Loss: 2.4825, Val Loss: 2.2156.      Train RMSE: 59497.1849, Val RMSE: 15267.0668\n",
      "Epoch 15.      Train Loss: 2.4816, Val Loss: 2.2407.      Train RMSE: 55586.6390, Val RMSE: 11749.8330\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [6:31:23, 966.37s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.001, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 8.7373, Val Loss: 9.3422.      Train RMSE: 3268966.7973, Val RMSE: 3269012.5063\n",
      "Epoch 5.      Train Loss: 7.5228, Val Loss: 8.1280.      Train RMSE: 3269011.3836, Val RMSE: 3269012.5063\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 9.8032, Val Loss: 10.4083.      Train RMSE: 3268996.6285, Val RMSE: 3269012.5063\n",
      "Epoch 5.      Train Loss: 2.8016, Val Loss: 3.1877.      Train RMSE: 764.8955, Val RMSE: 146.9589\n",
      "Epoch 10.      Train Loss: 2.8034, Val Loss: 3.1550.      Train RMSE: 1195.3352, Val RMSE: 156.7029\n",
      "Epoch 15.      Train Loss: 2.8030, Val Loss: 3.1582.      Train RMSE: 21386.8677, Val RMSE: 275.4210\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 3.0937, Val Loss: 3.6035.      Train RMSE: 17291.7201, Val RMSE: 406.2484\n",
      "Epoch 5.      Train Loss: 7.4134, Val Loss: 5.9227.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 10.      Train Loss: 2.8031, Val Loss: 3.1157.      Train RMSE: 218.3843, Val RMSE: 132.0378\n",
      "Epoch 15.      Train Loss: 2.8030, Val Loss: 3.1794.      Train RMSE: 446.8715, Val RMSE: 137.9891\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [6:43:07, 887.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.001, l2_lambda:3\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 8.7314, Val Loss: 7.2407.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 5.      Train Loss: 2.9510, Val Loss: 3.8524.      Train RMSE: 50.0820, Val RMSE: 32.9926\n",
      "Epoch 10.      Train Loss: 2.9890, Val Loss: 3.8583.      Train RMSE: 50.8430, Val RMSE: 36.1062\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 6.7570, Val Loss: 7.5414.      Train RMSE: 69.1503, Val RMSE: 66.3323\n",
      "Epoch 5.      Train Loss: 7.3565, Val Loss: 5.8659.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 7.8345, Val Loss: 6.3440.      Train RMSE: 13992.4168, Val RMSE: 15.0702\n",
      "Epoch 5.      Train Loss: 2.9486, Val Loss: 3.8687.      Train RMSE: 44.8619, Val RMSE: 29.8554\n",
      "Epoch 10.      Train Loss: 7.5131, Val Loss: 8.1183.      Train RMSE: 3269011.7044, Val RMSE: 3269012.5063\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [6:57:28, 879.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:128, lr: 0.001, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 8.5913, Val Loss: 7.1006.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 5.      Train Loss: 3.0338, Val Loss: 4.0063.      Train RMSE: 35.4024, Val RMSE: 23.1083\n",
      "Epoch 10.      Train Loss: 3.0353, Val Loss: 3.9985.      Train RMSE: 37.1176, Val RMSE: 24.0744\n",
      "Epoch 15.      Train Loss: 3.0328, Val Loss: 4.0245.      Train RMSE: 31.8512, Val RMSE: 20.6032\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 8.7031, Val Loss: 9.5352.      Train RMSE: 60.0884, Val RMSE: 57.8389\n",
      "Epoch 5.      Train Loss: 3.0327, Val Loss: 4.0103.      Train RMSE: 34.2949, Val RMSE: 22.1758\n",
      "Epoch 10.      Train Loss: 2.9925, Val Loss: 2.1175.      Train RMSE: 17.1086, Val RMSE: 14.7654\n",
      "Epoch 15.      Train Loss: 3.0345, Val Loss: 4.0380.      Train RMSE: 29.5189, Val RMSE: 19.2690\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 3.4234, Val Loss: 4.4887.      Train RMSE: 17.0077, Val RMSE: 14.9864\n",
      "Epoch 5.      Train Loss: 7.4965, Val Loss: 6.0058.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 10.      Train Loss: 3.0348, Val Loss: 3.9985.      Train RMSE: 36.5201, Val RMSE: 23.8916\n",
      "Epoch 15.      Train Loss: 7.3548, Val Loss: 5.8642.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [7:23:08, 1078.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 5e-05, l2_lambda:0.1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 20.4715, Val Loss: 20.4719.      Train RMSE: 26090.1379, Val RMSE: 22021.9251\n",
      "Epoch 5.      Train Loss: 19.9388, Val Loss: 19.9384.      Train RMSE: 26090.2078, Val RMSE: 34208.5893\n",
      "Epoch 10.      Train Loss: 19.4532, Val Loss: 19.4522.      Train RMSE: 27903.2110, Val RMSE: 43075.3996\n",
      "Epoch 15.      Train Loss: 8.0458, Val Loss: 6.5559.      Train RMSE: 68.9020, Val RMSE: 15.0702\n",
      "Epoch 20.      Train Loss: 7.8074, Val Loss: 6.3186.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 25.      Train Loss: 7.6402, Val Loss: 6.1506.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 30.      Train Loss: 2.5927, Val Loss: 1.8788.      Train RMSE: 22592.4759, Val RMSE: 249.7546\n",
      "Epoch 35.      Train Loss: 2.3997, Val Loss: 1.7049.      Train RMSE: 20092.0577, Val RMSE: 699.6084\n",
      "Epoch 40.      Train Loss: 2.3213, Val Loss: 1.6696.      Train RMSE: 33497.8256, Val RMSE: 2312.7356\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 20.6112, Val Loss: 20.6103.      Train RMSE: 26089.9453, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 20.0685, Val Loss: 20.0674.      Train RMSE: 24141.1967, Val RMSE: 50405.7774\n",
      "Epoch 10.      Train Loss: 19.5803, Val Loss: 19.5794.      Train RMSE: 27903.1135, Val RMSE: 50405.7774\n",
      "Epoch 15.      Train Loss: 8.2934, Val Loss: 8.8961.      Train RMSE: 3268637.1929, Val RMSE: 3268802.8814\n",
      "Epoch 20.      Train Loss: 8.0532, Val Loss: 8.6583.      Train RMSE: 3268936.6451, Val RMSE: 3268907.6153\n",
      "Epoch 25.      Train Loss: 7.8890, Val Loss: 8.4940.      Train RMSE: 3268951.8817, Val RMSE: 3268907.6153\n",
      "Epoch 30.      Train Loss: 7.7746, Val Loss: 8.3796.      Train RMSE: 3268966.7973, Val RMSE: 3268907.6153\n",
      "Epoch 35.      Train Loss: 7.6942, Val Loss: 8.2992.      Train RMSE: 3268981.7130, Val RMSE: 3268907.6153\n",
      "Epoch 40.      Train Loss: 7.6359, Val Loss: 8.2409.      Train RMSE: 3268981.5526, Val RMSE: 3268907.6153\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 13.0209, Val Loss: 12.2576.      Train RMSE: 526117.4644, Val RMSE: 371179.3429\n",
      "Epoch 5.      Train Loss: 6.3374, Val Loss: 6.5915.      Train RMSE: 579466.4757, Val RMSE: 459264.8919\n",
      "Epoch 10.      Train Loss: 3.9416, Val Loss: 3.9366.      Train RMSE: 465634.0573, Val RMSE: 387442.4259\n",
      "Epoch 15.      Train Loss: 3.2830, Val Loss: 3.1311.      Train RMSE: 321087.7257, Val RMSE: 268014.3184\n",
      "Epoch 20.      Train Loss: 2.9107, Val Loss: 2.7021.      Train RMSE: 197342.6794, Val RMSE: 175442.8301\n",
      "Epoch 25.      Train Loss: 2.7062, Val Loss: 2.4863.      Train RMSE: 104151.9235, Val RMSE: 103460.3748\n",
      "Epoch 30.      Train Loss: 2.5967, Val Loss: 2.3723.      Train RMSE: 64114.3898, Val RMSE: 64270.9345\n",
      "Epoch 35.      Train Loss: 2.5398, Val Loss: 2.3086.      Train RMSE: 50982.9374, Val RMSE: 23038.5395\n",
      "Epoch 40.      Train Loss: 2.5087, Val Loss: 2.2719.      Train RMSE: 51455.6020, Val RMSE: 14018.1654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [8:20:53, 1795.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 5e-05, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 38.9321, Val Loss: 38.9324.      Train RMSE: 26090.1404, Val RMSE: 22021.9251\n",
      "Epoch 5.      Train Loss: 33.6045, Val Loss: 33.6041.      Train RMSE: 26090.2090, Val RMSE: 34208.5893\n",
      "Epoch 10.      Train Loss: 28.7497, Val Loss: 28.7487.      Train RMSE: 27903.2110, Val RMSE: 43075.3996\n",
      "Epoch 15.      Train Loss: 14.2490, Val Loss: 12.7597.      Train RMSE: 68.9020, Val RMSE: 15.0702\n",
      "Epoch 20.      Train Loss: 11.8456, Val Loss: 10.3555.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 25.      Train Loss: 10.2036, Val Loss: 8.7120.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 30.      Train Loss: 4.3794, Val Loss: 3.7135.      Train RMSE: 20863.6771, Val RMSE: 67.2791\n",
      "Epoch 35.      Train Loss: 4.0092, Val Loss: 3.3841.      Train RMSE: 20025.0587, Val RMSE: 860.8380\n",
      "Epoch 40.      Train Loss: 3.5470, Val Loss: 2.9170.      Train RMSE: 17794.6347, Val RMSE: 593.6912\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 40.3302, Val Loss: 40.3292.      Train RMSE: 26089.9453, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 34.9020, Val Loss: 34.9009.      Train RMSE: 24141.1967, Val RMSE: 50405.7774\n",
      "Epoch 10.      Train Loss: 30.0215, Val Loss: 30.0205.      Train RMSE: 27903.1135, Val RMSE: 50405.7774\n",
      "Epoch 15.      Train Loss: 15.6647, Val Loss: 16.2678.      Train RMSE: 3268697.1817, Val RMSE: 3268802.8814\n",
      "Epoch 20.      Train Loss: 13.2850, Val Loss: 13.8887.      Train RMSE: 3268727.0154, Val RMSE: 3268907.6153\n",
      "Epoch 25.      Train Loss: 11.6357, Val Loss: 12.2408.      Train RMSE: 3268951.7213, Val RMSE: 3268907.6153\n",
      "Epoch 30.      Train Loss: 10.4941, Val Loss: 11.0990.      Train RMSE: 3268951.7213, Val RMSE: 3268907.6153\n",
      "Epoch 35.      Train Loss: 5.8576, Val Loss: 6.4666.      Train RMSE: 50114.4699, Val RMSE: 5666.2409\n",
      "Epoch 40.      Train Loss: 4.5752, Val Loss: 5.3026.      Train RMSE: 22929.6657, Val RMSE: 440.7376\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 27.5714, Val Loss: 26.8109.      Train RMSE: 526309.4468, Val RMSE: 370483.3247\n",
      "Epoch 5.      Train Loss: 16.8371, Val Loss: 17.0614.      Train RMSE: 554154.2955, Val RMSE: 425877.6790\n",
      "Epoch 10.      Train Loss: 9.9196, Val Loss: 10.0475.      Train RMSE: 369964.3766, Val RMSE: 294574.4735\n",
      "Epoch 15.      Train Loss: 6.7987, Val Loss: 6.9353.      Train RMSE: 209056.8204, Val RMSE: 173802.0605\n",
      "Epoch 20.      Train Loss: 4.9474, Val Loss: 5.1694.      Train RMSE: 94432.1952, Val RMSE: 93959.3894\n",
      "Epoch 25.      Train Loss: 3.8932, Val Loss: 4.1863.      Train RMSE: 30612.3991, Val RMSE: 6944.4274\n",
      "Epoch 30.      Train Loss: 3.3239, Val Loss: 3.6557.      Train RMSE: 4346.9295, Val RMSE: 236.0093\n",
      "Epoch 35.      Train Loss: 3.0364, Val Loss: 3.3932.      Train RMSE: 835.9611, Val RMSE: 152.5266\n",
      "Epoch 40.      Train Loss: 2.6410, Val Loss: 2.2291.      Train RMSE: 133.7502, Val RMSE: 132.7808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [8:56:55, 1905.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 5e-05, l2_lambda:3\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 79.9163, Val Loss: 79.9167.      Train RMSE: 26090.1404, Val RMSE: 22021.9251\n",
      "Epoch 5.      Train Loss: 63.3188, Val Loss: 63.3184.      Train RMSE: 26090.2078, Val RMSE: 34208.5893\n",
      "Epoch 10.      Train Loss: 47.9267, Val Loss: 47.9257.      Train RMSE: 27903.2110, Val RMSE: 43075.3996\n",
      "Epoch 15.      Train Loss: 26.2846, Val Loss: 26.8895.      Train RMSE: 3268787.0026, Val RMSE: 3268697.9837\n",
      "Epoch 20.      Train Loss: 19.0697, Val Loss: 19.6734.      Train RMSE: 3268360.3315, Val RMSE: 3268907.6153\n",
      "Epoch 25.      Train Loss: 14.4047, Val Loss: 15.0092.      Train RMSE: 3268981.5526, Val RMSE: 3269012.5063\n",
      "Epoch 30.      Train Loss: 8.1657, Val Loss: 9.2370.      Train RMSE: 31469.9125, Val RMSE: 35929.1000\n",
      "Epoch 35.      Train Loss: 6.5401, Val Loss: 5.8327.      Train RMSE: 18412.5470, Val RMSE: 666.0637\n",
      "Epoch 40.      Train Loss: 5.3280, Val Loss: 4.6889.      Train RMSE: 17638.7646, Val RMSE: 542.5838\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 84.1116, Val Loss: 84.1106.      Train RMSE: 26089.9453, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 67.2356, Val Loss: 67.2345.      Train RMSE: 24141.1967, Val RMSE: 50405.7774\n",
      "Epoch 10.      Train Loss: 51.8509, Val Loss: 51.8501.      Train RMSE: 31214.8354, Val RMSE: 50405.7774\n",
      "Epoch 15.      Train Loss: 30.3706, Val Loss: 30.9731.      Train RMSE: 3268607.3584, Val RMSE: 3268907.6153\n",
      "Epoch 20.      Train Loss: 23.2665, Val Loss: 23.8699.      Train RMSE: 3268697.1817, Val RMSE: 3268907.6153\n",
      "Epoch 25.      Train Loss: 18.5232, Val Loss: 19.1254.      Train RMSE: 3268517.6930, Val RMSE: 3268907.6153\n",
      "Epoch 30.      Train Loss: 13.4318, Val Loss: 13.4768.      Train RMSE: 8880.7139, Val RMSE: 205.3457\n",
      "Epoch 35.      Train Loss: 9.3453, Val Loss: 10.0450.      Train RMSE: 29242.0946, Val RMSE: 488.4234\n",
      "Epoch 40.      Train Loss: 7.2370, Val Loss: 8.0167.      Train RMSE: 76.1481, Val RMSE: 71.5834\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 59.7262, Val Loss: 58.9510.      Train RMSE: 534128.5251, Val RMSE: 372463.9447\n",
      "Epoch 5.      Train Loss: 39.5622, Val Loss: 39.7100.      Train RMSE: 550601.0424, Val RMSE: 399590.5181\n",
      "Epoch 10.      Train Loss: 22.8325, Val Loss: 23.1235.      Train RMSE: 340390.4724, Val RMSE: 250854.5538\n",
      "Epoch 15.      Train Loss: 13.3242, Val Loss: 13.7379.      Train RMSE: 156433.3542, Val RMSE: 131084.7650\n",
      "Epoch 20.      Train Loss: 8.1338, Val Loss: 8.7651.      Train RMSE: 46120.1270, Val RMSE: 39825.2905\n",
      "Epoch 25.      Train Loss: 5.5631, Val Loss: 6.3561.      Train RMSE: 1818.2752, Val RMSE: 167.7865\n",
      "Epoch 30.      Train Loss: 4.2090, Val Loss: 5.0863.      Train RMSE: 67.3580, Val RMSE: 43.8769\n",
      "Epoch 35.      Train Loss: 3.1373, Val Loss: 2.7413.      Train RMSE: 178.7520, Val RMSE: 153.9877\n",
      "Epoch 40.      Train Loss: 2.7207, Val Loss: 2.3784.      Train RMSE: 258.1379, Val RMSE: 34.3938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [9:30:11, 1932.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 5e-05, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 120.9137, Val Loss: 120.9140.      Train RMSE: 26090.1404, Val RMSE: 22021.9251\n",
      "Epoch 5.      Train Loss: 93.2512, Val Loss: 93.2508.      Train RMSE: 26090.2078, Val RMSE: 34208.5893\n",
      "Epoch 10.      Train Loss: 67.5977, Val Loss: 67.5966.      Train RMSE: 27903.2110, Val RMSE: 43075.3996\n",
      "Epoch 15.      Train Loss: 38.8256, Val Loss: 39.4305.      Train RMSE: 3268787.0026, Val RMSE: 3268697.9837\n",
      "Epoch 20.      Train Loss: 26.8009, Val Loss: 27.4045.      Train RMSE: 3268357.6044, Val RMSE: 3268907.6153\n",
      "Epoch 25.      Train Loss: 18.9483, Val Loss: 19.5528.      Train RMSE: 3268981.5526, Val RMSE: 3269012.5063\n",
      "Epoch 30.      Train Loss: 10.4985, Val Loss: 9.8565.      Train RMSE: 6037.0185, Val RMSE: 47.7102\n",
      "Epoch 35.      Train Loss: 8.7196, Val Loss: 8.0014.      Train RMSE: 25068.4005, Val RMSE: 1462.6384\n",
      "Epoch 40.      Train Loss: 6.6265, Val Loss: 5.9883.      Train RMSE: 27264.0669, Val RMSE: 1748.3216\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 127.9059, Val Loss: 127.9049.      Train RMSE: 26089.9453, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 99.7790, Val Loss: 99.7780.      Train RMSE: 24141.1967, Val RMSE: 50405.7774\n",
      "Epoch 10.      Train Loss: 74.1381, Val Loss: 74.1373.      Train RMSE: 31214.8354, Val RMSE: 50405.7774\n",
      "Epoch 15.      Train Loss: 45.6343, Val Loss: 46.2368.      Train RMSE: 3268607.3584, Val RMSE: 3268907.6153\n",
      "Epoch 20.      Train Loss: 33.7945, Val Loss: 34.3972.      Train RMSE: 3268592.9223, Val RMSE: 3268907.6153\n",
      "Epoch 25.      Train Loss: 25.8965, Val Loss: 26.4936.      Train RMSE: 3267202.7455, Val RMSE: 3268439.5746\n",
      "Epoch 30.      Train Loss: 18.7429, Val Loss: 18.7615.      Train RMSE: 1970.7843, Val RMSE: 126.3512\n",
      "Epoch 35.      Train Loss: 13.1970, Val Loss: 13.8863.      Train RMSE: 20744.1597, Val RMSE: 261.6326\n",
      "Epoch 40.      Train Loss: 9.8717, Val Loss: 10.6910.      Train RMSE: 62.8063, Val RMSE: 60.3923\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 91.7752, Val Loss: 90.9524.      Train RMSE: 521440.4703, Val RMSE: 367502.0325\n",
      "Epoch 5.      Train Loss: 61.7767, Val Loss: 61.8890.      Train RMSE: 544368.8162, Val RMSE: 385674.4293\n",
      "Epoch 10.      Train Loss: 35.9120, Val Loss: 36.2896.      Train RMSE: 341852.9791, Val RMSE: 250350.4207\n",
      "Epoch 15.      Train Loss: 20.4355, Val Loss: 20.9923.      Train RMSE: 154369.4461, Val RMSE: 123683.3366\n",
      "Epoch 20.      Train Loss: 11.5430, Val Loss: 12.3469.      Train RMSE: 42662.8232, Val RMSE: 29467.5111\n",
      "Epoch 25.      Train Loss: 6.9098, Val Loss: 7.8272.      Train RMSE: 185.4882, Val RMSE: 66.7713\n",
      "Epoch 30.      Train Loss: 4.8578, Val Loss: 5.8074.      Train RMSE: 43.0559, Val RMSE: 27.5854\n",
      "Epoch 35.      Train Loss: 3.4075, Val Loss: 3.0149.      Train RMSE: 173.3871, Val RMSE: 159.1828\n",
      "Epoch 40.      Train Loss: 3.4547, Val Loss: 4.4967.      Train RMSE: 32.9265, Val RMSE: 23.2153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [10:04:14, 1965.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0001, l2_lambda:0.1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 20.3227, Val Loss: 20.3230.      Train RMSE: 26090.2348, Val RMSE: 22021.9251\n",
      "Epoch 5.      Train Loss: 19.4680, Val Loss: 19.4670.      Train RMSE: 27903.2110, Val RMSE: 43075.3996\n",
      "Epoch 10.      Train Loss: 7.9592, Val Loss: 8.5643.      Train RMSE: 3268742.0925, Val RMSE: 3268593.2431\n",
      "Epoch 15.      Train Loss: 7.6779, Val Loss: 8.2823.      Train RMSE: 3268921.8897, Val RMSE: 3268907.6153\n",
      "Epoch 20.      Train Loss: 2.6564, Val Loss: 2.9457.      Train RMSE: 31053.6826, Val RMSE: 1407.4275\n",
      "Epoch 25.      Train Loss: 2.3768, Val Loss: 1.6415.      Train RMSE: 40601.0018, Val RMSE: 2914.3982\n",
      "Epoch 30.      Train Loss: 2.3363, Val Loss: 1.5751.      Train RMSE: 45607.0451, Val RMSE: 4091.9438\n",
      "Epoch 35.      Train Loss: 2.1755, Val Loss: 1.4966.      Train RMSE: 101192.8193, Val RMSE: 104961.6975\n",
      "Epoch 40.      Train Loss: 2.1586, Val Loss: 1.4518.      Train RMSE: 104475.4180, Val RMSE: 105529.8094\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 20.4592, Val Loss: 20.4582.      Train RMSE: 24141.1940, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 19.5953, Val Loss: 19.5943.      Train RMSE: 27903.1135, Val RMSE: 50405.7774\n",
      "Epoch 10.      Train Loss: 8.0842, Val Loss: 8.6885.      Train RMSE: 3268921.8897, Val RMSE: 3268907.6153\n",
      "Epoch 15.      Train Loss: 7.8002, Val Loss: 8.4051.      Train RMSE: 3268966.7973, Val RMSE: 3268907.6153\n",
      "Epoch 20.      Train Loss: 7.6531, Val Loss: 8.2583.      Train RMSE: 3268966.7973, Val RMSE: 3268907.6153\n",
      "Epoch 25.      Train Loss: 7.5732, Val Loss: 8.1785.      Train RMSE: 3268981.7130, Val RMSE: 3268907.6153\n",
      "Epoch 30.      Train Loss: 2.5950, Val Loss: 2.6718.      Train RMSE: 59350.9298, Val RMSE: 15109.9485\n",
      "Epoch 35.      Train Loss: 2.5195, Val Loss: 2.3168.      Train RMSE: 54588.1804, Val RMSE: 12177.6372\n",
      "Epoch 40.      Train Loss: 2.3525, Val Loss: 1.5533.      Train RMSE: 55006.6736, Val RMSE: 10403.1968\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 11.1880, Val Loss: 10.7576.      Train RMSE: 550128.3944, Val RMSE: 390513.5258\n",
      "Epoch 5.      Train Loss: 3.9569, Val Loss: 3.9626.      Train RMSE: 471226.7804, Val RMSE: 394035.7081\n",
      "Epoch 10.      Train Loss: 2.9810, Val Loss: 2.7958.      Train RMSE: 236975.8741, Val RMSE: 205025.0264\n",
      "Epoch 15.      Train Loss: 2.6512, Val Loss: 2.4388.      Train RMSE: 89772.2031, Val RMSE: 90932.5229\n",
      "Epoch 20.      Train Loss: 2.5371, Val Loss: 2.3130.      Train RMSE: 57721.5761, Val RMSE: 36784.9326\n",
      "Epoch 25.      Train Loss: 2.3688, Val Loss: 1.6088.      Train RMSE: 59186.7077, Val RMSE: 21138.8174\n",
      "Epoch 30.      Train Loss: 2.3369, Val Loss: 1.5917.      Train RMSE: 53485.3795, Val RMSE: 10647.2588\n",
      "Epoch 35.      Train Loss: 2.1671, Val Loss: 1.4794.      Train RMSE: 93099.4562, Val RMSE: 77051.2557\n",
      "Epoch 40.      Train Loss: 2.1511, Val Loss: 1.4529.      Train RMSE: 91407.8194, Val RMSE: 76526.1852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [10:36:15, 1952.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0001, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 37.4434, Val Loss: 37.4437.      Train RMSE: 26090.2360, Val RMSE: 22021.9251\n",
      "Epoch 5.      Train Loss: 28.8976, Val Loss: 28.8966.      Train RMSE: 27903.2110, Val RMSE: 43075.3996\n",
      "Epoch 10.      Train Loss: 12.2835, Val Loss: 12.8881.      Train RMSE: 3268891.8974, Val RMSE: 3268802.8814\n",
      "Epoch 15.      Train Loss: 9.5001, Val Loss: 10.1047.      Train RMSE: 3268966.7973, Val RMSE: 3269012.5063\n",
      "Epoch 20.      Train Loss: 3.2041, Val Loss: 2.6407.      Train RMSE: 13571.7613, Val RMSE: 341.7332\n",
      "Epoch 25.      Train Loss: 2.8472, Val Loss: 2.1797.      Train RMSE: 20431.2999, Val RMSE: 651.9859\n",
      "Epoch 30.      Train Loss: 2.5018, Val Loss: 1.9199.      Train RMSE: 26128.0857, Val RMSE: 612.4619\n",
      "Epoch 35.      Train Loss: 2.3426, Val Loss: 1.6718.      Train RMSE: 28985.3348, Val RMSE: 618.3293\n",
      "Epoch 40.      Train Loss: 2.8210, Val Loss: 3.6354.      Train RMSE: 2945.5938, Val RMSE: 121.1506\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 38.8093, Val Loss: 38.8083.      Train RMSE: 24141.1940, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 30.1707, Val Loss: 30.1697.      Train RMSE: 27903.1135, Val RMSE: 50405.7774\n",
      "Epoch 10.      Train Loss: 13.5887, Val Loss: 14.1906.      Train RMSE: 3268577.3633, Val RMSE: 3268907.6153\n",
      "Epoch 15.      Train Loss: 10.7502, Val Loss: 11.3551.      Train RMSE: 3268966.7973, Val RMSE: 3268907.6153\n",
      "Epoch 20.      Train Loss: 9.2842, Val Loss: 9.8889.      Train RMSE: 3268981.5526, Val RMSE: 3269012.5063\n",
      "Epoch 25.      Train Loss: 3.8106, Val Loss: 4.4790.      Train RMSE: 203.7280, Val RMSE: 100.9997\n",
      "Epoch 30.      Train Loss: 3.4042, Val Loss: 4.0437.      Train RMSE: 209.8609, Val RMSE: 104.5301\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 24.7393, Val Loss: 24.2773.      Train RMSE: 560137.7696, Val RMSE: 392052.7795\n",
      "Epoch 5.      Train Loss: 9.7908, Val Loss: 9.9254.      Train RMSE: 372857.7823, Val RMSE: 297575.6377\n",
      "Epoch 10.      Train Loss: 5.0256, Val Loss: 5.2432.      Train RMSE: 110423.7470, Val RMSE: 114197.6527\n",
      "Epoch 15.      Train Loss: 8.2980, Val Loss: 8.9026.      Train RMSE: 3268876.8210, Val RMSE: 3268907.6153\n",
      "Epoch 20.      Train Loss: 2.7630, Val Loss: 2.2992.      Train RMSE: 9897.8002, Val RMSE: 84.9703\n",
      "Epoch 25.      Train Loss: 2.8300, Val Loss: 3.4091.      Train RMSE: 378.9815, Val RMSE: 112.8079\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [11:04:13, 1870.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0001, l2_lambda:3\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 75.3433, Val Loss: 75.3436.      Train RMSE: 26090.2360, Val RMSE: 22021.9251\n",
      "Epoch 5.      Train Loss: 47.9776, Val Loss: 47.9766.      Train RMSE: 27903.2110, Val RMSE: 43075.3996\n",
      "Epoch 10.      Train Loss: 19.0221, Val Loss: 19.6265.      Train RMSE: 3268951.5609, Val RMSE: 3268907.6153\n",
      "Epoch 15.      Train Loss: 6.8750, Val Loss: 6.1620.      Train RMSE: 13861.7436, Val RMSE: 15.2620\n",
      "Epoch 20.      Train Loss: 5.6992, Val Loss: 5.1193.      Train RMSE: 21446.9847, Val RMSE: 877.1610\n",
      "Epoch 25.      Train Loss: 4.0690, Val Loss: 3.3908.      Train RMSE: 20423.8418, Val RMSE: 447.4381\n",
      "Epoch 30.      Train Loss: 2.8673, Val Loss: 2.2515.      Train RMSE: 12576.2678, Val RMSE: 187.8165\n",
      "Epoch 35.      Train Loss: 2.4380, Val Loss: 1.7933.      Train RMSE: 3446.2615, Val RMSE: 104.5026\n",
      "Epoch 40.      Train Loss: 2.3877, Val Loss: 1.7483.      Train RMSE: 1585.0720, Val RMSE: 86.6188\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 79.4445, Val Loss: 79.4434.      Train RMSE: 24141.1940, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 51.9333, Val Loss: 51.9324.      Train RMSE: 27903.1135, Val RMSE: 50405.7774\n",
      "Epoch 10.      Train Loss: 23.2570, Val Loss: 23.8571.      Train RMSE: 3268293.5989, Val RMSE: 3268907.6153\n",
      "Epoch 15.      Train Loss: 15.1912, Val Loss: 15.7959.      Train RMSE: 3268966.1558, Val RMSE: 3268891.0955\n",
      "Epoch 20.      Train Loss: 7.1303, Val Loss: 7.9086.      Train RMSE: 74.5569, Val RMSE: 70.7179\n",
      "Epoch 25.      Train Loss: 5.0633, Val Loss: 4.3426.      Train RMSE: 52.8399, Val RMSE: 47.7576\n",
      "Epoch 30.      Train Loss: 4.3921, Val Loss: 3.7414.      Train RMSE: 44.8231, Val RMSE: 38.7699\n",
      "Epoch 35.      Train Loss: 3.6998, Val Loss: 3.0618.      Train RMSE: 57.9261, Val RMSE: 42.9165\n",
      "Epoch 40.      Train Loss: 3.1746, Val Loss: 2.5365.      Train RMSE: 56.5505, Val RMSE: 40.8361\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 54.2773, Val Loss: 53.7854.      Train RMSE: 558836.0090, Val RMSE: 388219.0834\n",
      "Epoch 5.      Train Loss: 22.6901, Val Loss: 22.9867.      Train RMSE: 342364.6965, Val RMSE: 254134.2411\n",
      "Epoch 10.      Train Loss: 7.8488, Val Loss: 8.4847.      Train RMSE: 49803.0296, Val RMSE: 49077.5895\n",
      "Epoch 15.      Train Loss: 8.8250, Val Loss: 9.4301.      Train RMSE: 3268923.8143, Val RMSE: 3268907.6153\n",
      "Epoch 20.      Train Loss: 3.3162, Val Loss: 4.2516.      Train RMSE: 46.0673, Val RMSE: 30.5035\n",
      "Epoch 25.      Train Loss: 2.4673, Val Loss: 2.1468.      Train RMSE: 9894.7942, Val RMSE: 44.2220\n",
      "Epoch 30.      Train Loss: 2.3402, Val Loss: 1.9760.      Train RMSE: 404.0711, Val RMSE: 26.5538\n",
      "Epoch 35.      Train Loss: 2.3245, Val Loss: 1.9831.      Train RMSE: 288.2706, Val RMSE: 35.0225\n",
      "Epoch 40.      Train Loss: 2.3216, Val Loss: 1.9120.      Train RMSE: 347.9390, Val RMSE: 17.5953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [11:42:03, 1990.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0001, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 113.2919, Val Loss: 113.2923.      Train RMSE: 26090.2360, Val RMSE: 22021.9251\n",
      "Epoch 5.      Train Loss: 67.6825, Val Loss: 67.6815.      Train RMSE: 27903.2110, Val RMSE: 43075.3996\n",
      "Epoch 10.      Train Loss: 26.6634, Val Loss: 27.2678.      Train RMSE: 3268951.5609, Val RMSE: 3268907.6153\n",
      "Epoch 15.      Train Loss: 9.4503, Val Loss: 10.5619.      Train RMSE: 2292.0388, Val RMSE: 18.9440\n",
      "Epoch 20.      Train Loss: 6.1481, Val Loss: 5.4778.      Train RMSE: 21080.5981, Val RMSE: 420.3572\n",
      "Epoch 25.      Train Loss: 4.3588, Val Loss: 3.6982.      Train RMSE: 22005.0147, Val RMSE: 319.2543\n",
      "Epoch 30.      Train Loss: 3.2611, Val Loss: 4.2600.      Train RMSE: 66.4326, Val RMSE: 35.8851\n",
      "Epoch 35.      Train Loss: 2.4946, Val Loss: 1.8560.      Train RMSE: 445.8572, Val RMSE: 55.7967\n",
      "Epoch 40.      Train Loss: 2.4590, Val Loss: 1.8008.      Train RMSE: 65.6301, Val RMSE: 30.5736\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 120.1273, Val Loss: 120.1262.      Train RMSE: 24141.1940, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 74.2754, Val Loss: 74.2744.      Train RMSE: 27903.1135, Val RMSE: 50405.7774\n",
      "Epoch 10.      Train Loss: 33.7804, Val Loss: 34.3847.      Train RMSE: 3268907.6153, Val RMSE: 3268907.6153\n",
      "Epoch 15.      Train Loss: 20.3194, Val Loss: 20.9238.      Train RMSE: 3268940.1736, Val RMSE: 3268815.3919\n",
      "Epoch 20.      Train Loss: 9.5773, Val Loss: 10.4001.      Train RMSE: 61.4887, Val RMSE: 59.2408\n",
      "Epoch 25.      Train Loss: 6.7972, Val Loss: 6.1106.      Train RMSE: 45.2393, Val RMSE: 40.6768\n",
      "Epoch 30.      Train Loss: 5.5448, Val Loss: 4.8819.      Train RMSE: 37.9181, Val RMSE: 32.7350\n",
      "Epoch 35.      Train Loss: 4.3970, Val Loss: 3.7640.      Train RMSE: 34.5902, Val RMSE: 29.1463\n",
      "Epoch 40.      Train Loss: 3.5946, Val Loss: 2.9390.      Train RMSE: 31.4010, Val RMSE: 25.9217\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 83.6358, Val Loss: 83.0736.      Train RMSE: 543845.2335, Val RMSE: 371305.7615\n",
      "Epoch 5.      Train Loss: 35.7274, Val Loss: 36.1082.      Train RMSE: 343787.9369, Val RMSE: 251612.9457\n",
      "Epoch 10.      Train Loss: 11.1193, Val Loss: 11.9369.      Train RMSE: 44941.9252, Val RMSE: 32651.9782\n",
      "Epoch 15.      Train Loss: 4.6801, Val Loss: 5.6385.      Train RMSE: 44.1184, Val RMSE: 28.7405\n",
      "Epoch 20.      Train Loss: 2.9754, Val Loss: 2.5720.      Train RMSE: 3635.1212, Val RMSE: 142.6209\n",
      "Epoch 25.      Train Loss: 2.4808, Val Loss: 2.1338.      Train RMSE: 65.6877, Val RMSE: 66.7949\n",
      "Epoch 30.      Train Loss: 2.3833, Val Loss: 2.0532.      Train RMSE: 90.1402, Val RMSE: 88.2560\n",
      "Epoch 35.      Train Loss: 2.3686, Val Loss: 2.0176.      Train RMSE: 105.9305, Val RMSE: 102.5917\n",
      "Epoch 40.      Train Loss: 2.3618, Val Loss: 2.0268.      Train RMSE: 71.9211, Val RMSE: 72.3001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [12:09:27, 1886.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0005, l2_lambda:0.1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 19.4769, Val Loss: 19.4758.      Train RMSE: 26090.1943, Val RMSE: 43075.3996\n",
      "Epoch 5.      Train Loss: 7.5277, Val Loss: 8.1318.      Train RMSE: 3268936.6451, Val RMSE: 3268907.6153\n",
      "Epoch 10.      Train Loss: 2.4950, Val Loss: 2.3390.      Train RMSE: 52927.6179, Val RMSE: 8533.6023\n",
      "Epoch 15.      Train Loss: 2.4813, Val Loss: 2.2348.      Train RMSE: 54719.4643, Val RMSE: 13534.6967\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 19.6040, Val Loss: 19.6031.      Train RMSE: 27903.1124, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 2.8073, Val Loss: 3.3156.      Train RMSE: 51340.5870, Val RMSE: 10952.5811\n",
      "Epoch 10.      Train Loss: 7.3876, Val Loss: 5.8981.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 15.      Train Loss: 2.4820, Val Loss: 2.2452.      Train RMSE: 53005.0636, Val RMSE: 11516.1237\n",
      "Epoch 20.      Train Loss: 2.4812, Val Loss: 2.2389.      Train RMSE: 54527.0235, Val RMSE: 11579.3447\n",
      "Epoch 25.      Train Loss: 2.4818, Val Loss: 2.2463.      Train RMSE: 60394.4644, Val RMSE: 18855.6979\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 3.9181, Val Loss: 3.9054.      Train RMSE: 471632.2190, Val RMSE: 397860.0344\n",
      "Epoch 5.      Train Loss: 2.5703, Val Loss: 2.3812.      Train RMSE: 62631.6418, Val RMSE: 62916.1240\n",
      "Epoch 10.      Train Loss: 2.4873, Val Loss: 2.2520.      Train RMSE: 53797.8938, Val RMSE: 11376.3245\n",
      "Epoch 15.      Train Loss: 18.4285, Val Loss: 18.4285.      Train RMSE: 22021.0917, Val RMSE: 22021.9251\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [12:18:48, 1488.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0005, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 28.9853, Val Loss: 28.9842.      Train RMSE: 26090.1943, Val RMSE: 43075.3996\n",
      "Epoch 5.      Train Loss: 3.3869, Val Loss: 4.2703.      Train RMSE: 11911.0060, Val RMSE: 233.9440\n",
      "Epoch 10.      Train Loss: 2.8021, Val Loss: 3.2214.      Train RMSE: 512.8055, Val RMSE: 135.4022\n",
      "Epoch 15.      Train Loss: 2.3389, Val Loss: 1.8477.      Train RMSE: 28688.3123, Val RMSE: 535.2252\n",
      "Epoch 20.      Train Loss: 2.3134, Val Loss: 1.7778.      Train RMSE: 39663.6031, Val RMSE: 1400.6666\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 30.2586, Val Loss: 30.2576.      Train RMSE: 27903.1124, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 4.1680, Val Loss: 4.8576.      Train RMSE: 590.4712, Val RMSE: 109.6564\n",
      "Epoch 10.      Train Loss: 7.4640, Val Loss: 5.9745.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 9.6750, Val Loss: 9.7987.      Train RMSE: 368980.9534, Val RMSE: 299694.5543\n",
      "Epoch 5.      Train Loss: 2.8824, Val Loss: 3.2663.      Train RMSE: 2643.9845, Val RMSE: 181.7478\n",
      "Epoch 10.      Train Loss: 7.3511, Val Loss: 5.8612.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [12:25:41, 1165.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0005, l2_lambda:3\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 47.9948, Val Loss: 47.9938.      Train RMSE: 27903.2133, Val RMSE: 43075.3996\n",
      "Epoch 5.      Train Loss: 4.5312, Val Loss: 3.8823.      Train RMSE: 17745.6239, Val RMSE: 36.2151\n",
      "Epoch 10.      Train Loss: 2.7375, Val Loss: 2.1274.      Train RMSE: 12765.1664, Val RMSE: 169.3573\n",
      "Epoch 15.      Train Loss: 2.3858, Val Loss: 1.7349.      Train RMSE: 1094.3165, Val RMSE: 75.5994\n",
      "Epoch 20.      Train Loss: 2.9476, Val Loss: 3.8805.      Train RMSE: 41.8192, Val RMSE: 27.1829\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 51.9672, Val Loss: 51.9662.      Train RMSE: 27903.1124, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 5.0850, Val Loss: 5.8867.      Train RMSE: 64.5910, Val RMSE: 59.2530\n",
      "Epoch 10.      Train Loss: 3.0670, Val Loss: 2.5271.      Train RMSE: 38.7578, Val RMSE: 31.5656\n",
      "Epoch 15.      Train Loss: 2.9613, Val Loss: 3.8828.      Train RMSE: 43.4057, Val RMSE: 29.8989\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 22.3747, Val Loss: 22.6651.      Train RMSE: 343724.8090, Val RMSE: 252788.6175\n",
      "Epoch 5.      Train Loss: 7.4598, Val Loss: 5.9681.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 10.      Train Loss: 2.4188, Val Loss: 1.8889.      Train RMSE: 30.2650, Val RMSE: 19.8921\n",
      "Epoch 15.      Train Loss: 2.4080, Val Loss: 1.7190.      Train RMSE: 559.0371, Val RMSE: 60.2007\n",
      "Epoch 20.      Train Loss: 2.4198, Val Loss: 1.7210.      Train RMSE: 643.6203, Val RMSE: 60.9590\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [12:34:49, 980.35s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.0005, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 67.7112, Val Loss: 67.7102.      Train RMSE: 27903.2133, Val RMSE: 43075.3996\n",
      "Epoch 5.      Train Loss: 3.5055, Val Loss: 3.0296.      Train RMSE: 1600.9967, Val RMSE: 15.0414\n",
      "Epoch 10.      Train Loss: 2.4568, Val Loss: 1.8636.      Train RMSE: 26.0961, Val RMSE: 17.8783\n",
      "Epoch 15.      Train Loss: 3.0321, Val Loss: 4.0252.      Train RMSE: 31.4933, Val RMSE: 20.3810\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 74.3318, Val Loss: 74.3309.      Train RMSE: 27903.1124, Val RMSE: 50405.7774\n",
      "Epoch 5.      Train Loss: 5.7140, Val Loss: 6.5870.      Train RMSE: 52.9187, Val RMSE: 47.6646\n",
      "Epoch 10.      Train Loss: 3.1935, Val Loss: 2.5691.      Train RMSE: 29.8331, Val RMSE: 23.4061\n",
      "Epoch 15.      Train Loss: 2.4860, Val Loss: 1.7901.      Train RMSE: 23.8471, Val RMSE: 16.3296\n",
      "Epoch 20.      Train Loss: 3.0320, Val Loss: 4.0152.      Train RMSE: 33.2298, Val RMSE: 21.5370\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 35.5359, Val Loss: 35.9160.      Train RMSE: 340084.5463, Val RMSE: 250296.8161\n",
      "Epoch 5.      Train Loss: 2.8387, Val Loss: 2.2840.      Train RMSE: 17137.2147, Val RMSE: 44.0129\n",
      "Epoch 10.      Train Loss: 2.4487, Val Loss: 2.0529.      Train RMSE: 157.7752, Val RMSE: 155.2556\n",
      "Epoch 15.      Train Loss: 3.1435, Val Loss: 2.9251.      Train RMSE: 31408.1111, Val RMSE: 3771.1525\n",
      "Epoch 20.      Train Loss: 2.4457, Val Loss: 2.0892.      Train RMSE: 282.9783, Val RMSE: 228.1105\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [12:43:31, 842.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:0.1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 7.9965, Val Loss: 8.6016.      Train RMSE: 3268936.6451, Val RMSE: 3268802.8814\n",
      "Epoch 5.      Train Loss: 2.4820, Val Loss: 2.2428.      Train RMSE: 52105.8639, Val RMSE: 8694.7239\n",
      "Epoch 10.      Train Loss: 7.4729, Val Loss: 8.0777.      Train RMSE: 3269011.7044, Val RMSE: 3269012.5063\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 8.1214, Val Loss: 8.7261.      Train RMSE: 3268906.6530, Val RMSE: 3268907.6153\n",
      "Epoch 5.      Train Loss: 2.5219, Val Loss: 2.4728.      Train RMSE: 47562.4161, Val RMSE: 4374.8554\n",
      "Epoch 10.      Train Loss: 2.4812, Val Loss: 2.2308.      Train RMSE: 56528.3147, Val RMSE: 13143.9665\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 3.0137, Val Loss: 2.8409.      Train RMSE: 252497.6420, Val RMSE: 214115.2880\n",
      "Epoch 5.      Train Loss: 7.5089, Val Loss: 8.1136.      Train RMSE: 3268981.7130, Val RMSE: 3269012.5063\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [12:48:05, 672.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:1\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 12.6686, Val Loss: 13.2736.      Train RMSE: 3268921.7293, Val RMSE: 3268802.8814\n",
      "Epoch 5.      Train Loss: 2.8121, Val Loss: 3.4160.      Train RMSE: 517.9571, Val RMSE: 115.5022\n",
      "Epoch 10.      Train Loss: 2.8081, Val Loss: 3.1553.      Train RMSE: 626.7453, Val RMSE: 146.4218\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 13.9501, Val Loss: 14.5545.      Train RMSE: 3268861.7445, Val RMSE: 3268802.8814\n",
      "Epoch 5.      Train Loss: 7.5284, Val Loss: 6.0372.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 10.      Train Loss: 2.8012, Val Loss: 3.1983.      Train RMSE: 725.7118, Val RMSE: 143.4726\n",
      "Epoch 15.      Train Loss: 7.3632, Val Loss: 5.8718.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 5.0277, Val Loss: 5.2115.      Train RMSE: 117365.3736, Val RMSE: 115006.1079\n",
      "Epoch 5.      Train Loss: 7.3729, Val Loss: 5.8810.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 10.      Train Loss: 2.8251, Val Loss: 3.5603.      Train RMSE: 81.8024, Val RMSE: 57.8740\n",
      "Epoch 15.      Train Loss: 2.8095, Val Loss: 3.2747.      Train RMSE: 115.0721, Val RMSE: 91.2312\n",
      "Epoch 20.      Train Loss: 2.8031, Val Loss: 3.2087.      Train RMSE: 2575.4981, Val RMSE: 170.1435\n",
      "Epoch 25.      Train Loss: 18.4937, Val Loss: 18.4937.      Train RMSE: 22616.0184, Val RMSE: 22890.7436\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [12:55:57, 612.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:3\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 18.8592, Val Loss: 19.4642.      Train RMSE: 3268936.6451, Val RMSE: 3268802.8814\n",
      "Epoch 5.      Train Loss: 7.3868, Val Loss: 5.8960.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 10.      Train Loss: 2.9485, Val Loss: 3.8625.      Train RMSE: 46.0582, Val RMSE: 30.2357\n",
      "Epoch 15.      Train Loss: 2.9636, Val Loss: 3.8672.      Train RMSE: 49.6469, Val RMSE: 34.7105\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 23.1928, Val Loss: 23.7974.      Train RMSE: 3268891.8974, Val RMSE: 3268907.6153\n",
      "Epoch 5.      Train Loss: 3.0498, Val Loss: 3.9542.      Train RMSE: 46.7751, Val RMSE: 34.4832\n",
      "Epoch 10.      Train Loss: 2.9487, Val Loss: 3.8797.      Train RMSE: 42.2306, Val RMSE: 27.4681\n",
      "Epoch 15.      Train Loss: 2.9508, Val Loss: 3.8894.      Train RMSE: 40.5996, Val RMSE: 26.3837\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 7.2922, Val Loss: 7.9384.      Train RMSE: 46781.8932, Val RMSE: 36739.8587\n",
      "Epoch 5.      Train Loss: 7.4750, Val Loss: 8.0799.      Train RMSE: 3269011.8648, Val RMSE: 3269012.5063\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [13:02:14, 541.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/3\n",
      "Epoch 1.      Train Loss: 26.4498, Val Loss: 27.0548.      Train RMSE: 3268936.6451, Val RMSE: 3268802.8814\n",
      "Epoch 5.      Train Loss: 7.4777, Val Loss: 8.0823.      Train RMSE: 3269011.3836, Val RMSE: 3269012.5063\n",
      "Epoch 10.      Train Loss: 3.0335, Val Loss: 4.0059.      Train RMSE: 35.0410, Val RMSE: 22.7617\n",
      "Epoch 15.      Train Loss: 3.0320, Val Loss: 4.0233.      Train RMSE: 31.8013, Val RMSE: 20.5744\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/3\n",
      "Epoch 1.      Train Loss: 33.6719, Val Loss: 34.2765.      Train RMSE: 3268891.8974, Val RMSE: 3268907.6153\n",
      "Epoch 5.      Train Loss: 3.0821, Val Loss: 4.0470.      Train RMSE: 34.8480, Val RMSE: 24.0935\n",
      "Epoch 10.      Train Loss: 3.0608, Val Loss: 4.0002.      Train RMSE: 37.6673, Val RMSE: 25.3910\n",
      "Epoch 15.      Train Loss: 3.0330, Val Loss: 4.0205.      Train RMSE: 31.9824, Val RMSE: 20.6636\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/3\n",
      "Epoch 1.      Train Loss: 10.4105, Val Loss: 11.2312.      Train RMSE: 44893.9387, Val RMSE: 31724.3901\n",
      "Epoch 5.      Train Loss: 2.7141, Val Loss: 1.9362.      Train RMSE: 16.7712, Val RMSE: 14.6138\n",
      "Epoch 10.      Train Loss: 3.0328, Val Loss: 4.0143.      Train RMSE: 32.9366, Val RMSE: 21.3848\n",
      "Early stopping triggered.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [13:09:00, 1479.39s/it]\n"
     ]
    }
   ],
   "source": [
    "h_list = [10]#[4, 6, 8, 10]\n",
    "batch_size_list = [128, 256]\n",
    "lr_list = [5e-5, 1e-4, 5e-4, 1e-3]\n",
    "l2_lambda_list = [0.1, 1, 3, 5]\n",
    "\n",
    "d1=source1_tensor.shape[1]  # number of features of the 1st source\n",
    "d2=source2_tensor.shape[1]  # number of features of the 2nd source\n",
    "\n",
    "# Make sure the folder exists\n",
    "output_dir = 'validation_results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for h, batch_size, lr, l2_lambda in tqdm.tqdm(itertools.product(h_list, batch_size_list, lr_list, l2_lambda_list)):\n",
    "    # if [h, batch_size, lr, l2_lambda] in existing_results:\n",
    "    #     continue\n",
    "\n",
    "    # create dataset\n",
    "    train_loader, val_loader, test_loader = create_datasets(source1_array, source2_array, target_direct_array, weight_array, batch_size, h)\n",
    "\n",
    "    print(f\"\\n📊 h: {h}, batch_size:{batch_size}, lr: {lr}, l2_lambda:{l2_lambda}\")\n",
    "    \n",
    "    ensemble, losses = train_tme_ensemble(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        d1=d1,  # number of features of the 1st source\n",
    "        d2=d2,  # number of features of the 2nd source\n",
    "        h=h,  # lag length\n",
    "        num_models=3,#20,\n",
    "        device=device,\n",
    "        lr=lr,\n",
    "        weight_decay=0.1,\n",
    "        l2_lambda=l2_lambda,\n",
    "        max_epochs=40,\n",
    "        patience=5,\n",
    "        adam=True,\n",
    "        direct_target=True\n",
    "    )\n",
    "\n",
    "    y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, val_loader, device='cpu', direct_target=True)\n",
    "\n",
    "    results_to_save = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'val_losses': losses\n",
    "    }\n",
    "\n",
    "    # Create a filename based on hyperparameters\n",
    "    filename = f\"h{h}_batch{batch_size}_lr{lr:.0e}_lambda{l2_lambda}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Save the dictionary to a JSON file\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results_to_save, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "93fc0506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 0.1, 'rmse': 1090049.2927131238, 'mae': 1090033.8120599193, 'val_losses': [2.2335792174104783, 2.2248938916648022, 2.2156353583101365]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 1.0, 'rmse': 147.9306646846928, 'mae': 91.75115358035862, 'val_losses': [3.205442184307536, 3.155019033150595, 3.1157379365358198]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 3.0, 'rmse': 1089672.4822779745, 'mae': 1089672.4821690312, 'val_losses': [3.8523679932609936, 3.8758978902316485, 3.868692558319842]}, {'h': 10, 'batch_size': 128, 'lr': 0.001, 'l2_lambda': 5.0, 'rmse': 15.735735470120613, 'mae': 5.61969919791881, 'val_losses': [3.9867418515877646, 2.1175166185731524, 3.9932443470251364]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 0.1, 'rmse': 34193.6550314017, 'mae': 1176.888619656484, 'val_losses': [1.4340720767246895, 1.4380639163685627, 2.084253795933528]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 1.0, 'rmse': 1175.4541490562478, 'mae': 79.50691012335008, 'val_losses': [1.6194059895809556, 1.643354708847941, 1.609789587435175]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 3.0, 'rmse': 66.05775328634188, 'mae': 10.88469835165486, 'val_losses': [1.7202989059637805, 1.7289260508584194, 1.724798969558028]}, {'h': 10, 'batch_size': 128, 'lr': 0.0001, 'l2_lambda': 5.0, 'rmse': 92.22405190314798, 'mae': 91.70081735724835, 'val_losses': [1.809129945445256, 1.8852881078348784, 2.0091775697274286]}, {'h': 10, 'batch_size': 128, 'lr': 0.0005, 'l2_lambda': 0.1, 'rmse': 2573.2221518943065, 'mae': 167.4842135499621, 'val_losses': [2.228704726598302, 2.236524794678219, 2.2236723667774045]}, {'h': 10, 'batch_size': 128, 'lr': 0.0005, 'l2_lambda': 1.0, 'rmse': 40.207108760103225, 'mae': 30.84473899612633, 'val_losses': [3.2421056198292093, 3.493430658442075, 2.070863493672404]}, {'h': 10, 'batch_size': 128, 'lr': 0.0005, 'l2_lambda': 3.0, 'rmse': 24.74483794754134, 'mae': 7.089829762234778, 'val_losses': [1.7247410456909509, 1.8098387199408206, 1.7274844171326669]}, {'h': 10, 'batch_size': 128, 'lr': 0.0005, 'l2_lambda': 5.0, 'rmse': 51.22676573224279, 'mae': 49.83824139368775, 'val_losses': [1.7926337028868864, 1.7927632595671983, 1.9907914875991275]}, {'h': 10, 'batch_size': 128, 'lr': 5e-05, 'l2_lambda': 0.1, 'rmse': 94752.075238218, 'mae': 4295.3475469006635, 'val_losses': [1.4559130670472247, 1.4889741511862786, 1.469656901617275]}, {'h': 10, 'batch_size': 128, 'lr': 5e-05, 'l2_lambda': 1.0, 'rmse': 1586.9076312012146, 'mae': 79.09177742672114, 'val_losses': [1.6301412224525311, 1.9280452727050077, 1.6802186030344886]}, {'h': 10, 'batch_size': 128, 'lr': 5e-05, 'l2_lambda': 3.0, 'rmse': 40.15473602606238, 'mae': 17.68114408535366, 'val_losses': [1.7360578602576842, 2.8795792064705834, 1.9476968528183758]}, {'h': 10, 'batch_size': 128, 'lr': 5e-05, 'l2_lambda': 5.0, 'rmse': 37.217877664859394, 'mae': 35.54003178688995, 'val_losses': [1.8198008092089755, 3.408201170260789, 2.020508965630023]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 0.1, 'rmse': 2179475.473094153, 'mae': 2179474.7598158633, 'val_losses': [2.242765640626188, 2.229587850512051, 2.3599100347425117]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 1.0, 'rmse': 7661.3715058678, 'mae': 7661.307339816226, 'val_losses': [3.139574842374833, 3.1983015263666874, 3.208670885836492]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 3.0, 'rmse': 1089675.1907988072, 'mae': 1089675.1906016585, 'val_losses': [3.8625384588710596, 3.8687695323443805, 3.8568349080007582]}, {'h': 10, 'batch_size': 256, 'lr': 0.001, 'l2_lambda': 5.0, 'rmse': 16.418327031711122, 'mae': 5.853721463859088, 'val_losses': [3.767888710147045, 3.9999983037104374, 1.9362438539745377]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 0.1, 'rmse': 61933.32692900896, 'mae': 2704.386243395541, 'val_losses': [1.4518052655287454, 1.5533426030248891, 1.4529323119853363]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 1.0, 'rmse': 105.44464601979183, 'mae': 55.74916763413729, 'val_losses': [1.6703491287275416, 2.5139078038637757, 2.29923791230702]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 3.0, 'rmse': 42.12649819095518, 'mae': 10.91575453363137, 'val_losses': [1.7482722812011593, 2.5364784168415384, 1.9119989671668067]}, {'h': 10, 'batch_size': 256, 'lr': 0.0001, 'l2_lambda': 5.0, 'rmse': 30.590224424826996, 'mae': 26.348724473118427, 'val_losses': [1.8007626371061216, 2.9390000163531695, 2.0060807797508162]}, {'h': 10, 'batch_size': 256, 'lr': 0.0005, 'l2_lambda': 0.1, 'rmse': 1089915.753040842, 'mae': 1089907.5836417906, 'val_losses': [2.2320382726974173, 2.2237735376006267, 2.2440939389291357]}, {'h': 10, 'batch_size': 256, 'lr': 0.0005, 'l2_lambda': 1.0, 'rmse': 370.0622634029216, 'mae': 27.31170798853765, 'val_losses': [1.6108234039217721, 3.47728355986173, 3.2662709189243]}, {'h': 10, 'batch_size': 256, 'lr': 0.0005, 'l2_lambda': 3.0, 'rmse': 33.027058415666694, 'mae': 9.768569878681967, 'val_losses': [1.7297815034379724, 2.052061457125867, 1.7190201606173985]}, {'h': 10, 'batch_size': 256, 'lr': 0.0005, 'l2_lambda': 5.0, 'rmse': 37.878813004581765, 'mae': 35.259050972563166, 'val_losses': [1.8094378482611453, 1.7900777059744617, 1.9586310770179405]}, {'h': 10, 'batch_size': 256, 'lr': 5e-05, 'l2_lambda': 0.1, 'rmse': 1089917.4202866743, 'mae': 1089871.7474610491, 'val_losses': [1.6372163864921352, 8.240880223571278, 2.2718673988443907]}, {'h': 10, 'batch_size': 256, 'lr': 5e-05, 'l2_lambda': 1.0, 'rmse': 282.2447723196523, 'mae': 78.64972506608606, 'val_losses': [2.9169614989249433, 5.30260817731013, 2.229086528058912]}, {'h': 10, 'batch_size': 256, 'lr': 5e-05, 'l2_lambda': 3.0, 'rmse': 185.9836442226922, 'mae': 28.685043031810057, 'val_losses': [4.688891344383115, 8.016703175716712, 2.3783686163484075]}, {'h': 10, 'batch_size': 256, 'lr': 5e-05, 'l2_lambda': 5.0, 'rmse': 585.5444293112988, 'mae': 20.846915035732298, 'val_losses': [5.9883367155419025, 10.691008739784115, 2.5112457578299474]}]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing your saved results\n",
    "input_dir = 'validation_results'\n",
    "\n",
    "# Prepare a list to collect all results\n",
    "all_results = []\n",
    "existing_results = []\n",
    "\n",
    "# Loop over all files in the directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        filepath = os.path.join(input_dir, filename)\n",
    "        \n",
    "        # Open and load the JSON file\n",
    "        with open(filepath, 'r') as f:\n",
    "            result = json.load(f)\n",
    "        \n",
    "        # Parse hyperparameters from the filename\n",
    "        name_parts = filename.replace('.json', '').split('_')\n",
    "        h = int(name_parts[0][1:])  # strip the 'h'\n",
    "        batch_size = int(name_parts[1][5:])  # strip 'batch'\n",
    "        lr = float(name_parts[2][2:].replace('e', 'e'))  # scientific notation stays\n",
    "        l2_lambda = float(name_parts[3][6:])  # strip 'lambda'\n",
    "\n",
    "        existing_results.append([h,batch_size,lr,l2_lambda])\n",
    "        \n",
    "        # Combine hyperparameters and results\n",
    "        entry = {\n",
    "            'h': h,\n",
    "            'batch_size': batch_size,\n",
    "            'lr': lr,\n",
    "            'l2_lambda': l2_lambda,\n",
    "            **result  # unpack the RMSE, MAE, etc.\n",
    "        }\n",
    "        all_results.append(entry)\n",
    "\n",
    "# Now `all_results` is a list of dictionaries\n",
    "print(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d4eddd2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>l2_lambda</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mae</th>\n",
       "      <th>val_losses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.573574e+01</td>\n",
       "      <td>5.619699e+00</td>\n",
       "      <td>[3.9867418515877646, 2.1175166185731524, 3.993...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.641833e+01</td>\n",
       "      <td>5.853721e+00</td>\n",
       "      <td>[3.767888710147045, 3.9999983037104374, 1.9362...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.474484e+01</td>\n",
       "      <td>7.089830e+00</td>\n",
       "      <td>[1.7247410456909509, 1.8098387199408206, 1.727...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.059022e+01</td>\n",
       "      <td>2.634872e+01</td>\n",
       "      <td>[1.8007626371061216, 2.9390000163531695, 2.006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.302706e+01</td>\n",
       "      <td>9.768570e+00</td>\n",
       "      <td>[1.7297815034379724, 2.052061457125867, 1.7190...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.721788e+01</td>\n",
       "      <td>3.554003e+01</td>\n",
       "      <td>[1.8198008092089755, 3.408201170260789, 2.0205...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.787881e+01</td>\n",
       "      <td>3.525905e+01</td>\n",
       "      <td>[1.8094378482611453, 1.7900777059744617, 1.958...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.015474e+01</td>\n",
       "      <td>1.768114e+01</td>\n",
       "      <td>[1.7360578602576842, 2.8795792064705834, 1.947...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.020711e+01</td>\n",
       "      <td>3.084474e+01</td>\n",
       "      <td>[3.2421056198292093, 3.493430658442075, 2.0708...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.212650e+01</td>\n",
       "      <td>1.091575e+01</td>\n",
       "      <td>[1.7482722812011593, 2.5364784168415384, 1.911...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.122677e+01</td>\n",
       "      <td>4.983824e+01</td>\n",
       "      <td>[1.7926337028868864, 1.7927632595671983, 1.990...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.605775e+01</td>\n",
       "      <td>1.088470e+01</td>\n",
       "      <td>[1.7202989059637805, 1.7289260508584194, 1.724...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.222405e+01</td>\n",
       "      <td>9.170082e+01</td>\n",
       "      <td>[1.809129945445256, 1.8852881078348784, 2.0091...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.054446e+02</td>\n",
       "      <td>5.574917e+01</td>\n",
       "      <td>[1.6703491287275416, 2.5139078038637757, 2.299...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.479307e+02</td>\n",
       "      <td>9.175115e+01</td>\n",
       "      <td>[3.205442184307536, 3.155019033150595, 3.11573...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.859836e+02</td>\n",
       "      <td>2.868504e+01</td>\n",
       "      <td>[4.688891344383115, 8.016703175716712, 2.37836...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.822448e+02</td>\n",
       "      <td>7.864973e+01</td>\n",
       "      <td>[2.9169614989249433, 5.30260817731013, 2.22908...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.700623e+02</td>\n",
       "      <td>2.731171e+01</td>\n",
       "      <td>[1.6108234039217721, 3.47728355986173, 3.26627...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.855444e+02</td>\n",
       "      <td>2.084692e+01</td>\n",
       "      <td>[5.9883367155419025, 10.691008739784115, 2.511...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.175454e+03</td>\n",
       "      <td>7.950691e+01</td>\n",
       "      <td>[1.6194059895809556, 1.643354708847941, 1.6097...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.586908e+03</td>\n",
       "      <td>7.909178e+01</td>\n",
       "      <td>[1.6301412224525311, 1.9280452727050077, 1.680...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.573222e+03</td>\n",
       "      <td>1.674842e+02</td>\n",
       "      <td>[2.228704726598302, 2.236524794678219, 2.22367...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.661372e+03</td>\n",
       "      <td>7.661307e+03</td>\n",
       "      <td>[3.139574842374833, 3.1983015263666874, 3.2086...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.419366e+04</td>\n",
       "      <td>1.176889e+03</td>\n",
       "      <td>[1.4340720767246895, 1.4380639163685627, 2.084...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.1</td>\n",
       "      <td>6.193333e+04</td>\n",
       "      <td>2.704386e+03</td>\n",
       "      <td>[1.4518052655287454, 1.5533426030248891, 1.452...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>9.475208e+04</td>\n",
       "      <td>4.295348e+03</td>\n",
       "      <td>[1.4559130670472247, 1.4889741511862786, 1.469...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.089672e+06</td>\n",
       "      <td>1.089672e+06</td>\n",
       "      <td>[3.8523679932609936, 3.8758978902316485, 3.868...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.089675e+06</td>\n",
       "      <td>1.089675e+06</td>\n",
       "      <td>[3.8625384588710596, 3.8687695323443805, 3.856...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.089916e+06</td>\n",
       "      <td>1.089908e+06</td>\n",
       "      <td>[2.2320382726974173, 2.2237735376006267, 2.244...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.089917e+06</td>\n",
       "      <td>1.089872e+06</td>\n",
       "      <td>[1.6372163864921352, 8.240880223571278, 2.2718...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>128</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.090049e+06</td>\n",
       "      <td>1.090034e+06</td>\n",
       "      <td>[2.2335792174104783, 2.2248938916648022, 2.215...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>10</td>\n",
       "      <td>256</td>\n",
       "      <td>0.00100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.179475e+06</td>\n",
       "      <td>2.179475e+06</td>\n",
       "      <td>[2.242765640626188, 2.229587850512051, 2.35991...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     h  batch_size       lr  l2_lambda          rmse           mae  \\\n",
       "3   10         128  0.00100        5.0  1.573574e+01  5.619699e+00   \n",
       "19  10         256  0.00100        5.0  1.641833e+01  5.853721e+00   \n",
       "10  10         128  0.00050        3.0  2.474484e+01  7.089830e+00   \n",
       "23  10         256  0.00010        5.0  3.059022e+01  2.634872e+01   \n",
       "26  10         256  0.00050        3.0  3.302706e+01  9.768570e+00   \n",
       "15  10         128  0.00005        5.0  3.721788e+01  3.554003e+01   \n",
       "27  10         256  0.00050        5.0  3.787881e+01  3.525905e+01   \n",
       "14  10         128  0.00005        3.0  4.015474e+01  1.768114e+01   \n",
       "9   10         128  0.00050        1.0  4.020711e+01  3.084474e+01   \n",
       "22  10         256  0.00010        3.0  4.212650e+01  1.091575e+01   \n",
       "11  10         128  0.00050        5.0  5.122677e+01  4.983824e+01   \n",
       "6   10         128  0.00010        3.0  6.605775e+01  1.088470e+01   \n",
       "7   10         128  0.00010        5.0  9.222405e+01  9.170082e+01   \n",
       "21  10         256  0.00010        1.0  1.054446e+02  5.574917e+01   \n",
       "1   10         128  0.00100        1.0  1.479307e+02  9.175115e+01   \n",
       "30  10         256  0.00005        3.0  1.859836e+02  2.868504e+01   \n",
       "29  10         256  0.00005        1.0  2.822448e+02  7.864973e+01   \n",
       "25  10         256  0.00050        1.0  3.700623e+02  2.731171e+01   \n",
       "31  10         256  0.00005        5.0  5.855444e+02  2.084692e+01   \n",
       "5   10         128  0.00010        1.0  1.175454e+03  7.950691e+01   \n",
       "13  10         128  0.00005        1.0  1.586908e+03  7.909178e+01   \n",
       "8   10         128  0.00050        0.1  2.573222e+03  1.674842e+02   \n",
       "17  10         256  0.00100        1.0  7.661372e+03  7.661307e+03   \n",
       "4   10         128  0.00010        0.1  3.419366e+04  1.176889e+03   \n",
       "20  10         256  0.00010        0.1  6.193333e+04  2.704386e+03   \n",
       "12  10         128  0.00005        0.1  9.475208e+04  4.295348e+03   \n",
       "2   10         128  0.00100        3.0  1.089672e+06  1.089672e+06   \n",
       "18  10         256  0.00100        3.0  1.089675e+06  1.089675e+06   \n",
       "24  10         256  0.00050        0.1  1.089916e+06  1.089908e+06   \n",
       "28  10         256  0.00005        0.1  1.089917e+06  1.089872e+06   \n",
       "0   10         128  0.00100        0.1  1.090049e+06  1.090034e+06   \n",
       "16  10         256  0.00100        0.1  2.179475e+06  2.179475e+06   \n",
       "\n",
       "                                           val_losses  \n",
       "3   [3.9867418515877646, 2.1175166185731524, 3.993...  \n",
       "19  [3.767888710147045, 3.9999983037104374, 1.9362...  \n",
       "10  [1.7247410456909509, 1.8098387199408206, 1.727...  \n",
       "23  [1.8007626371061216, 2.9390000163531695, 2.006...  \n",
       "26  [1.7297815034379724, 2.052061457125867, 1.7190...  \n",
       "15  [1.8198008092089755, 3.408201170260789, 2.0205...  \n",
       "27  [1.8094378482611453, 1.7900777059744617, 1.958...  \n",
       "14  [1.7360578602576842, 2.8795792064705834, 1.947...  \n",
       "9   [3.2421056198292093, 3.493430658442075, 2.0708...  \n",
       "22  [1.7482722812011593, 2.5364784168415384, 1.911...  \n",
       "11  [1.7926337028868864, 1.7927632595671983, 1.990...  \n",
       "6   [1.7202989059637805, 1.7289260508584194, 1.724...  \n",
       "7   [1.809129945445256, 1.8852881078348784, 2.0091...  \n",
       "21  [1.6703491287275416, 2.5139078038637757, 2.299...  \n",
       "1   [3.205442184307536, 3.155019033150595, 3.11573...  \n",
       "30  [4.688891344383115, 8.016703175716712, 2.37836...  \n",
       "29  [2.9169614989249433, 5.30260817731013, 2.22908...  \n",
       "25  [1.6108234039217721, 3.47728355986173, 3.26627...  \n",
       "31  [5.9883367155419025, 10.691008739784115, 2.511...  \n",
       "5   [1.6194059895809556, 1.643354708847941, 1.6097...  \n",
       "13  [1.6301412224525311, 1.9280452727050077, 1.680...  \n",
       "8   [2.228704726598302, 2.236524794678219, 2.22367...  \n",
       "17  [3.139574842374833, 3.1983015263666874, 3.2086...  \n",
       "4   [1.4340720767246895, 1.4380639163685627, 2.084...  \n",
       "20  [1.4518052655287454, 1.5533426030248891, 1.452...  \n",
       "12  [1.4559130670472247, 1.4889741511862786, 1.469...  \n",
       "2   [3.8523679932609936, 3.8758978902316485, 3.868...  \n",
       "18  [3.8625384588710596, 3.8687695323443805, 3.856...  \n",
       "24  [2.2320382726974173, 2.2237735376006267, 2.244...  \n",
       "28  [1.6372163864921352, 8.240880223571278, 2.2718...  \n",
       "0   [2.2335792174104783, 2.2248938916648022, 2.215...  \n",
       "16  [2.242765640626188, 2.229587850512051, 2.35991...  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(all_results).sort_values(\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc83ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "h_list = [10]#[4, 6, 8, 10]\n",
    "batch_size_list = [64, 128, 256]\n",
    "lr_list = [5e-5, 1e-4, 5e-4, 1e-3]\n",
    "l2_lambda_list = [0.1, 1, 3, 5]\n",
    "\n",
    "d1=source1_tensor.shape[1]  # number of features of the 1st source\n",
    "d2=source2_tensor.shape[1]  # number of features of the 2nd source\n",
    "\n",
    "# Make sure the folder exists\n",
    "output_dir = 'validation_results'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# printing such hyperparams combination that are not already processed\n",
    "for h, batch_size, lr, l2_lambda in tqdm.tqdm(itertools.product(h_list, batch_size_list, lr_list, l2_lambda_list)):\n",
    "    if [h, batch_size, lr, l2_lambda] in existing_results:\n",
    "        continue\n",
    "    \n",
    "    print(h, batch_size, lr, l2_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c49cbf",
   "metadata": {},
   "source": [
    "## Training final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "45bc9523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[105], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m train_loader, val_loader, test_loader \u001b[38;5;241m=\u001b[39m create_datasets(source1_array, source2_array, target_array, weight_array, batch_size, h)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📊 h: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mh\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, batch_size:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, lr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, l2_lambda:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ml2_lambda\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m ensemble, losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_tme_ensemble\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43md1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# number of features of the 1st source\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43md2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# number of features of the 2nd source\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# lag length\u001b[39;49;00m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_models\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;66;43;03m#20,\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43ml2_lambda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml2_lambda\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43madam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m y_preds, y_preds_median, rmse, mae \u001b[38;5;241m=\u001b[39m evaluate_tme_ensemble(ensemble, test_loader, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[97], line 173\u001b[0m, in \u001b[0;36mtrain_tme_ensemble\u001b[1;34m(train_loader, val_loader, d1, d2, h, num_models, device, adam, direct_target, **train_kwargs)\u001b[0m\n\u001b[0;32m    170\u001b[0m model \u001b[38;5;241m=\u001b[39m TME(d1, d2, h)  \u001b[38;5;66;03m# Initialize new model\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;66;03m# Train the model using your function\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m trained_model, best_val_loss \u001b[38;5;241m=\u001b[39m train_tme_model(\n\u001b[0;32m    174\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    175\u001b[0m     train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[0;32m    176\u001b[0m     val_loader\u001b[38;5;241m=\u001b[39mval_loader,\n\u001b[0;32m    177\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m    178\u001b[0m     adam\u001b[38;5;241m=\u001b[39madam,\n\u001b[0;32m    179\u001b[0m     direct_target\u001b[38;5;241m=\u001b[39mdirect_target,\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtrain_kwargs\n\u001b[0;32m    181\u001b[0m )\n\u001b[0;32m    183\u001b[0m \u001b[38;5;66;03m# Save the model and its validation loss\u001b[39;00m\n\u001b[0;32m    184\u001b[0m ensemble\u001b[38;5;241m.\u001b[39mappend(trained_model)\n",
      "Cell \u001b[1;32mIn[97], line 58\u001b[0m, in \u001b[0;36mtrain_tme_model\u001b[1;34m(model, train_loader, val_loader, lr, weight_decay, l2_lambda, max_epochs, patience, device, adam, direct_target)\u001b[0m\n\u001b[0;32m     52\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# train_losses = []\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# y_preds_train = []\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# y_true_train = []\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x1, x2, y, w \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     59\u001b[0m     x1, x2, y, w \u001b[38;5;241m=\u001b[39m x1\u001b[38;5;241m.\u001b[39mto(device), x2\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device), w\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     61\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\aliak\\my_space\\ETH\\ML Complex Systems\\MLFCS\\ML_fin_project\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\aliak\\my_space\\ETH\\ML Complex Systems\\MLFCS\\ML_fin_project\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\aliak\\my_space\\ETH\\ML Complex Systems\\MLFCS\\ML_fin_project\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aliak\\my_space\\ETH\\ML Complex Systems\\MLFCS\\ML_fin_project\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\aliak\\my_space\\ETH\\ML Complex Systems\\MLFCS\\ML_fin_project\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:211\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m--> 211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    212\u001b[0m         collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map)\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aliak\\my_space\\ETH\\ML Complex Systems\\MLFCS\\ML_fin_project\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\aliak\\my_space\\ETH\\ML Complex Systems\\MLFCS\\ML_fin_project\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\aliak\\my_space\\ETH\\ML Complex Systems\\MLFCS\\ML_fin_project\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "h, batch_size, lr, l2_lambda = 10, 256, 1e-3, 5\n",
    "\n",
    "train_loader, val_loader, test_loader = create_datasets(source1_array, source2_array, target_array, weight_array, batch_size, h)\n",
    "\n",
    "print(f\"\\n📊 h: {h}, batch_size:{batch_size}, lr: {lr}, l2_lambda:{l2_lambda}\")\n",
    "\n",
    "ensemble, losses = train_tme_ensemble(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    d1=d1,  # number of features of the 1st source\n",
    "    d2=d2,  # number of features of the 2nd source\n",
    "    h=h,  # lag length\n",
    "    num_models=20,#20,\n",
    "    device=device,\n",
    "    lr=lr,\n",
    "    weight_decay=0.1,\n",
    "    l2_lambda=l2_lambda,\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    adam=True\n",
    ")\n",
    "\n",
    "y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, test_loader, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa093470",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, test_loader, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "cf3d4839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106.65681468511072 89.68795543227324\n"
     ]
    }
   ],
   "source": [
    "print(rmse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "107a5f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, val_loader, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b193cf22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107.48528647251614 90.31830594075106\n"
     ]
    }
   ],
   "source": [
    "print(rmse, mae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0680fe9b",
   "metadata": {},
   "source": [
    "## Direct target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "793d0124",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/8\n",
      "Epoch 1.      Train Loss: 26.4498, Val Loss: 27.0549.      Train RMSE: 3268936.6451, Val RMSE: 3268802.8814\n",
      "Epoch 5.      Train Loss: 7.3809, Val Loss: 5.8902.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 10.      Train Loss: 2.5106, Val Loss: 2.0291.      Train RMSE: 29.9533, Val RMSE: 19.5687\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/8\n",
      "Epoch 1.      Train Loss: 33.6719, Val Loss: 34.2765.      Train RMSE: 3268891.8974, Val RMSE: 3268907.6153\n",
      "Epoch 5.      Train Loss: 3.0258, Val Loss: 3.9920.      Train RMSE: 30.6748, Val RMSE: 19.6259\n",
      "Epoch 10.      Train Loss: 2.4538, Val Loss: 1.8122.      Train RMSE: 22.5426, Val RMSE: 15.8390\n",
      "Epoch 15.      Train Loss: 2.5813, Val Loss: 2.2480.      Train RMSE: 31.5759, Val RMSE: 21.8355\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/8\n",
      "Epoch 1.      Train Loss: 10.4106, Val Loss: 11.2312.      Train RMSE: 44895.0321, Val RMSE: 31727.2839\n",
      "Epoch 5.      Train Loss: 7.3554, Val Loss: 5.8636.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 10.      Train Loss: 2.5791, Val Loss: 1.8219.      Train RMSE: 24.5154, Val RMSE: 16.5159\n",
      "Epoch 15.      Train Loss: 2.4551, Val Loss: 1.8352.      Train RMSE: 24.3766, Val RMSE: 17.0693\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 4/8\n",
      "Epoch 1.      Train Loss: 14.5321, Val Loss: 15.4715.      Train RMSE: 27565.8386, Val RMSE: 10528.8016\n",
      "Epoch 5.      Train Loss: 6.9097, Val Loss: 5.1725.      Train RMSE: 17.4273, Val RMSE: 15.0681\n",
      "Epoch 10.      Train Loss: 2.5982, Val Loss: 2.2969.      Train RMSE: 25.8498, Val RMSE: 17.9930\n",
      "Epoch 15.      Train Loss: 2.6881, Val Loss: 1.8924.      Train RMSE: 18.9557, Val RMSE: 14.3548\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 5/8\n",
      "Epoch 1.      Train Loss: 22.3720, Val Loss: 22.7952.      Train RMSE: 82156.9293, Val RMSE: 65874.6018\n",
      "Epoch 5.      Train Loss: 3.3165, Val Loss: 2.3010.      Train RMSE: 17.2491, Val RMSE: 14.0967\n",
      "Epoch 10.      Train Loss: 2.4508, Val Loss: 1.8293.      Train RMSE: 23.0539, Val RMSE: 16.2584\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 6/8\n",
      "Epoch 1.      Train Loss: 9.8526, Val Loss: 10.4976.      Train RMSE: 18890.3438, Val RMSE: 261.1970\n",
      "Epoch 5.      Train Loss: 2.4584, Val Loss: 1.7955.      Train RMSE: 24.2415, Val RMSE: 16.6701\n",
      "Epoch 10.      Train Loss: 2.8240, Val Loss: 1.9894.      Train RMSE: 18.0782, Val RMSE: 14.0542\n",
      "Epoch 15.      Train Loss: 2.4496, Val Loss: 1.8360.      Train RMSE: 23.8129, Val RMSE: 16.5881\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 7/8\n",
      "Epoch 1.      Train Loss: 23.6660, Val Loss: 24.4366.      Train RMSE: 503890.5461, Val RMSE: 477770.3522\n",
      "Epoch 5.      Train Loss: 3.0945, Val Loss: 4.1490.      Train RMSE: 19.7060, Val RMSE: 15.9019\n",
      "Epoch 10.      Train Loss: 2.5374, Val Loss: 1.8098.      Train RMSE: 22.2232, Val RMSE: 15.6170\n",
      "Epoch 15.      Train Loss: 2.4709, Val Loss: 1.9121.      Train RMSE: 25.5710, Val RMSE: 17.5100\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 8/8\n",
      "Epoch 1.      Train Loss: 41.4516, Val Loss: 39.9603.      Train RMSE: 19788.2244, Val RMSE: 26178.1099\n",
      "Epoch 5.      Train Loss: 2.6134, Val Loss: 2.3402.      Train RMSE: 25.2238, Val RMSE: 17.5577\n",
      "Epoch 10.      Train Loss: 2.4588, Val Loss: 1.7987.      Train RMSE: 21.4602, Val RMSE: 15.4423\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "h, batch_size, lr, l2_lambda = 10, 256, 1e-3, 5\n",
    "\n",
    "train_loader, val_loader, test_loader = create_datasets(source1_array, source2_array, target_direct_array, weight_array, batch_size, h)\n",
    "\n",
    "print(f\"\\n📊 h: {h}, batch_size:{batch_size}, lr: {lr}, l2_lambda:{l2_lambda}\")\n",
    "\n",
    "ensemble, losses = train_tme_ensemble(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    d1=d1,  # number of features of the 1st source\n",
    "    d2=d2,  # number of features of the 2nd source\n",
    "    h=h,  # lag length\n",
    "    num_models=8,#20,\n",
    "    device=device,\n",
    "    lr=lr,\n",
    "    weight_decay=0.1,\n",
    "    l2_lambda=l2_lambda,\n",
    "    max_epochs=100,\n",
    "    patience=10,\n",
    "    adam=True,\n",
    "    direct_target=True\n",
    ")\n",
    "\n",
    "y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, test_loader, device='cpu', direct_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9e029113",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1733.741818863391 1733.6873583279287\n"
     ]
    }
   ],
   "source": [
    "print(rmse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87a069e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 h: 10, batch_size:256, lr: 0.001, l2_lambda:5\n",
      "\n",
      "🌱 Training ensemble model 1/20\n",
      "Epoch 1.      Train Loss: 26.4498, Val Loss: 27.0548.      Train RMSE: 3268936.6451, Val RMSE: 3268802.8814\n",
      "Epoch 5.      Train Loss: 7.4777, Val Loss: 8.0823.      Train RMSE: 3269011.3836, Val RMSE: 3269012.5063\n",
      "Epoch 10.      Train Loss: 3.0335, Val Loss: 4.0059.      Train RMSE: 35.0410, Val RMSE: 22.7617\n",
      "Epoch 15.      Train Loss: 3.0320, Val Loss: 4.0233.      Train RMSE: 31.8013, Val RMSE: 20.5744\n",
      "Epoch 20.      Train Loss: 7.3631, Val Loss: 5.8706.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 25.      Train Loss: 4.0139, Val Loss: 2.7961.      Train RMSE: 16.3452, Val RMSE: 14.0884\n",
      "Epoch 30.      Train Loss: 3.0332, Val Loss: 4.0308.      Train RMSE: 30.3432, Val RMSE: 19.7020\n",
      "Epoch 35.      Train Loss: 3.0329, Val Loss: 4.0135.      Train RMSE: 33.3686, Val RMSE: 21.5440\n",
      "Epoch 40.      Train Loss: 3.0321, Val Loss: 4.0121.      Train RMSE: 33.7835, Val RMSE: 21.8402\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 2/20\n",
      "Epoch 1.      Train Loss: 33.6719, Val Loss: 34.2765.      Train RMSE: 3268891.8974, Val RMSE: 3268907.6153\n",
      "Epoch 5.      Train Loss: 3.0821, Val Loss: 4.0470.      Train RMSE: 34.8480, Val RMSE: 24.0935\n",
      "Epoch 10.      Train Loss: 3.0608, Val Loss: 4.0002.      Train RMSE: 37.6673, Val RMSE: 25.3910\n",
      "Epoch 15.      Train Loss: 3.0330, Val Loss: 4.0205.      Train RMSE: 31.9824, Val RMSE: 20.6636\n",
      "Epoch 20.      Train Loss: 3.3388, Val Loss: 4.3916.      Train RMSE: 16.8435, Val RMSE: 14.5307\n",
      "Epoch 25.      Train Loss: 3.0337, Val Loss: 4.0206.      Train RMSE: 32.1577, Val RMSE: 20.7441\n",
      "Epoch 30.      Train Loss: 3.0326, Val Loss: 4.0148.      Train RMSE: 33.0880, Val RMSE: 21.4325\n",
      "Epoch 35.      Train Loss: 7.5398, Val Loss: 8.1443.      Train RMSE: 3269011.7044, Val RMSE: 3269012.5063\n",
      "Epoch 40.      Train Loss: 3.0328, Val Loss: 4.0197.      Train RMSE: 32.3471, Val RMSE: 20.9484\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 3/20\n",
      "Epoch 1.      Train Loss: 10.4105, Val Loss: 11.2312.      Train RMSE: 44893.9387, Val RMSE: 31724.3901\n",
      "Epoch 5.      Train Loss: 2.7141, Val Loss: 1.9362.      Train RMSE: 16.7712, Val RMSE: 14.6138\n",
      "Epoch 10.      Train Loss: 3.0328, Val Loss: 4.0143.      Train RMSE: 32.9366, Val RMSE: 21.3848\n",
      "Epoch 15.      Train Loss: 3.0340, Val Loss: 4.0320.      Train RMSE: 29.7487, Val RMSE: 19.3812\n",
      "Epoch 20.      Train Loss: 7.3592, Val Loss: 5.8701.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 25.      Train Loss: 3.0342, Val Loss: 4.0064.      Train RMSE: 35.1723, Val RMSE: 22.7820\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 4/20\n",
      "Epoch 1.      Train Loss: 14.5321, Val Loss: 15.4715.      Train RMSE: 27565.8595, Val RMSE: 10528.8115\n",
      "Epoch 5.      Train Loss: 7.4784, Val Loss: 8.0832.      Train RMSE: 3269011.8648, Val RMSE: 3269012.5063\n",
      "Epoch 10.      Train Loss: 3.0784, Val Loss: 3.9877.      Train RMSE: 42.6695, Val RMSE: 29.4771\n",
      "Epoch 15.      Train Loss: 3.0374, Val Loss: 4.0364.      Train RMSE: 30.0027, Val RMSE: 19.6415\n",
      "Epoch 20.      Train Loss: 2.4745, Val Loss: 1.8485.      Train RMSE: 32.8937, Val RMSE: 21.0810\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 5/20\n",
      "Epoch 1.      Train Loss: 22.3720, Val Loss: 22.7952.      Train RMSE: 82156.8950, Val RMSE: 65874.5902\n",
      "Epoch 5.      Train Loss: 3.0340, Val Loss: 4.0204.      Train RMSE: 32.2558, Val RMSE: 20.8643\n",
      "Epoch 10.      Train Loss: 3.0343, Val Loss: 3.9990.      Train RMSE: 36.3810, Val RMSE: 23.6462\n",
      "Epoch 15.      Train Loss: 3.0330, Val Loss: 4.0041.      Train RMSE: 35.5728, Val RMSE: 23.0344\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 6/20\n",
      "Epoch 1.      Train Loss: 9.8526, Val Loss: 10.4976.      Train RMSE: 18890.3683, Val RMSE: 261.1979\n",
      "Epoch 5.      Train Loss: 7.3644, Val Loss: 5.8737.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 10.      Train Loss: 3.0323, Val Loss: 4.0218.      Train RMSE: 31.6783, Val RMSE: 20.5044\n",
      "Epoch 15.      Train Loss: 3.0326, Val Loss: 4.0186.      Train RMSE: 32.3312, Val RMSE: 20.8727\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 7/20\n",
      "Epoch 1.      Train Loss: 23.6660, Val Loss: 24.4366.      Train RMSE: 503890.4485, Val RMSE: 477770.2836\n",
      "Epoch 5.      Train Loss: 3.0430, Val Loss: 3.9908.      Train RMSE: 39.8043, Val RMSE: 26.4234\n",
      "Epoch 10.      Train Loss: 3.0357, Val Loss: 3.9923.      Train RMSE: 37.9241, Val RMSE: 24.6570\n",
      "Epoch 15.      Train Loss: 3.0324, Val Loss: 4.0107.      Train RMSE: 33.8942, Val RMSE: 21.9708\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 8/20\n",
      "Epoch 1.      Train Loss: 41.4516, Val Loss: 39.9603.      Train RMSE: 19788.2244, Val RMSE: 26178.1099\n",
      "Epoch 5.      Train Loss: 7.5217, Val Loss: 8.1264.      Train RMSE: 3269011.7044, Val RMSE: 3269012.5063\n",
      "Epoch 10.      Train Loss: 3.0330, Val Loss: 4.0176.      Train RMSE: 32.5496, Val RMSE: 20.9917\n",
      "Epoch 15.      Train Loss: 7.3562, Val Loss: 5.8656.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 20.      Train Loss: 3.0360, Val Loss: 4.0008.      Train RMSE: 34.6494, Val RMSE: 22.5341\n",
      "Epoch 25.      Train Loss: 7.3577, Val Loss: 5.8667.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 9/20\n",
      "Epoch 1.      Train Loss: 21.4545, Val Loss: 21.8798.      Train RMSE: 63696.6551, Val RMSE: 78320.1455\n",
      "Epoch 5.      Train Loss: 3.0531, Val Loss: 4.0242.      Train RMSE: 33.8618, Val RMSE: 22.4649\n",
      "Epoch 10.      Train Loss: 3.0350, Val Loss: 4.0342.      Train RMSE: 29.7648, Val RMSE: 19.4509\n",
      "Epoch 15.      Train Loss: 5.5075, Val Loss: 3.5737.      Train RMSE: 17.4092, Val RMSE: 15.0181\n",
      "Epoch 20.      Train Loss: 4.6868, Val Loss: 3.0679.      Train RMSE: 17.3694, Val RMSE: 14.9361\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 10/20\n",
      "Epoch 1.      Train Loss: 17.1013, Val Loss: 17.6060.      Train RMSE: 81232.1970, Val RMSE: 77387.2217\n",
      "Epoch 5.      Train Loss: 3.1650, Val Loss: 4.2402.      Train RMSE: 17.1816, Val RMSE: 15.1929\n",
      "Epoch 10.      Train Loss: 3.0321, Val Loss: 4.0200.      Train RMSE: 32.1573, Val RMSE: 20.7955\n",
      "Epoch 15.      Train Loss: 7.4874, Val Loss: 8.0918.      Train RMSE: 3269011.3836, Val RMSE: 3269012.5063\n",
      "Epoch 20.      Train Loss: 7.3627, Val Loss: 5.8725.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 25.      Train Loss: 3.0351, Val Loss: 4.0096.      Train RMSE: 34.4186, Val RMSE: 22.2100\n",
      "Epoch 30.      Train Loss: 3.0324, Val Loss: 4.0072.      Train RMSE: 34.7785, Val RMSE: 22.4365\n",
      "Epoch 35.      Train Loss: 3.0338, Val Loss: 4.0316.      Train RMSE: 30.0459, Val RMSE: 19.5680\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 11/20\n",
      "Epoch 1.      Train Loss: 19.9265, Val Loss: 20.4553.      Train RMSE: 73326.9614, Val RMSE: 48011.7479\n",
      "Epoch 5.      Train Loss: 3.0327, Val Loss: 4.0143.      Train RMSE: 33.4820, Val RMSE: 21.6568\n",
      "Epoch 10.      Train Loss: 3.0336, Val Loss: 4.0037.      Train RMSE: 35.2809, Val RMSE: 22.8193\n",
      "Epoch 15.      Train Loss: 3.0331, Val Loss: 4.0173.      Train RMSE: 32.7503, Val RMSE: 21.1776\n",
      "Epoch 20.      Train Loss: 3.0336, Val Loss: 4.0267.      Train RMSE: 31.1077, Val RMSE: 20.1952\n",
      "Epoch 25.      Train Loss: 7.3540, Val Loss: 5.8637.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 30.      Train Loss: 7.3532, Val Loss: 5.8631.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 12/20\n",
      "Epoch 1.      Train Loss: 28.0534, Val Loss: 28.6566.      Train RMSE: 3268472.6188, Val RMSE: 3268697.9837\n",
      "Epoch 5.      Train Loss: 3.0404, Val Loss: 4.0427.      Train RMSE: 29.5740, Val RMSE: 19.4222\n",
      "Epoch 10.      Train Loss: 3.0331, Val Loss: 4.0108.      Train RMSE: 33.7563, Val RMSE: 21.8594\n",
      "Epoch 15.      Train Loss: 3.0339, Val Loss: 3.9999.      Train RMSE: 36.2021, Val RMSE: 23.5676\n",
      "Epoch 20.      Train Loss: 3.0326, Val Loss: 4.0264.      Train RMSE: 31.2366, Val RMSE: 20.1236\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 13/20\n",
      "Epoch 1.      Train Loss: 8.5522, Val Loss: 7.3681.      Train RMSE: 79.6245, Val RMSE: 14.4543\n",
      "Epoch 5.      Train Loss: 3.0330, Val Loss: 4.0075.      Train RMSE: 34.8999, Val RMSE: 22.5699\n",
      "Epoch 10.      Train Loss: 3.0654, Val Loss: 3.9953.      Train RMSE: 38.9029, Val RMSE: 26.8323\n",
      "Epoch 15.      Train Loss: 3.0333, Val Loss: 4.0254.      Train RMSE: 31.1663, Val RMSE: 20.1840\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 14/20\n",
      "Epoch 1.      Train Loss: 36.9005, Val Loss: 36.9005.      Train RMSE: 22020.9900, Val RMSE: 22021.9251\n",
      "Epoch 5.      Train Loss: 3.0330, Val Loss: 4.0115.      Train RMSE: 33.8300, Val RMSE: 21.8677\n",
      "Epoch 10.      Train Loss: 3.0345, Val Loss: 4.0319.      Train RMSE: 29.9639, Val RMSE: 19.5110\n",
      "Epoch 15.      Train Loss: 7.3309, Val Loss: 5.6318.      Train RMSE: 17.4282, Val RMSE: 15.0696\n",
      "Epoch 20.      Train Loss: 7.3583, Val Loss: 5.8674.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 25.      Train Loss: 7.4757, Val Loss: 8.0792.      Train RMSE: 3269006.7326, Val RMSE: 3268992.6190\n",
      "Epoch 30.      Train Loss: 16.9504, Val Loss: 15.2986.      Train RMSE: 17.4305, Val RMSE: 15.0725\n",
      "Epoch 35.      Train Loss: 3.0603, Val Loss: 4.0987.      Train RMSE: 22.7571, Val RMSE: 16.2670\n",
      "Epoch 40.      Train Loss: 3.0346, Val Loss: 4.0191.      Train RMSE: 31.4809, Val RMSE: 20.1797\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 15/20\n",
      "Epoch 1.      Train Loss: 14.7497, Val Loss: 15.3578.      Train RMSE: 166200.5697, Val RMSE: 168233.7848\n",
      "Epoch 5.      Train Loss: 3.0328, Val Loss: 4.0118.      Train RMSE: 33.6094, Val RMSE: 21.7839\n",
      "Epoch 10.      Train Loss: 3.0366, Val Loss: 4.0292.      Train RMSE: 30.9691, Val RMSE: 20.1501\n",
      "Epoch 15.      Train Loss: 3.0542, Val Loss: 3.9824.      Train RMSE: 40.9829, Val RMSE: 28.0236\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 16/20\n",
      "Epoch 1.      Train Loss: 32.4199, Val Loss: 30.9310.      Train RMSE: 9894.4922, Val RMSE: 15.0702\n",
      "Epoch 5.      Train Loss: 7.3553, Val Loss: 5.8640.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 10.      Train Loss: 7.4622, Val Loss: 5.9720.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 15.      Train Loss: 7.4745, Val Loss: 8.0790.      Train RMSE: 3269011.3836, Val RMSE: 3269012.5063\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 17/20\n",
      "Epoch 1.      Train Loss: 36.1168, Val Loss: 36.1160.      Train RMSE: 22020.7902, Val RMSE: 22021.2181\n",
      "Epoch 5.      Train Loss: 3.0476, Val Loss: 4.0306.      Train RMSE: 32.9402, Val RMSE: 21.3014\n",
      "Epoch 10.      Train Loss: 3.0322, Val Loss: 4.0218.      Train RMSE: 31.6123, Val RMSE: 20.4872\n",
      "Epoch 15.      Train Loss: 3.0352, Val Loss: 3.9951.      Train RMSE: 37.6083, Val RMSE: 24.3535\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 18/20\n",
      "Epoch 1.      Train Loss: 18.4140, Val Loss: 18.8320.      Train RMSE: 236422.3843, Val RMSE: 178202.4233\n",
      "Epoch 5.      Train Loss: 3.0320, Val Loss: 4.0157.      Train RMSE: 32.8029, Val RMSE: 21.2419\n",
      "Epoch 10.      Train Loss: 3.0337, Val Loss: 4.0061.      Train RMSE: 34.9144, Val RMSE: 22.7072\n",
      "Epoch 15.      Train Loss: 3.0322, Val Loss: 4.0226.      Train RMSE: 31.3056, Val RMSE: 20.2297\n",
      "Epoch 20.      Train Loss: 3.0327, Val Loss: 4.0049.      Train RMSE: 34.8780, Val RMSE: 22.6616\n",
      "Epoch 25.      Train Loss: 2.4831, Val Loss: 1.8137.      Train RMSE: 26.5845, Val RMSE: 17.5215\n",
      "Epoch 30.      Train Loss: 3.0360, Val Loss: 4.0199.      Train RMSE: 32.6961, Val RMSE: 21.1171\n",
      "Epoch 35.      Train Loss: 3.0329, Val Loss: 4.0189.      Train RMSE: 32.3684, Val RMSE: 20.9470\n",
      "Epoch 40.      Train Loss: 7.4752, Val Loss: 8.0797.      Train RMSE: 3269011.7044, Val RMSE: 3269012.5063\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 19/20\n",
      "Epoch 1.      Train Loss: 26.3459, Val Loss: 26.9866.      Train RMSE: 52469.8624, Val RMSE: 53658.6092\n",
      "Epoch 5.      Train Loss: 2.7318, Val Loss: 2.8425.      Train RMSE: 32.9957, Val RMSE: 21.3082\n",
      "Epoch 10.      Train Loss: 7.3929, Val Loss: 5.9005.      Train RMSE: 17.4283, Val RMSE: 15.0702\n",
      "Epoch 15.      Train Loss: 3.0326, Val Loss: 4.0075.      Train RMSE: 34.3851, Val RMSE: 22.3249\n",
      "Epoch 20.      Train Loss: 3.0336, Val Loss: 4.0209.      Train RMSE: 32.1052, Val RMSE: 20.8245\n",
      "Early stopping triggered.\n",
      "\n",
      "🌱 Training ensemble model 20/20\n",
      "Epoch 1.      Train Loss: 37.3269, Val Loss: 37.3273.      Train RMSE: 22020.4872, Val RMSE: 22021.9251\n",
      "Epoch 5.      Train Loss: 3.1351, Val Loss: 4.1799.      Train RMSE: 929.5801, Val RMSE: 66.9758\n",
      "Epoch 10.      Train Loss: 3.0326, Val Loss: 4.0188.      Train RMSE: 31.8575, Val RMSE: 20.6690\n",
      "Epoch 15.      Train Loss: 3.0338, Val Loss: 4.0340.      Train RMSE: 29.6181, Val RMSE: 19.4335\n",
      "Epoch 20.      Train Loss: 7.4767, Val Loss: 8.0813.      Train RMSE: 3269011.3836, Val RMSE: 3269012.5063\n",
      "Epoch 25.      Train Loss: 3.4321, Val Loss: 4.4950.      Train RMSE: 16.9187, Val RMSE: 14.5809\n",
      "Epoch 30.      Train Loss: 3.0334, Val Loss: 4.0210.      Train RMSE: 32.1200, Val RMSE: 20.6878\n",
      "Epoch 35.      Train Loss: 3.0325, Val Loss: 4.0030.      Train RMSE: 35.3681, Val RMSE: 23.0096\n",
      "Epoch 40.      Train Loss: 3.0327, Val Loss: 4.0190.      Train RMSE: 32.3009, Val RMSE: 20.8286\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "h, batch_size, lr, l2_lambda = 10, 256, 1e-3, 5\n",
    "\n",
    "train_loader, val_loader, test_loader = create_datasets(source1_array, source2_array, target_direct_array, weight_array, batch_size, h)\n",
    "\n",
    "d1=source1_tensor.shape[1]  # number of features of the 1st source\n",
    "d2=source2_tensor.shape[1]  # number of features of the 2nd source\n",
    "\n",
    "print(f\"\\n📊 h: {h}, batch_size:{batch_size}, lr: {lr}, l2_lambda:{l2_lambda}\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "ensemble, losses = train_tme_ensemble(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    d1=d1,  # number of features of the 1st source\n",
    "    d2=d2,  # number of features of the 2nd source\n",
    "    h=h,  # lag length\n",
    "    num_models=20,#20,\n",
    "    device=device,\n",
    "    lr=lr,\n",
    "    weight_decay=0.1,\n",
    "    l2_lambda=l2_lambda,\n",
    "    max_epochs=100,\n",
    "    patience=15,\n",
    "    adam=True,\n",
    "    direct_target=True\n",
    ")\n",
    "\n",
    "y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, test_loader, device='cpu', direct_target=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "447bb13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.02415103234464 5.254571181093764\n"
     ]
    }
   ],
   "source": [
    "print(rmse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "03610308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.311172156212304 8.250561849001937\n"
     ]
    }
   ],
   "source": [
    "print(rmse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ecb3109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.311172156212304 8.250561849001937\n"
     ]
    }
   ],
   "source": [
    "print(rmse, mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec042c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_fin_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
