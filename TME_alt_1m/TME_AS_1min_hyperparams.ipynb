{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20b0a2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import sys\n",
    "import numpy as np\n",
    "from numpy import cumsum, log, polyfit, sqrt, std, subtract\n",
    "from numpy.random import randn\n",
    "import pandas as pd\n",
    "# from pandas_datareader import data as web\n",
    "import seaborn as sns\n",
    "from pylab import rcParams \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from arch import arch_model\n",
    "from numpy.linalg import LinAlgError\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.tsa.api as tsa\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, q_stat, adfuller\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import probplot, moment\n",
    "from arch import arch_model\n",
    "from arch.univariate import ConstantMean, GARCH, Normal\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from itertools import product\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import itertools\n",
    "import json\n",
    "import os\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4b333be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_txn_data, preprocess_txn_data, compute_lob_features, create_lob_dataset, merge_txn_and_lob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fff05f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"darkgrid\", color_codes=True)\n",
    "rcParams['figure.figsize'] = 8,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "153d5254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd9a0999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trx Data loaded successfully.\n",
      "preprocessed lob Data loaded successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>buy_volume</th>\n",
       "      <th>sell_volume</th>\n",
       "      <th>buy_txn</th>\n",
       "      <th>sell_txn</th>\n",
       "      <th>volume_imbalance</th>\n",
       "      <th>txn_imbalance</th>\n",
       "      <th>total_volume</th>\n",
       "      <th>mean_volume</th>\n",
       "      <th>deseasoned_total_volume</th>\n",
       "      <th>log_deseasoned_total_volume</th>\n",
       "      <th>ask_volume</th>\n",
       "      <th>bid_volume</th>\n",
       "      <th>ask_slope_1</th>\n",
       "      <th>ask_slope_5</th>\n",
       "      <th>ask_slope_10</th>\n",
       "      <th>bid_slope_1</th>\n",
       "      <th>bid_slope_5</th>\n",
       "      <th>bid_slope_10</th>\n",
       "      <th>spread</th>\n",
       "      <th>lob_volume_imbalance</th>\n",
       "      <th>slope_imbalance_1</th>\n",
       "      <th>slope_imbalance_5</th>\n",
       "      <th>slope_imbalance_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5819</th>\n",
       "      <td>2018-06-04 22:00:05+00:00</td>\n",
       "      <td>0.059804</td>\n",
       "      <td>0.730357</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.670553</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.790162</td>\n",
       "      <td>4.380444</td>\n",
       "      <td>0.180384</td>\n",
       "      <td>-1.712667</td>\n",
       "      <td>2695.804973</td>\n",
       "      <td>586356.113693</td>\n",
       "      <td>1761.630667</td>\n",
       "      <td>2695.804973</td>\n",
       "      <td>2695.804973</td>\n",
       "      <td>3.972121</td>\n",
       "      <td>53.502450</td>\n",
       "      <td>160.246934</td>\n",
       "      <td>6.19</td>\n",
       "      <td>583660.308720</td>\n",
       "      <td>1757.658546</td>\n",
       "      <td>2642.302523</td>\n",
       "      <td>2535.558040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5820</th>\n",
       "      <td>2018-06-04 22:01:05+00:00</td>\n",
       "      <td>0.089359</td>\n",
       "      <td>0.849477</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.760118</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.938836</td>\n",
       "      <td>3.692009</td>\n",
       "      <td>0.254289</td>\n",
       "      <td>-1.369285</td>\n",
       "      <td>2699.165417</td>\n",
       "      <td>586350.938081</td>\n",
       "      <td>1765.312385</td>\n",
       "      <td>2699.165417</td>\n",
       "      <td>2699.165417</td>\n",
       "      <td>4.017044</td>\n",
       "      <td>52.408273</td>\n",
       "      <td>155.071322</td>\n",
       "      <td>4.97</td>\n",
       "      <td>583651.772664</td>\n",
       "      <td>1761.295341</td>\n",
       "      <td>2646.757144</td>\n",
       "      <td>2544.094095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5821</th>\n",
       "      <td>2018-06-04 22:02:05+00:00</td>\n",
       "      <td>0.313458</td>\n",
       "      <td>0.508952</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.195494</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.822410</td>\n",
       "      <td>3.324900</td>\n",
       "      <td>0.247349</td>\n",
       "      <td>-1.396955</td>\n",
       "      <td>2657.946212</td>\n",
       "      <td>586317.596946</td>\n",
       "      <td>1723.843180</td>\n",
       "      <td>2657.946212</td>\n",
       "      <td>2657.946212</td>\n",
       "      <td>3.831055</td>\n",
       "      <td>46.578294</td>\n",
       "      <td>158.194750</td>\n",
       "      <td>4.90</td>\n",
       "      <td>583659.650734</td>\n",
       "      <td>1720.012125</td>\n",
       "      <td>2611.367918</td>\n",
       "      <td>2499.751462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5822</th>\n",
       "      <td>2018-06-04 22:03:05+00:00</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.199219</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.198227</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.200211</td>\n",
       "      <td>4.128645</td>\n",
       "      <td>0.048493</td>\n",
       "      <td>-3.026331</td>\n",
       "      <td>2650.599402</td>\n",
       "      <td>586308.612876</td>\n",
       "      <td>1718.061157</td>\n",
       "      <td>2650.599402</td>\n",
       "      <td>2650.599402</td>\n",
       "      <td>3.631836</td>\n",
       "      <td>51.036074</td>\n",
       "      <td>160.641345</td>\n",
       "      <td>4.32</td>\n",
       "      <td>583658.013474</td>\n",
       "      <td>1714.429321</td>\n",
       "      <td>2599.563327</td>\n",
       "      <td>2489.958056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5823</th>\n",
       "      <td>2018-06-04 22:04:05+00:00</td>\n",
       "      <td>0.172042</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.172042</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.172042</td>\n",
       "      <td>6.271124</td>\n",
       "      <td>0.027434</td>\n",
       "      <td>-3.595966</td>\n",
       "      <td>2650.082079</td>\n",
       "      <td>586314.173248</td>\n",
       "      <td>1715.979046</td>\n",
       "      <td>2650.082079</td>\n",
       "      <td>2650.082079</td>\n",
       "      <td>3.704804</td>\n",
       "      <td>51.092926</td>\n",
       "      <td>160.489197</td>\n",
       "      <td>4.32</td>\n",
       "      <td>583664.091169</td>\n",
       "      <td>1712.274243</td>\n",
       "      <td>2598.989153</td>\n",
       "      <td>2489.592882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      datetime  buy_volume  sell_volume  buy_txn  sell_txn  \\\n",
       "5819 2018-06-04 22:00:05+00:00    0.059804     0.730357      5.0      10.0   \n",
       "5820 2018-06-04 22:01:05+00:00    0.089359     0.849477      3.0       4.0   \n",
       "5821 2018-06-04 22:02:05+00:00    0.313458     0.508952      2.0       4.0   \n",
       "5822 2018-06-04 22:03:05+00:00    0.000992     0.199219      1.0       4.0   \n",
       "5823 2018-06-04 22:04:05+00:00    0.172042     0.000000      7.0       0.0   \n",
       "\n",
       "      volume_imbalance  txn_imbalance  total_volume  mean_volume  \\\n",
       "5819          0.670553            5.0      0.790162     4.380444   \n",
       "5820          0.760118            1.0      0.938836     3.692009   \n",
       "5821          0.195494            2.0      0.822410     3.324900   \n",
       "5822          0.198227            3.0      0.200211     4.128645   \n",
       "5823          0.172042            7.0      0.172042     6.271124   \n",
       "\n",
       "      deseasoned_total_volume  log_deseasoned_total_volume   ask_volume  \\\n",
       "5819                 0.180384                    -1.712667  2695.804973   \n",
       "5820                 0.254289                    -1.369285  2699.165417   \n",
       "5821                 0.247349                    -1.396955  2657.946212   \n",
       "5822                 0.048493                    -3.026331  2650.599402   \n",
       "5823                 0.027434                    -3.595966  2650.082079   \n",
       "\n",
       "         bid_volume  ask_slope_1  ask_slope_5  ask_slope_10  bid_slope_1  \\\n",
       "5819  586356.113693  1761.630667  2695.804973   2695.804973     3.972121   \n",
       "5820  586350.938081  1765.312385  2699.165417   2699.165417     4.017044   \n",
       "5821  586317.596946  1723.843180  2657.946212   2657.946212     3.831055   \n",
       "5822  586308.612876  1718.061157  2650.599402   2650.599402     3.631836   \n",
       "5823  586314.173248  1715.979046  2650.082079   2650.082079     3.704804   \n",
       "\n",
       "      bid_slope_5  bid_slope_10  spread  lob_volume_imbalance  \\\n",
       "5819    53.502450    160.246934    6.19         583660.308720   \n",
       "5820    52.408273    155.071322    4.97         583651.772664   \n",
       "5821    46.578294    158.194750    4.90         583659.650734   \n",
       "5822    51.036074    160.641345    4.32         583658.013474   \n",
       "5823    51.092926    160.489197    4.32         583664.091169   \n",
       "\n",
       "      slope_imbalance_1  slope_imbalance_5  slope_imbalance_10  \n",
       "5819        1757.658546        2642.302523         2535.558040  \n",
       "5820        1761.295341        2646.757144         2544.094095  \n",
       "5821        1720.012125        2611.367918         2499.751462  \n",
       "5822        1714.429321        2599.563327         2489.958056  \n",
       "5823        1712.274243        2598.989153         2489.592882  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trx_df = read_txn_data(use_load=False)\n",
    "trx_df = preprocess_txn_data(trx_df, freq='1min')\n",
    "trx_df['log_deseasoned_total_volume'] = np.log(trx_df['deseasoned_total_volume'] + 1e-07)\n",
    "\n",
    "lob_df = create_lob_dataset(use_load=False)\n",
    "\n",
    "df_merged = merge_txn_and_lob(trx_df, lob_df)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec60b8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "130fc55d",
   "metadata": {},
   "source": [
    "## TME implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b543942",
   "metadata": {},
   "source": [
    "### Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0222567",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 10  # window length\n",
    "batch_size = 128\n",
    "\n",
    "# -----------------------------\n",
    "df = df_merged.sort_values('datetime').reset_index(drop=True)\n",
    "# STEP 1: Create time-of-day feature\n",
    "df['time_of_day'] = df['datetime'].dt.strftime('%H:%M')\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 2: Split indices (AFTER creating lags!)\n",
    "n_total = len(df)\n",
    "n_train = int(0.7 * n_total)\n",
    "n_val = int(0.1 * n_total)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 3: Create deseasonalizing map using per-time volume means from train only\n",
    "train_deseason_df = df.iloc[:n_train]\n",
    "mean_volume_by_time = train_deseason_df.groupby('time_of_day')['total_volume'].mean()\n",
    "df['mean_volume'] = df['time_of_day'].map(mean_volume_by_time)\n",
    "\n",
    "df['deseasoned_total_volume'] = df['total_volume'] / df['mean_volume']\n",
    "df['log_deseasoned_total_volume'] = np.log(df['deseasoned_total_volume'] + 1e-7)\n",
    "df['target'] = df['deseasoned_total_volume']\n",
    "\n",
    "del train_deseason_df\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 4: Define the source-specific features\n",
    "source1_cols = ['buy_volume', 'sell_volume', 'buy_txn', 'sell_txn', 'volume_imbalance', 'txn_imbalance']\n",
    "source2_cols = ['ask_volume', 'bid_volume', 'ask_slope_1', 'ask_slope_5', 'ask_slope_10', 'bid_slope_1', 'bid_slope_5', 'bid_slope_10', 'spread',\n",
    "       'lob_volume_imbalance', 'slope_imbalance_1', 'slope_imbalance_5', 'slope_imbalance_10']\n",
    "# target_col = 'log_deseasoned_total_volume'\n",
    "target_col = 'target'\n",
    "target_direct_col = 'total_volume'\n",
    "weight_col = 'mean_volume'\n",
    "datetime_col = 'datetime'\n",
    "\n",
    "# # Normalize source1 and source2 features using training data only\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler1 = StandardScaler()\n",
    "# scaler2 = StandardScaler()\n",
    "\n",
    "# # Fit only on training portion\n",
    "# source1_train_raw = df[source1_cols].iloc[:n_train]\n",
    "# source2_train_raw = df[source2_cols].iloc[:n_train]\n",
    "\n",
    "# scaler1.fit(source1_train_raw)\n",
    "# scaler2.fit(source2_train_raw)\n",
    "\n",
    "# # Apply normalization to the whole dataset\n",
    "# df[source1_cols] = scaler1.transform(df[source1_cols])\n",
    "# df[source2_cols] = scaler2.transform(df[source2_cols])\n",
    "\n",
    "# --- Create rolling windows efficiently ---\n",
    "source1_array = df[source1_cols].values  # shape (N, F1)\n",
    "source2_array = df[source2_cols].values  # shape (N, F2)\n",
    "target_array = df[target_col].values + 1e-7  # shape (N,)\n",
    "target_direct_array = df[target_direct_col].values + 1e-7  # shape (N,)\n",
    "weight_array = df[weight_col].values  # shape (N,)\n",
    "timestamps_array = df[datetime_col].values\n",
    "\n",
    "\n",
    "# Create sliding windows ([:-1] in windows and [h:] in targets make sure the targets are matched with corresponding features)\n",
    "source1_windows = sliding_window_view(source1_array, window_shape=(h,), axis=0)[:-1]  # shape (N - h, F1, h)\n",
    "source2_windows = sliding_window_view(source2_array, window_shape=(h,), axis=0)[:-1]  # shape (N - h, F2, h)\n",
    "y = target_array[h:]\n",
    "w = weight_array[h:]\n",
    "timestamps = timestamps_array[h:]\n",
    "\n",
    "# Convert to tensors\n",
    "source1_tensor = torch.tensor(source1_windows, dtype=torch.float32)\n",
    "source2_tensor = torch.tensor(source2_windows, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "w_tensor = torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "# --- Time-based split (preserving time order) ---\n",
    "n_total = len(y_tensor)\n",
    "n_train = int(n_total * 0.7)\n",
    "n_val = int(n_total * 0.1)\n",
    "\n",
    "source1_train, source1_val, source1_test = source1_tensor[:n_train], source1_tensor[n_train:n_train + n_val], source1_tensor[n_train + n_val:]\n",
    "source2_train, source2_val, source2_test = source2_tensor[:n_train], source2_tensor[n_train:n_train + n_val], source2_tensor[n_train + n_val:]\n",
    "y_train, y_val, y_test = y_tensor[:n_train], y_tensor[n_train:n_train + n_val], y_tensor[n_train + n_val:]\n",
    "w_train, w_val, w_test = w_tensor[:n_train], w_tensor[n_train:n_train + n_val], w_tensor[n_train + n_val:]\n",
    "\n",
    "# (Optional) timestamps split for tracking\n",
    "timestamps_train = timestamps[:n_train]\n",
    "timestamps_val = timestamps[n_train:n_train + n_val]\n",
    "timestamps_test = timestamps[n_train + n_val:]\n",
    "\n",
    "# Dataset ready for PyTorch training\n",
    "train_dataset = torch.utils.data.TensorDataset(source1_train, source2_train, y_train, w_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(source1_val, source2_val, y_val, w_val)\n",
    "test_dataset = torch.utils.data.TensorDataset(source1_test, source2_test, y_test, w_test)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "## function for dataset creation for hyperparams search\n",
    "def create_datasets(source1_array, source2_array, target_array, weight_array, batch_size, h):\n",
    "       # Create sliding windows ([:-1] in windows and [h:] in targets make sure the targets are matched with corresponding features)\n",
    "       source1_windows = sliding_window_view(source1_array, window_shape=(h,), axis=0)[:-1]  # shape (N - h, F1, h)\n",
    "       source2_windows = sliding_window_view(source2_array, window_shape=(h,), axis=0)[:-1]  # shape (N - h, F2, h)\n",
    "       y = target_array[h:]\n",
    "       w = weight_array[h:]\n",
    "\n",
    "       # Convert to tensors\n",
    "       source1_tensor = torch.tensor(source1_windows, dtype=torch.float32)\n",
    "       source2_tensor = torch.tensor(source2_windows, dtype=torch.float32)\n",
    "       y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "       w_tensor = torch.tensor(w, dtype=torch.float32)\n",
    "\n",
    "       # --- Time-based split (preserving time order) ---\n",
    "       n_total = len(y_tensor)\n",
    "       n_train = int(n_total * 0.7)\n",
    "       n_val = int(n_total * 0.1)\n",
    "\n",
    "       source1_train, source1_val, source1_test = source1_tensor[:n_train], source1_tensor[n_train:n_train + n_val], source1_tensor[n_train + n_val:]\n",
    "       source2_train, source2_val, source2_test = source2_tensor[:n_train], source2_tensor[n_train:n_train + n_val], source2_tensor[n_train + n_val:]\n",
    "       y_train, y_val, y_test = y_tensor[:n_train], y_tensor[n_train:n_train + n_val], y_tensor[n_train + n_val:]\n",
    "       w_train, w_val, w_test = w_tensor[:n_train], w_tensor[n_train:n_train + n_val], w_tensor[n_train + n_val:]\n",
    "\n",
    "       # Dataset ready for PyTorch training\n",
    "       train_dataset = torch.utils.data.TensorDataset(source1_train, source2_train, y_train, w_train)\n",
    "       val_dataset = torch.utils.data.TensorDataset(source1_val, source2_val, y_val, w_val)\n",
    "       test_dataset = torch.utils.data.TensorDataset(source1_test, source2_test, y_test, w_test)\n",
    "\n",
    "       train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "       val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "       test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "       return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1073bf4",
   "metadata": {},
   "source": [
    "### TME components "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6876e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilinearRegressor(nn.Module):\n",
    "    def __init__(self, d, h, latent_variable):\n",
    "        \"\"\"\n",
    "        d: number of features in the source data\n",
    "        h: number of lags in the source data\n",
    "        latent_variable (bool): if True the class is devoted for modeling latent variable z, if False => y|s_i\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.latent_variable = latent_variable\n",
    "\n",
    "        # Mean parameters\n",
    "        self.L_mu = nn.Parameter(torch.empty(d))\n",
    "        self.R_mu = nn.Parameter(torch.empty(h))\n",
    "        self.b_mu = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "        # Xavier init for 1D weight tensors\n",
    "        # nn.init.xavier_uniform_(self.L_mu.unsqueeze(0))\n",
    "        # nn.init.xavier_uniform_(self.R_mu.unsqueeze(0))\n",
    "\n",
    "        nn.init.xavier_normal_(self.L_mu.unsqueeze(0))\n",
    "        nn.init.xavier_normal_(self.R_mu.unsqueeze(0))\n",
    "\n",
    "        if not self.latent_variable:\n",
    "            self.L_sigma = nn.Parameter(torch.empty(d))\n",
    "            self.R_sigma = nn.Parameter(torch.empty(h))\n",
    "            self.b_sigma = nn.Parameter(torch.zeros(1))\n",
    "\n",
    "            # nn.init.xavier_uniform_(self.L_sigma.unsqueeze(0))\n",
    "            # nn.init.xavier_uniform_(self.R_sigma.unsqueeze(0))\n",
    "\n",
    "            nn.init.xavier_normal_(self.L_sigma.unsqueeze(0))\n",
    "            nn.init.xavier_normal_(self.R_sigma.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):  # x: (B, d, h)\n",
    "        mu = torch.einsum('bdh,d,h->b', x, self.L_mu, self.R_mu) + self.b_mu  # [B]\n",
    "        if self.latent_variable:\n",
    "            return mu\n",
    "        log_var = torch.einsum('bdh,d,h->b', x, self.L_sigma, self.R_sigma) + self.b_sigma\n",
    "        log_var = torch.clamp(log_var, min=-10, max=10)\n",
    "        var = torch.exp(log_var)  # Ensure positivity\n",
    "        return mu, var\n",
    "    \n",
    "\n",
    "class TME(nn.Module):\n",
    "    def __init__(self, d1, d2, h):\n",
    "        super().__init__()\n",
    "        self.target1 = BilinearRegressor(d1, h, latent_variable=False)\n",
    "        self.target2 = BilinearRegressor(d2, h, latent_variable=False)\n",
    "        self.latent1 = BilinearRegressor(d1, h, latent_variable=True)\n",
    "        self.latent2 = BilinearRegressor(d2, h, latent_variable=True)\n",
    "\n",
    "    def forward(self, x1, x2, return_all=False):\n",
    "        # x1: (B, d1, h), x2: (B, d2, h)\n",
    "        mu1, var1 = self.target1(x1)  # [B], [B]\n",
    "        mu2, var2 = self.target2(x2)\n",
    "\n",
    "        logit1 = self.latent1(x1)\n",
    "        logit2 = self.latent2(x2)\n",
    "\n",
    "        logits = torch.stack([logit1, logit2], dim=1)  # [B, num_sources]\n",
    "        probs = F.softmax(logits, dim=1)     # [B, num_sources]\n",
    "\n",
    "        if True:#not return_all:\n",
    "            # Clamp to avoid numerical instability\n",
    "            mu1 = torch.clamp(mu1, -10, 10)\n",
    "            mu2 = torch.clamp(mu2, -10, 10)\n",
    "            var1 = torch.clamp(var1, min=1e-5, max=10)\n",
    "            var2 = torch.clamp(var2, min=1e-5, max=10)\n",
    "\n",
    "        # Mixture of expected values under log-normal\n",
    "        exp1 = torch.exp(mu1 + 0.5 * var1)\n",
    "        exp2 = torch.exp(mu2 + 0.5 * var2)\n",
    "        final_pred = probs[:, 0] * exp1 + probs[:, 1] * exp2  # [B]\n",
    "\n",
    "        if return_all:\n",
    "            return final_pred, mu1, var1, mu2, var2, probs\n",
    "        return final_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bfa6c8",
   "metadata": {},
   "source": [
    "### Training routine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7834722",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4fb34df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tme_loss(y, mu1, var1, mu2, var2, probs, model, l2_lambda=0.1):\n",
    "    \"\"\"\n",
    "    Implements:\n",
    "        -ln ∑_s [ lognormal(y_t | μ_s, σ_s^2) * P(z_t = s | x) ] + λ * ||θ||^2\n",
    "    \"\"\"\n",
    "\n",
    "    eps = 1e-8  # for numerical stability\n",
    "    log_y = torch.log(y + eps)\n",
    "\n",
    "    # Log-normal density terms (not in log-space)\n",
    "    def lognormal_pdf(y, log_y, mu, var):\n",
    "        coef = 1.0 / (y * torch.sqrt(2 * torch.pi * var + eps))\n",
    "        exponent = torch.exp(- (log_y - mu) ** 2 / (2 * var + eps))\n",
    "        return coef * exponent\n",
    "\n",
    "    p1 = lognormal_pdf(y, log_y, mu1, var1)\n",
    "    p2 = lognormal_pdf(y, log_y, mu2, var2)\n",
    "\n",
    "    # Combine with selector probabilities\n",
    "    # print(probs[:,1])\n",
    "    weighted_sum = probs[:,0] * p1 + probs[:,1] * p2\n",
    "\n",
    "    # Negative log-likelihood (mean over batch)\n",
    "    nll = -torch.log(weighted_sum + eps).mean() #maybe mean or sum\n",
    "\n",
    "    # L2 Regularization (Gaussian prior on θ)\n",
    "    l2_penalty = sum((p**2).sum() for p in model.parameters())\n",
    "    reg = l2_lambda * l2_penalty\n",
    "\n",
    "    return nll + reg\n",
    "\n",
    "\n",
    "def train_tme_model(model, train_loader, val_loader, lr=5e-4, weight_decay=0.1, l2_lambda=0.1,\n",
    "                    max_epochs=100, patience=10, device='cpu', adam=False, direct_target=False):\n",
    "    model.to(device)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    # optimizer = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "    # optimizer = torch.optim.SGD(model.parameters(), lr=lr)#, weight_decay=0.1)#, momentum=0.9)\n",
    "    if adam:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    else:\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_rmse = float('inf')\n",
    "    best_state_dict = None\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "\n",
    "        for x1, x2, y, w in train_loader:\n",
    "            x1, x2, y, w = x1.to(device), x2.to(device), y.to(device), w.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            final_pred, mu1, var1, mu2, var2, probs = model(x1, x2, return_all=True)\n",
    "\n",
    "            loss = tme_loss(y, mu1, var1, mu2, var2, probs, model, l2_lambda=l2_lambda)\n",
    "            loss.backward()\n",
    "\n",
    "            # total_norm = 0\n",
    "            # for p in model.parameters():\n",
    "            #     if p.grad is not None:\n",
    "            #         param_norm = p.grad.data.norm(2)\n",
    "            #         total_norm += param_norm.item() ** 2\n",
    "            # total_norm = total_norm ** 0.5\n",
    "            # print(f\"Gradient norm: {total_norm:.4f}\")\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        y_preds_val = []\n",
    "        y_true_val = []\n",
    "        w_val = []\n",
    "\n",
    "        train_losses = []\n",
    "        y_preds_train = []\n",
    "        y_true_train = []\n",
    "        w_train = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x1, x2, y, w in val_loader:\n",
    "                x1, x2, y, w = x1.to(device), x2.to(device), y.to(device), w.to(device)\n",
    "\n",
    "                final_pred, mu1, var1, mu2, var2, probs = model(x1, x2, return_all=True)\n",
    "\n",
    "                val_loss = tme_loss(y, mu1, var1, mu2, var2, probs, model, l2_lambda=l2_lambda)\n",
    "                val_losses.append(val_loss.item())\n",
    "                y_preds_val.append(final_pred.detach().cpu())\n",
    "                y_true_val.append(y.detach().cpu())\n",
    "                w_val.append(w.detach().cpu())\n",
    "\n",
    "            for x1, x2, y, w in train_loader:\n",
    "                x1, x2, y, w = x1.to(device), x2.to(device), y.to(device), w.to(device)\n",
    "\n",
    "                final_pred, mu1, var1, mu2, var2, probs = model(x1, x2, return_all=True)\n",
    "\n",
    "                train_loss = tme_loss(y, mu1, var1, mu2, var2, probs, model, l2_lambda=l2_lambda)\n",
    "                train_losses.append(train_loss.item())\n",
    "                y_preds_train.append(final_pred.detach().cpu())\n",
    "                y_true_train.append(y.detach().cpu())\n",
    "                w_train.append(w.detach().cpu())\n",
    "            \n",
    "\n",
    "        avg_val_loss = sum(val_losses) / len(val_losses)\n",
    "        y_preds_val = torch.cat(y_preds_val).numpy()\n",
    "        y_true_val = torch.cat(y_true_val).numpy()\n",
    "        w_val = torch.cat(w_val).numpy()\n",
    "        if direct_target:\n",
    "            rmse_val = np.sqrt(mean_squared_error(y_true_val, y_preds_val))\n",
    "        else:\n",
    "            rmse_val = np.sqrt(mean_squared_error(y_true_val*w_val, y_preds_val*w_val))\n",
    "\n",
    "        avg_train_loss = sum(train_losses) / len(train_losses)\n",
    "        y_preds_train = torch.cat(y_preds_train).numpy()\n",
    "        y_true_train = torch.cat(y_true_train).numpy()\n",
    "        w_train = torch.cat(w_train).numpy()\n",
    "        if direct_target:\n",
    "            rmse_train = np.sqrt(mean_squared_error(y_true_train, y_preds_train))\n",
    "        else:    \n",
    "            rmse_train = np.sqrt(mean_squared_error(y_true_train*w_train, y_preds_train*w_train))\n",
    "\n",
    "        if (epoch+1) % 5 == 0 or epoch == 0:\n",
    "            print(f\"Epoch {epoch+1}.      Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}.      Train RMSE: {rmse_train:.4f}, Val RMSE: {rmse_val:.4f}\")\n",
    "\n",
    "        # Early stopping\n",
    "        if rmse_val < best_val_rmse - 1e-4:#avg_val_loss < best_val_loss - 1e-4\n",
    "            # best_val_loss = avg_val_loss\n",
    "            best_val_rmse = rmse_val\n",
    "            best_state_dict = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "\n",
    "    # Restore best model\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    return model, best_val_loss\n",
    "\n",
    "\n",
    "def train_tme_ensemble(train_loader, val_loader, d1, d2, h, num_models=20, device='cpu', adam=False, direct_target=False, **train_kwargs):\n",
    "    ensemble = []\n",
    "    val_losses = []\n",
    "\n",
    "    for i in range(num_models):\n",
    "        print(f\"\\n🌱 Training ensemble model {i + 1}/{num_models}\")\n",
    "\n",
    "        # Set seed for reproducibility\n",
    "        torch.manual_seed(i)\n",
    "        model = TME(d1, d2, h)  # Initialize new model\n",
    "\n",
    "        # Train the model using your function\n",
    "        trained_model, best_val_loss = train_tme_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            device=device,\n",
    "            adam=adam,\n",
    "            direct_target=direct_target,\n",
    "            **train_kwargs\n",
    "        )\n",
    "\n",
    "        # Save the model and its validation loss\n",
    "        ensemble.append(trained_model)\n",
    "        val_losses.append(best_val_loss)\n",
    "\n",
    "    return ensemble, val_losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc2596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tme_ensemble(ensemble, test_loader, all_preds=False, device='cpu', direct_target=False):\n",
    "    all_preds = []\n",
    "    all_preds_median = []\n",
    "    y_trues = []\n",
    "    w_trues = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, y, w in test_loader:\n",
    "            x1, x2, y, w = x1.to(device).to(torch.float64), x2.to(device).to(torch.float64), y.to(device).to(torch.float64), w.to(device).to(torch.float64)\n",
    "            batch_preds = []\n",
    "\n",
    "            for model in ensemble:\n",
    "                model.eval()\n",
    "                model.to(device).to(torch.float64)\n",
    "                pred = model(x1, x2)\n",
    "                # print(pred)\n",
    "                # return\n",
    "                batch_preds.append(pred.cpu())\n",
    "\n",
    "            # Average predictions from all models\n",
    "            avg_pred = torch.stack(batch_preds).mean(dim=0)\n",
    "            median_pred = torch.stack(batch_preds).median(dim=0).values\n",
    "            # avg_pred = torch.stack(batch_preds).median(dim=0)\n",
    "            all_preds.append(avg_pred)\n",
    "            all_preds_median.append(median_pred)\n",
    "            y_trues.append(y.cpu())\n",
    "            w_trues.append(w.cpu())\n",
    "\n",
    "    y_preds = torch.cat(all_preds).numpy()\n",
    "    y_preds_median = torch.cat(all_preds_median).numpy()\n",
    "    y_trues = torch.cat(y_trues).numpy()\n",
    "    w_trues = torch.cat(w_trues).numpy()\n",
    "\n",
    "    if direct_target:\n",
    "        rmse = np.sqrt(mean_squared_error(y_trues, y_preds))\n",
    "        mae = mean_absolute_error(y_trues, y_preds)\n",
    "    else:\n",
    "        rmse = np.sqrt(mean_squared_error(y_trues*w_trues, y_preds*w_trues))\n",
    "        mae = mean_absolute_error(y_trues*w_trues, y_preds*w_trues)\n",
    "\n",
    "    # print(f\"📊 Ensemble Test RMSE: {rmse:.4f}\")\n",
    "    # print(f\"📊 Ensemble Test MAE: {mae:.4f}\")\n",
    "    return y_preds, y_preds_median, rmse, mae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84aa31a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_list = [5, 10, 20, 40]\n",
    "batch_size_list = [128, 256]\n",
    "lr_list = [1e-4, 5e-4, 1e-3]\n",
    "l2_lambda_list = [0.1, 1, 3, 5]\n",
    "\n",
    "d1=source1_tensor.shape[1]  # number of features of the 1st source\n",
    "d2=source2_tensor.shape[1]  # number of features of the 2nd source\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Make sure the folder exists\n",
    "output_dir = 'validation_results_AS_1min'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for h, batch_size, lr, l2_lambda in tqdm.tqdm(itertools.product(h_list, batch_size_list, lr_list, l2_lambda_list)):\n",
    "    # if [h, batch_size, lr, l2_lambda] in existing_results:\n",
    "    #     continue\n",
    "\n",
    "    # create dataset\n",
    "    train_loader, val_loader, test_loader = create_datasets(source1_array, source2_array, target_direct_array, weight_array, batch_size, h)\n",
    "\n",
    "    print(f\"\\n📊 h: {h}, batch_size:{batch_size}, lr: {lr}, l2_lambda:{l2_lambda}\")\n",
    "    \n",
    "    ensemble, losses = train_tme_ensemble(\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        d1=d1,  # number of features of the 1st source\n",
    "        d2=d2,  # number of features of the 2nd source\n",
    "        h=h,  # lag length\n",
    "        num_models=5,#20,\n",
    "        device=device,\n",
    "        lr=lr,\n",
    "        weight_decay=0.1,\n",
    "        l2_lambda=l2_lambda,\n",
    "        max_epochs=50,\n",
    "        patience=10,\n",
    "        adam=True,\n",
    "        direct_target=True\n",
    "    )\n",
    "\n",
    "    y_preds, y_preds_median, rmse, mae = evaluate_tme_ensemble(ensemble, val_loader, device=device, direct_target=True)\n",
    "\n",
    "    results_to_save = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'val_losses': losses\n",
    "    }\n",
    "\n",
    "    # Create a filename based on hyperparameters\n",
    "    filename = f\"h{h}_batch{batch_size}_lr{lr:.0e}_lambda{l2_lambda}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Save the dictionary to a JSON file\n",
    "    with open(filepath, 'w') as f:\n",
    "        json.dump(results_to_save, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30585f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_fin_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
